{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpJYoQL3DIqz"
      },
      "source": [
        "## Part 3 ‚Äì Case Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLcR4kueCmig",
        "outputId": "5bc2c27b-be33-4d85-b327-9bd349ea7bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktEZ58HPDYJ3"
      },
      "source": [
        "## Representasi Vektor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVrCuKdyDcMv",
        "outputId": "bc78d527-83e0-408a-d16b-305d0a2c22d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ STARTING ENHANCED TF-IDF VECTORIZATION\n",
            "================================================================================\n",
            "üìä i. ENHANCED TF-IDF REPRESENTASI VEKTOR\n",
            "Input processed: /content/drive/MyDrive/perdagangan_orang/data/processed\n",
            "Input raw: /content/drive/MyDrive/perdagangan_orang/CLEANED\n",
            "Output: /content/drive/MyDrive/perdagangan_orang/data/vectors\n",
            "üìù Using enhanced stopwords: 61 terms\n",
            "   Keeping legal domain terms for better representation\n",
            "üöÄ ENHANCED TF-IDF REPRESENTASI VEKTOR\n",
            "======================================================================\n",
            "üìÅ Loaded 79 cases from CSV\n",
            "üìã Preparing case data for enhanced TF-IDF...\n",
            "‚úÖ Prepared 79 cases for TF-IDF vectorization\n",
            "üìù Sample case text length: 4862 chars\n",
            "   Preview: perkara pidana perkara pidana perkara pidana pasal 11 jo pasal 4 jo pasal 48 uu; pasal 10 jo pasal 4 jo pasal 48 uu; pasal 81 uu; pasal 10 jo. pasal 4 jo pasal 48 uu; pasal 10 jo. pasal 4 jo. pasal 48...\n",
            "\n",
            "üìä Creating Enhanced TF-IDF Vectors\n",
            "==================================================\n",
            "Features:\n",
            "  ‚Ä¢ 20K vocabulary (expanded)\n",
            "  ‚Ä¢ Trigrams (1-3 n-grams)\n",
            "  ‚Ä¢ Enhanced legal stopwords\n",
            "  ‚Ä¢ Legal term boosting\n",
            "  ‚Ä¢ Smart text preprocessing\n",
            "==================================================\n",
            "üîÑ Fitting TF-IDF vectorizer...\n",
            "üöÄ Applying legal term boosting...\n",
            "üìà Applied legal term boosting:\n",
            "   Boosted terms: 8/32\n",
            "   Total boost applied: 5.37\n",
            "\n",
            "üìà Analyzing vocabulary...\n",
            "üìã Legal terms in vocabulary: 6/19\n",
            "   Found: ['perdagangan', 'anak', 'perempuan', 'terdakwa', 'hakim', 'penjara']\n",
            "   Missing: ['eksploitasi', 'korban', 'perekrutan', 'pengangkutan', 'penampungan', 'pemaksaan', 'jaksa', 'pengadilan', 'pasal', 'putusan', 'vonis', 'hukuman', 'denda']\n",
            "\n",
            "‚úÖ Enhanced TF-IDF vectors created successfully!\n",
            "üìä Matrix shape: (79, 4489)\n",
            "üìö Vocabulary size: 4,489\n",
            "üéØ Sparsity: 92.29%\n",
            "\n",
            "üß™ Testing with sample queries...\n",
            "Query: 'perdagangan orang lintas negara'\n",
            "  Non-zero elements: 5\n",
            "  ‚úÖ Good representation\n",
            "\n",
            "Query: 'eksploitasi seksual terhadap anak'\n",
            "  Non-zero elements: 2\n",
            "  ‚úÖ Good representation\n",
            "\n",
            "Query: 'perekrutan perempuan secara paksa'\n",
            "  Non-zero elements: 2\n",
            "  ‚úÖ Good representation\n",
            "\n",
            "Query: 'pemaksaan kerja di luar negeri'\n",
            "  Non-zero elements: 1\n",
            "  ‚úÖ Good representation\n",
            "\n",
            "Query: 'penampungan korban perdagangan orang'\n",
            "  Non-zero elements: 3\n",
            "  ‚úÖ Good representation\n",
            "\n",
            "\n",
            "üíæ Saving enhanced TF-IDF vectors...\n",
            "‚úÖ Enhanced TF-IDF vectors saved: enhanced_tfidf_only_20250625_125040.pkl\n",
            "üìä Package contents:\n",
            "   ‚Ä¢ TF-IDF matrix: (79, 4489)\n",
            "   ‚Ä¢ Vocabulary: 4,489 terms\n",
            "   ‚Ä¢ Cases: 79 documents\n",
            "   ‚Ä¢ Sparsity: 92.29%\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ENHANCED TF-IDF REPRESENTASI VEKTOR COMPLETED!\n",
            "üéØ Optimizations Applied:\n",
            "  ‚úÖ 20K vocabulary (expanded)\n",
            "  ‚úÖ Trigrams for better phrase matching\n",
            "  ‚úÖ Enhanced legal stopwords\n",
            "  ‚úÖ Comprehensive legal term boosting\n",
            "  ‚úÖ Smart text preprocessing\n",
            "  ‚úÖ Legal entity extraction\n",
            "üìÅ Files saved to: /content/drive/MyDrive/perdagangan_orang/data/vectors\n",
            "üìä Ready for: similarity search, clustering, classification\n",
            "======================================================================\n",
            "\n",
            "üéâ ENHANCED TF-IDF VECTORIZATION BERHASIL!\n",
            "‚ú® Key Features:\n",
            "  üî§ 20,000 vocabulary size\n",
            "  üìù Trigram support (1-3 n-grams)\n",
            "  ‚öñÔ∏è Legal domain optimization\n",
            "  üöÄ Smart term boosting\n",
            "  üßπ Enhanced preprocessing\n",
            "\n",
            "üìà Performance Benefits:\n",
            "  ‚ö° Fast processing & querying\n",
            "  üéØ High accuracy for legal docs\n",
            "  üìä Interpretable results\n",
            "  üíæ Efficient storage\n",
            "\n",
            "Next steps: Load vectors for similarity search/clustering\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# i. REPRESENTASI VEKTOR - TF-IDF ONLY (ENHANCED)\n",
        "# Enhanced TF-IDF: sklearn.feature_extraction.text.TfidfVectorizer\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EnhancedTFIDFVectorizer:\n",
        "    \"\"\"\n",
        "    i. Representasi Vektor - Enhanced TF-IDF Only\n",
        "    Optimized TF-IDF dengan legal term boosting dan preprocessing khusus dokumen hukum\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.processed_dir = os.path.join(base_dir, \"data\", \"processed\")\n",
        "        self.raw_dir = os.path.join(base_dir, \"CLEANED\")\n",
        "        self.output_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üìä i. ENHANCED TF-IDF REPRESENTASI VEKTOR\")\n",
        "        print(f\"Input processed: {self.processed_dir}\")\n",
        "        print(f\"Input raw: {self.raw_dir}\")\n",
        "        print(f\"Output: {self.output_dir}\")\n",
        "\n",
        "        # Enhanced TF-IDF Vectorizer Configuration\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=20000,          # ‚¨ÜÔ∏è Increased vocabulary\n",
        "            min_df=2,                   # Min document frequency\n",
        "            max_df=0.85,                # Max document frequency\n",
        "            ngram_range=(1, 3),         # Unigram, bigram, trigram\n",
        "            lowercase=True,\n",
        "            stop_words=self.get_enhanced_legal_stopwords(),\n",
        "            sublinear_tf=True,          # Use log scaling\n",
        "            norm='l2',                  # L2 normalization\n",
        "            smooth_idf=True,           # Smooth IDF weights\n",
        "            use_idf=True,              # Enable IDF\n",
        "            token_pattern=r'(?u)\\b\\w+\\b'  # Token pattern\n",
        "        )\n",
        "\n",
        "        # Data storage\n",
        "        self.cases_df = None\n",
        "        self.case_ids = []\n",
        "        self.case_texts = {}\n",
        "        self.tfidf_vectors = None\n",
        "        self.feature_names = None\n",
        "        self.vocabulary_stats = {}\n",
        "\n",
        "    def get_enhanced_legal_stopwords(self) -> List[str]:\n",
        "        \"\"\"Enhanced legal stopwords - remove common words but keep legal terms\"\"\"\n",
        "        # Basic Indonesian stopwords only\n",
        "        basic_stopwords = [\n",
        "            'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'dengan', 'untuk',\n",
        "            'dalam', 'oleh', 'adalah', 'akan', 'telah', 'sudah', 'dapat',\n",
        "            'tidak', 'belum', 'juga', 'bahwa', 'sebagai', 'atau', 'jika',\n",
        "            'karena', 'sehingga', 'maka', 'agar', 'itu', 'ini', 'tersebut',\n",
        "            'hal', 'ada', 'sebuah', 'suatu', 'semua', 'setiap', 'beberapa',\n",
        "            'antara', 'selama', 'sampai', 'hingga', 'sejak', 'setelah',\n",
        "            'sebelum', 'kecuali', 'tanpa', 'bisa', 'hanya', 'masih',\n",
        "            'pun', 'lah', 'kah', 'nya', 'mu', 'ku', 'dia', 'mereka',\n",
        "            'kita', 'kami', 'saya', 'anda', 'beliau'\n",
        "        ]\n",
        "\n",
        "       # EXPLICITLY KEEP these important legal terms (don't add to stopwords):\n",
        "       # terdakwa, jaksa, hakim, perdagangan, eksploitasi, anak, perempuan,\n",
        "       # pasal, pengadilan, putusan, vonis, hukuman, denda, penjara, korban, perekrutan, pemaksaan\n",
        "\n",
        "        print(f\"üìù Using enhanced stopwords: {len(basic_stopwords)} terms\")\n",
        "        print(f\"   Keeping legal domain terms for better representation\")\n",
        "\n",
        "        return basic_stopwords\n",
        "\n",
        "    def enhanced_text_preprocessing(self, text: str) -> str:\n",
        "        \"\"\"Enhanced preprocessing untuk dokumen hukum\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Handle legal abbreviations - EXPAND them for better matching\n",
        "        legal_abbrev = {\n",
        "        'ps': 'pasal', 'ps.': 'pasal',\n",
        "        'uu': 'undang_undang', 'u.u': 'undang_undang',\n",
        "        'pp': 'peraturan_pemerintah', 'p.p': 'peraturan_pemerintah',\n",
        "        'ma': 'mahkamah_agung', 'm.a': 'mahkamah_agung',\n",
        "        'rp': 'rupiah', 'rp.': 'rupiah'\n",
        "        }\n",
        "\n",
        "\n",
        "        for abbrev, expansion in legal_abbrev.items():\n",
        "            text = re.sub(r'\\b' + re.escape(abbrev) + r'\\b', expansion, text)\n",
        "\n",
        "        # Normalize money amounts for better clustering\n",
        "        text = re.sub(r'rupiah\\s*\\d+[\\d\\.,]*(?:\\s*(?:juta|miliar|ribu|triliun))?',\n",
        "                     'nominal_uang', text)\n",
        "\n",
        "        # Normalize case numbers and long digits\n",
        "        text = re.sub(r'\\b\\d{4,}\\b', 'nomor_kasus', text)\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters but keep legal punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\-/\\.]', ' ', text)\n",
        "\n",
        "        # Remove very short words (< 3 chars) except important ones\n",
        "        important_short = {'ps', 'uu', 'pp', 'ma', 'rp', 'pt', 'cv', 'ud'}\n",
        "        words = text.split()\n",
        "        words = [word for word in words if len(word) >= 3 or word in important_short]\n",
        "        text = ' '.join(words)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_legal_entities(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract important legal entities and concepts\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        # Money amounts - critical for corruption cases\n",
        "        money_pattern = r'(rp\\.?\\s*\\d+[\\d\\.,]*(?:\\s*(?:juta|miliar|ribu|triliun))?)'\n",
        "        money_matches = re.findall(money_pattern, text.lower())\n",
        "        entities.extend([f'nominal_{match.replace(\" \", \"_\").replace(\".\", \"\")}' for match in money_matches[:3]])\n",
        "\n",
        "        # Legal institutions\n",
        "        institutions = [\n",
        "            'kejaksaan', 'pengadilan', 'kpk', 'mahkamah', 'dpr', 'dprd',\n",
        "            'kemenkeu', 'kementerian', 'dinas', 'bumn', 'bumd', 'pemerintah',\n",
        "            'kepolisian', 'bpk', 'bkn'\n",
        "        ]\n",
        "        for inst in institutions:\n",
        "            if inst in text.lower():\n",
        "                entities.append(f'institusi_{inst}')\n",
        "\n",
        "        # Pasal references - very important for legal similarity\n",
        "        pasal_pattern = r'pasal\\s+(\\d+)'\n",
        "        pasal_matches = re.findall(pasal_pattern, text.lower())\n",
        "        entities.extend([f'pasal_{match}' for match in pasal_matches[:5]])\n",
        "\n",
        "        # Legal processes\n",
        "        processes = ['tender', 'lelang', 'pengadaan', 'kontrak', 'proyek']\n",
        "        for process in processes:\n",
        "            if process in text.lower():\n",
        "                entities.append(f'proses_{process}')\n",
        "\n",
        "        return entities[:15]  # Limit entities\n",
        "\n",
        "    def load_cases_data(self) -> bool:\n",
        "        \"\"\"Load data dari cases.csv yang sudah diproses\"\"\"\n",
        "        cases_file = os.path.join(self.processed_dir, \"cases.csv\")\n",
        "\n",
        "        if not os.path.exists(cases_file):\n",
        "            logger.error(f\"File tidak ditemukan: {cases_file}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            self.cases_df = pd.read_csv(cases_file, encoding='utf-8')\n",
        "            print(f\"üìÅ Loaded {len(self.cases_df)} cases from CSV\")\n",
        "\n",
        "            # Prepare case data\n",
        "            self.prepare_case_data()\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading cases.csv: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_raw_document_text(self, case_id: str) -> str:\n",
        "        \"\"\"Load raw document text dari file .txt\"\"\"\n",
        "        filepath = os.path.join(self.raw_dir, f\"{case_id}.txt\")\n",
        "\n",
        "        if os.path.exists(filepath):\n",
        "            try:\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error reading {filepath}: {e}\")\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def prepare_case_data(self):\n",
        "        \"\"\"Enhanced: Siapkan data kasus untuk TF-IDF vectorization\"\"\"\n",
        "        print(\"üìã Preparing case data for enhanced TF-IDF...\")\n",
        "\n",
        "        for idx, row in self.cases_df.iterrows():\n",
        "            filename = row['nama_file']\n",
        "            case_id = filename.replace('.txt', '') if filename.endswith('.txt') else filename\n",
        "\n",
        "            # Gabungkan metadata dengan intelligent weighting\n",
        "            text_parts = []\n",
        "\n",
        "            # Jenis perkara - triple weight (very important for clustering)\n",
        "            if pd.notna(row.get('jenis_perkara')):\n",
        "                jenis = str(row['jenis_perkara'])\n",
        "                text_parts.extend([jenis] * 3)\n",
        "\n",
        "            # Pasal - double weight (important for legal similarity)\n",
        "            if pd.notna(row.get('pasal_yang_dilanggar')):\n",
        "                pasal = str(row['pasal_yang_dilanggar'])\n",
        "                text_parts.extend([pasal] * 2)\n",
        "\n",
        "            # Other metadata - single weight\n",
        "            metadata_fields = ['terdakwa', 'jaksa_penuntut_umum', 'hakim']\n",
        "            for field in metadata_fields:\n",
        "                if pd.notna(row.get(field)):\n",
        "                    text_parts.append(str(row[field]))\n",
        "\n",
        "            # Load and process raw document text\n",
        "            raw_text = self.load_raw_document_text(case_id)\n",
        "\n",
        "            if raw_text.strip():\n",
        "                # Enhanced preprocessing\n",
        "                cleaned_raw = self.enhanced_text_preprocessing(raw_text)\n",
        "\n",
        "                # Extract legal entities\n",
        "                entities = self.extract_legal_entities(raw_text)\n",
        "\n",
        "                # Intelligent text truncation - keep decision parts\n",
        "                if len(cleaned_raw) > 4000:  # Increased limit\n",
        "                    # Try to keep the judgment/decision part\n",
        "                    decision_keywords = ['putusan', 'memutuskan', 'menjatuhkan', 'menghukum']\n",
        "                    decision_start = -1\n",
        "\n",
        "                    for keyword in decision_keywords:\n",
        "                        pos = cleaned_raw.find(keyword)\n",
        "                        if pos > 0:\n",
        "                            decision_start = pos\n",
        "                            break\n",
        "\n",
        "                    if decision_start > 0:\n",
        "                        # Keep beginning + decision part\n",
        "                        beginning = cleaned_raw[:2000]\n",
        "                        decision_part = cleaned_raw[decision_start:decision_start+2000]\n",
        "                        cleaned_raw = beginning + ' ' + decision_part\n",
        "                    else:\n",
        "                        cleaned_raw = cleaned_raw[:4000]\n",
        "\n",
        "                text_parts.append(cleaned_raw)\n",
        "                text_parts.extend(entities)\n",
        "            else:\n",
        "                # Fallback for missing text\n",
        "                text_parts.append(f\"dokumen_hukum_perdagangan_orang {case_id}\")\n",
        "\n",
        "            # Final combined text\n",
        "            final_text = ' '.join(text_parts) if text_parts else f\"dokumen hukum {case_id}\"\n",
        "\n",
        "            self.case_ids.append(case_id)\n",
        "            self.case_texts[case_id] = final_text\n",
        "\n",
        "        print(f\"‚úÖ Prepared {len(self.case_ids)} cases for TF-IDF vectorization\")\n",
        "\n",
        "        # Sample analysis\n",
        "        if self.case_texts:\n",
        "            sample_case = list(self.case_texts.keys())[0]\n",
        "            sample_text = self.case_texts[sample_case]\n",
        "            print(f\"üìù Sample case text length: {len(sample_text)} chars\")\n",
        "            print(f\"   Preview: {sample_text[:200]}...\")\n",
        "\n",
        "    def apply_legal_term_boosting(self, tfidf_matrix):\n",
        "        \"\"\"Enhanced legal term boosting for corruption domain\"\"\"\n",
        "        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Comprehensive legal term boosting weights\n",
        "        legal_boost_terms = {\n",
        "          # Core TPPO terms - highest boost\n",
        "          'perdagangan_orang': 3.0,\n",
        "          'tindak_pidana_perdagangan_orang': 3.0,\n",
        "          'eksploitasi': 2.8,\n",
        "\n",
        "          # Bentuk eksploitasi - high boost\n",
        "          'eksploitasi_seksual': 2.5,\n",
        "          'kerja_paksa': 2.5,\n",
        "          'perbudakan': 2.3,\n",
        "          'pengambilan_organs': 2.3,\n",
        "\n",
        "          # Proses kejahatan TPPO - medium-high boost\n",
        "          'perekrutan': 2.2,\n",
        "          'pemindahan': 2.0,\n",
        "          'penampungan': 2.0,\n",
        "          'pemaksaan': 2.2,\n",
        "          'penipuan': 2.0,\n",
        "          'ancaman': 2.0,\n",
        "\n",
        "          # Korban & pelaku - medium boost\n",
        "          'korban': 2.3,\n",
        "          'anak': 2.2,\n",
        "          'perempuan': 2.2,\n",
        "          'terdakwa': 1.8,\n",
        "          'pelaku': 1.6,\n",
        "          'jaksa': 1.5,\n",
        "          'hakim': 1.5,\n",
        "\n",
        "          # Proses hukum - medium boost\n",
        "          'pasal': 1.6,\n",
        "          'undang_undang': 1.5,\n",
        "          'putusan': 1.7,\n",
        "          'vonis': 1.7,\n",
        "          'dakwaan': 1.6,\n",
        "          'tuntutan': 1.5,\n",
        "          'pengadilan': 1.5,\n",
        "          'mahkamah_agung': 1.5,\n",
        "\n",
        "          # Hukuman - medium boost\n",
        "          'hukuman': 1.5,\n",
        "          'penjara': 1.5,\n",
        "          'denda': 1.5,\n",
        "          'pidana': 1.4,\n",
        "      }\n",
        "\n",
        "\n",
        "        boosted_count = 0\n",
        "        total_boost_applied = 0\n",
        "\n",
        "        for term, boost in legal_boost_terms.items():\n",
        "            term_indices = np.where(feature_names == term)[0]\n",
        "            if len(term_indices) > 0:\n",
        "                original_sum = tfidf_matrix[:, term_indices[0]].sum()\n",
        "                tfidf_matrix[:, term_indices[0]] *= boost\n",
        "                new_sum = tfidf_matrix[:, term_indices[0]].sum()\n",
        "                total_boost_applied += (new_sum - original_sum)\n",
        "                boosted_count += 1\n",
        "\n",
        "        print(f\"üìà Applied legal term boosting:\")\n",
        "        print(f\"   Boosted terms: {boosted_count}/{len(legal_boost_terms)}\")\n",
        "        print(f\"   Total boost applied: {total_boost_applied:.2f}\")\n",
        "\n",
        "        return tfidf_matrix\n",
        "\n",
        "    def create_enhanced_tfidf_vectors(self) -> bool:\n",
        "        \"\"\"Create enhanced TF-IDF vectors dengan optimizations\"\"\"\n",
        "        print(\"\\nüìä Creating Enhanced TF-IDF Vectors\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"Features:\")\n",
        "        print(\"  ‚Ä¢ 20K vocabulary (expanded)\")\n",
        "        print(\"  ‚Ä¢ Trigrams (1-3 n-grams)\")\n",
        "        print(\"  ‚Ä¢ Enhanced legal stopwords\")\n",
        "        print(\"  ‚Ä¢ Legal term boosting\")\n",
        "        print(\"  ‚Ä¢ Smart text preprocessing\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        if len(self.case_texts) == 0:\n",
        "            logger.error(\"No case texts available\")\n",
        "            return False\n",
        "\n",
        "        # Prepare texts for TF-IDF\n",
        "        texts = [self.case_texts[case_id] for case_id in self.case_ids]\n",
        "\n",
        "        try:\n",
        "            # Fit TF-IDF vectorizer\n",
        "            print(\"üîÑ Fitting TF-IDF vectorizer...\")\n",
        "            self.tfidf_vectors = self.tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "            # Get feature names\n",
        "            self.feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "            # Apply legal term boosting\n",
        "            print(\"üöÄ Applying legal term boosting...\")\n",
        "            self.tfidf_vectors = self.apply_legal_term_boosting(self.tfidf_vectors)\n",
        "\n",
        "            # Generate vocabulary statistics\n",
        "            self.generate_vocabulary_stats()\n",
        "\n",
        "            print(f\"\\n‚úÖ Enhanced TF-IDF vectors created successfully!\")\n",
        "            print(f\"üìä Matrix shape: {self.tfidf_vectors.shape}\")\n",
        "            print(f\"üìö Vocabulary size: {len(self.feature_names):,}\")\n",
        "            print(f\"üéØ Sparsity: {(1 - self.tfidf_vectors.nnz / (self.tfidf_vectors.shape[0] * self.tfidf_vectors.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating TF-IDF vectors: {e}\")\n",
        "            return False\n",
        "\n",
        "    def generate_vocabulary_stats(self):\n",
        "        \"\"\"Generate comprehensive vocabulary statistics\"\"\"\n",
        "        print(\"\\nüìà Analyzing vocabulary...\")\n",
        "\n",
        "        # Important legal terms check\n",
        "        important_legal_terms = [\n",
        "            'perdagangan', 'eksploitasi', 'korban', 'anak', 'perempuan',\n",
        "            'perekrutan', 'pengangkutan', 'penampungan', 'pemaksaan',\n",
        "            'terdakwa', 'jaksa', 'hakim', 'pengadilan', 'pasal',\n",
        "            'putusan', 'vonis', 'hukuman', 'denda', 'penjara'\n",
        "        ]\n",
        "\n",
        "        found_terms = [term for term in important_legal_terms if term in self.feature_names]\n",
        "        missing_terms = [term for term in important_legal_terms if term not in self.feature_names]\n",
        "\n",
        "        self.vocabulary_stats = {\n",
        "            'total_features': len(self.feature_names),\n",
        "            'legal_terms_found': len(found_terms),\n",
        "            'legal_terms_missing': len(missing_terms),\n",
        "            'found_terms': found_terms,\n",
        "            'missing_terms': missing_terms\n",
        "        }\n",
        "\n",
        "        print(f\"üìã Legal terms in vocabulary: {len(found_terms)}/{len(important_legal_terms)}\")\n",
        "        print(f\"   Found: {found_terms[:10]}{'...' if len(found_terms) > 10 else ''}\")\n",
        "        if missing_terms:\n",
        "            print(f\"   Missing: {missing_terms}\")\n",
        "\n",
        "    def test_query_vectors(self):\n",
        "        \"\"\"Test TF-IDF dengan sample queries\"\"\"\n",
        "        print(\"\\nüß™ Testing with sample queries...\")\n",
        "\n",
        "        test_queries = [\n",
        "            \"perdagangan orang lintas negara\",\n",
        "            \"eksploitasi seksual terhadap anak\",\n",
        "            \"perekrutan perempuan secara paksa\",\n",
        "            \"pemaksaan kerja di luar negeri\",\n",
        "            \"penampungan korban perdagangan orang\"\n",
        "        ]\n",
        "\n",
        "        for query in test_queries:\n",
        "            test_vector = self.tfidf_vectorizer.transform([query])\n",
        "            non_zero_count = test_vector.nnz\n",
        "\n",
        "            print(f\"Query: '{query}'\")\n",
        "            print(f\"  Non-zero elements: {non_zero_count}\")\n",
        "\n",
        "            if non_zero_count == 0:\n",
        "                print(f\"  ‚ö†Ô∏è Empty vector - checking vocabulary overlap...\")\n",
        "                query_words = query.lower().split()\n",
        "                overlap = [word for word in query_words if word in self.feature_names]\n",
        "                print(f\"  Words found in vocab: {overlap}\")\n",
        "            else:\n",
        "                print(f\"  ‚úÖ Good representation\")\n",
        "            print()\n",
        "\n",
        "    def save_enhanced_vectors(self) -> Dict[str, str]:\n",
        "        \"\"\"Save enhanced TF-IDF vectors dengan metadata lengkap\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        saved_files = {}\n",
        "\n",
        "        print(\"\\nüíæ Saving enhanced TF-IDF vectors...\")\n",
        "\n",
        "        if self.tfidf_vectors is not None:\n",
        "            filename = f\"enhanced_tfidf_only_{timestamp}.pkl\"\n",
        "            filepath = os.path.join(self.output_dir, filename)\n",
        "\n",
        "            # Comprehensive data package\n",
        "            tfidf_data = {\n",
        "                # Core vectors and metadata\n",
        "                'vectors': self.tfidf_vectors,\n",
        "                'vectorizer': self.tfidf_vectorizer,\n",
        "                'case_ids': self.case_ids,\n",
        "                'feature_names': self.feature_names,\n",
        "                'case_texts': self.case_texts,\n",
        "                'cases_metadata': self.cases_df,\n",
        "\n",
        "                # Configuration\n",
        "                'config': {\n",
        "                    'max_features': 20000,\n",
        "                    'ngram_range': (1, 3),\n",
        "                    'min_df': 2,\n",
        "                    'max_df': 0.85,\n",
        "                    'legal_term_boosting': True,\n",
        "                    'enhanced_preprocessing': True,\n",
        "                    'vectorizer_type': 'enhanced_tfidf_only'\n",
        "                },\n",
        "\n",
        "                # Statistics\n",
        "                'stats': {\n",
        "                    'vocabulary_size': len(self.feature_names),\n",
        "                    'document_count': len(self.case_ids),\n",
        "                    'matrix_shape': self.tfidf_vectors.shape,\n",
        "                    'sparsity': (1 - self.tfidf_vectors.nnz / (self.tfidf_vectors.shape[0] * self.tfidf_vectors.shape[1])) * 100,\n",
        "                    'vocabulary_stats': self.vocabulary_stats\n",
        "                },\n",
        "\n",
        "                # Metadata\n",
        "                'created_timestamp': timestamp,\n",
        "                'creation_date': datetime.now().isoformat(),\n",
        "                'enhanced': True,\n",
        "                'version': '2.0'\n",
        "            }\n",
        "\n",
        "            # Save to pickle\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump(tfidf_data, f)\n",
        "\n",
        "            saved_files['enhanced_tfidf'] = filepath\n",
        "\n",
        "            print(f\"‚úÖ Enhanced TF-IDF vectors saved: {filename}\")\n",
        "            print(f\"üìä Package contents:\")\n",
        "            print(f\"   ‚Ä¢ TF-IDF matrix: {self.tfidf_vectors.shape}\")\n",
        "            print(f\"   ‚Ä¢ Vocabulary: {len(self.feature_names):,} terms\")\n",
        "            print(f\"   ‚Ä¢ Cases: {len(self.case_ids)} documents\")\n",
        "            print(f\"   ‚Ä¢ Sparsity: {tfidf_data['stats']['sparsity']:.2f}%\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "    def process_tfidf_representation(self) -> bool:\n",
        "        \"\"\"Main process untuk enhanced TF-IDF representation\"\"\"\n",
        "        print(\"üöÄ ENHANCED TF-IDF REPRESENTASI VEKTOR\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Load cases data\n",
        "        if not self.load_cases_data():\n",
        "            print(\"‚ùå Failed to load cases data\")\n",
        "            return False\n",
        "\n",
        "        # Create enhanced TF-IDF vectors\n",
        "        if not self.create_enhanced_tfidf_vectors():\n",
        "            print(\"‚ùå Failed to create TF-IDF vectors\")\n",
        "            return False\n",
        "\n",
        "        # Test with sample queries\n",
        "        self.test_query_vectors()\n",
        "\n",
        "        # Save vectors\n",
        "        saved_files = self.save_enhanced_vectors()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"‚úÖ ENHANCED TF-IDF REPRESENTASI VEKTOR COMPLETED!\")\n",
        "        print(\"üéØ Optimizations Applied:\")\n",
        "        print(\"  ‚úÖ 20K vocabulary (expanded)\")\n",
        "        print(\"  ‚úÖ Trigrams for better phrase matching\")\n",
        "        print(\"  ‚úÖ Enhanced legal stopwords\")\n",
        "        print(\"  ‚úÖ Comprehensive legal term boosting\")\n",
        "        print(\"  ‚úÖ Smart text preprocessing\")\n",
        "        print(\"  ‚úÖ Legal entity extraction\")\n",
        "        print(f\"üìÅ Files saved to: {self.output_dir}\")\n",
        "        print(f\"üìä Ready for: similarity search, clustering, classification\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function untuk enhanced TF-IDF vectorization\"\"\"\n",
        "    print(\"üöÄ STARTING ENHANCED TF-IDF VECTORIZATION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        vectorizer = EnhancedTFIDFVectorizer()\n",
        "        success = vectorizer.process_tfidf_representation()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\nüéâ ENHANCED TF-IDF VECTORIZATION BERHASIL!\")\n",
        "            print(\"‚ú® Key Features:\")\n",
        "            print(\"  üî§ 20,000 vocabulary size\")\n",
        "            print(\"  üìù Trigram support (1-3 n-grams)\")\n",
        "            print(\"  ‚öñÔ∏è Legal domain optimization\")\n",
        "            print(\"  üöÄ Smart term boosting\")\n",
        "            print(\"  üßπ Enhanced preprocessing\")\n",
        "            print(\"\\nüìà Performance Benefits:\")\n",
        "            print(\"  ‚ö° Fast processing & querying\")\n",
        "            print(\"  üéØ High accuracy for legal docs\")\n",
        "            print(\"  üìä Interpretable results\")\n",
        "            print(\"  üíæ Efficient storage\")\n",
        "            print(\"\\nNext steps: Load vectors for similarity search/clustering\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå TF-IDF vectorization failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25pj5NdbJ8ff",
        "outputId": "dbd8f913-60d6-499b-8cdb-a6d61da2ba7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ MULAI ii. SPLITTING DATA\n",
            "======================================================================\n",
            "‚úÇÔ∏è ii. SPLITTING DATA\n",
            "Input vectors: /content/drive/MyDrive/perdagangan_orang/data/vectors\n",
            "Output splits: /content/drive/MyDrive/perdagangan_orang/data/splits\n",
            "‚úÇÔ∏è ii. SPLITTING DATA\n",
            "============================================================\n",
            "1. Split data untuk train dan test\n",
            "2. Rasio 70:30 atau 80:20 berdasarkan artikel penelitian\n",
            "============================================================\n",
            "\n",
            "üì• Loading vectors from previous step...\n",
            "‚úÖ TF-IDF vectors loaded: (79, 4489)\n",
            "üìä Total cases loaded: 79\n",
            "\n",
            "üîÑ Creating multiple splits based on research articles...\n",
            "\n",
            "üìä Creating 70_30 split...\n",
            "\n",
            "‚úÇÔ∏è Creating 70_30 split (test_size=0.3)...\n",
            "üè∑Ô∏è Creating labels for stratified splitting...\n",
            "‚úÖ Stratification possible. Classes: 4, Min samples: 6\n",
            "üìä Using stratified split\n",
            "‚úÖ 70_30 split created:\n",
            "   üìö Training: 55 cases (69.6%)\n",
            "   üß™ Testing: 24 cases (30.4%)\n",
            "\n",
            "üìä Creating 80_20 split...\n",
            "\n",
            "‚úÇÔ∏è Creating 80_20 split (test_size=0.2)...\n",
            "üè∑Ô∏è Creating labels for stratified splitting...\n",
            "‚úÖ Stratification possible. Classes: 4, Min samples: 6\n",
            "üìä Using stratified split\n",
            "‚úÖ 80_20 split created:\n",
            "   üìö Training: 63 cases (79.7%)\n",
            "   üß™ Testing: 16 cases (20.3%)\n",
            "\n",
            "üîç Validating splits...\n",
            "\n",
            "üìä Validating 70_30:\n",
            "‚úÖ No overlap between train and test\n",
            "‚úÖ Complete split: 79 cases\n",
            "‚úÖ TF-IDF dimensions match: 4489 features\n",
            "\n",
            "üìä Validating 80_20:\n",
            "‚úÖ No overlap between train and test\n",
            "‚úÖ Complete split: 79 cases\n",
            "‚úÖ TF-IDF dimensions match: 4489 features\n",
            "\n",
            "‚úÖ All splits are valid!\n",
            "\n",
            "üíæ Saving splits data...\n",
            "üìÑ Data splits saved: data_splits_20250625_130652.pkl\n",
            "üìã Split summary saved: split_summary_20250625_130652.json\n",
            "\n",
            "============================================================\n",
            "‚úÖ ii. SPLITTING DATA COMPLETED!\n",
            "üìä Splits created: ['70_30', '80_20']\n",
            "üìÅ Total cases: 79\n",
            "   70_30: 55 train, 24 test\n",
            "   80_20: 63 train, 16 test\n",
            "üíæ Files saved to: /content/drive/MyDrive/perdagangan_orang/data/splits\n",
            "Langkah selanjutnya: iii. Model Retrieval\n",
            "============================================================\n",
            "\n",
            "üéâ SPLITTING DATA BERHASIL!\n",
            "‚ú® Yang telah dilakukan:\n",
            "  ‚úÖ Load vectors dari tahap i. Representasi Vektor\n",
            "  ‚úÖ Split data dengan rasio 70:30 dan 80:20\n",
            "  ‚úÖ Stratified splitting jika memungkinkan\n",
            "  ‚úÖ Validasi splits untuk memastikan tidak ada overlap\n",
            "  ‚úÖ Simpan splits untuk tahap selanjutnya\n",
            "Langkah selanjutnya: iii. Model Retrieval\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ii. SPLITTING DATA\n",
        "# 1. Lakukan splitting data untuk membagi data menjadi data train dan data test\n",
        "# 2. Rasio perbandingan data dapat berdasarkan kebutuhan atau merujuk pada artikel penelitian,\n",
        "#    missal 70:30 atau 80:20.\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SplittingData:\n",
        "    \"\"\"\n",
        "    ii. Splitting Data sesuai spesifikasi:\n",
        "    1. Split data menjadi train dan test\n",
        "    2. Rasio 70:30 atau 80:20 berdasarkan artikel penelitian\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.splits_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"‚úÇÔ∏è ii. SPLITTING DATA\")\n",
        "        print(f\"Input vectors: {self.vectors_dir}\")\n",
        "        print(f\"Output splits: {self.splits_dir}\")\n",
        "\n",
        "        # Data storage\n",
        "        self.tfidf_data = None\n",
        "        self.bert_data = None\n",
        "        self.case_ids = []\n",
        "\n",
        "        # Split configurations berdasarkan artikel penelitian\n",
        "        self.split_ratios = {\n",
        "            \"70_30\": 0.3,  # 70:30\n",
        "            \"80_20\": 0.2,  # 80:20 (lebih umum)\n",
        "        }\n",
        "        self.random_state = 42\n",
        "\n",
        "    def load_vectors(self) -> bool:\n",
        "        \"\"\"Load vectors yang sudah dibuat dari tahap sebelumnya\"\"\"\n",
        "        print(\"\\nüì• Loading vectors from previous step...\")\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            logger.error(f\"Vectors directory not found: {self.vectors_dir}\")\n",
        "            return False\n",
        "\n",
        "        # Find latest vector files\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir) if f.endswith('.pkl')]\n",
        "\n",
        "        if not vector_files:\n",
        "            logger.error(\"No vector files found\")\n",
        "            return False\n",
        "\n",
        "        # Load TF-IDF vectors\n",
        "        tfidf_files = [f for f in vector_files if 'tfidf' in f]\n",
        "        if tfidf_files:\n",
        "            latest_tfidf = max(tfidf_files)\n",
        "            tfidf_path = os.path.join(self.vectors_dir, latest_tfidf)\n",
        "\n",
        "            try:\n",
        "                with open(tfidf_path, 'rb') as f:\n",
        "                    self.tfidf_data = pickle.load(f)\n",
        "\n",
        "                self.case_ids = self.tfidf_data['case_ids']\n",
        "                print(f\"‚úÖ TF-IDF vectors loaded: {self.tfidf_data['vectors'].shape}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading TF-IDF vectors: {e}\")\n",
        "\n",
        "        # Load BERT vectors\n",
        "        bert_files = [f for f in vector_files if f.startswith('bert_vectors_')]\n",
        "        if bert_files:\n",
        "            latest_bert = max(bert_files)\n",
        "            bert_path = os.path.join(self.vectors_dir, latest_bert)\n",
        "\n",
        "            try:\n",
        "                with open(bert_path, 'rb') as f:\n",
        "                    self.bert_data = pickle.load(f)\n",
        "\n",
        "                if not self.case_ids:  # If not loaded from TF-IDF\n",
        "                    self.case_ids = self.bert_data['case_ids']\n",
        "\n",
        "                print(f\"‚úÖ BERT vectors loaded: {self.bert_data['vectors'].shape}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading BERT vectors: {e}\")\n",
        "\n",
        "        print(f\"üìä Total cases loaded: {len(self.case_ids)}\")\n",
        "        return len(self.case_ids) > 0\n",
        "\n",
        "    def create_labels_for_stratification(self) -> np.ndarray:\n",
        "        \"\"\"Buat labels untuk stratified splitting jika diperlukan\"\"\"\n",
        "        print(\"üè∑Ô∏è Creating labels for stratified splitting...\")\n",
        "\n",
        "        # Strategy: Use case metadata for stratification\n",
        "        if self.tfidf_data and 'cases_metadata' in self.tfidf_data:\n",
        "            cases_df = self.tfidf_data['cases_metadata']\n",
        "\n",
        "            labels = []\n",
        "            for case_id in self.case_ids:\n",
        "                case_row = cases_df[cases_df['nama_file'].str.replace('.txt', '') == case_id]\n",
        "\n",
        "                if len(case_row) > 0:\n",
        "                    row = case_row.iloc[0]\n",
        "\n",
        "                    # Create label based on case type\n",
        "                    if pd.notna(row.get('jenis_perkara')):\n",
        "                        jenis = str(row['jenis_perkara']).lower()\n",
        "                        if 'pidana' in jenis:\n",
        "                            if 'perdagangan orang' in jenis:\n",
        "                                labels.append('pidana_perdagangan_orang')\n",
        "                            else:\n",
        "                                labels.append('pidana_umum')\n",
        "                        elif 'perdata' in jenis:\n",
        "                            labels.append('perdata')\n",
        "                        else:\n",
        "                            labels.append('lainnya')\n",
        "                    else:\n",
        "                        labels.append('unknown')\n",
        "                else:\n",
        "                    labels.append('unknown')\n",
        "\n",
        "            # Convert to numeric labels\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            label_encoder = LabelEncoder()\n",
        "            numeric_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "            # Check if we have enough samples per class for stratification\n",
        "            unique_labels, counts = np.unique(numeric_labels, return_counts=True)\n",
        "            min_samples = min(counts)\n",
        "\n",
        "            if min_samples >= 2:  # Minimum for train/test split\n",
        "                print(f\"‚úÖ Stratification possible. Classes: {len(unique_labels)}, Min samples: {min_samples}\")\n",
        "                return numeric_labels, label_encoder\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Not enough samples per class for stratification. Min: {min_samples}\")\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def create_split(self, test_size: float, split_name: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Buat train-test split dengan rasio tertentu\n",
        "        Args:\n",
        "            test_size: float - Ukuran test set (0.2 untuk 80:20, 0.3 untuk 70:30)\n",
        "            split_name: str - Nama split untuk identifikasi\n",
        "        \"\"\"\n",
        "        print(f\"\\n‚úÇÔ∏è Creating {split_name} split (test_size={test_size})...\")\n",
        "\n",
        "        n_samples = len(self.case_ids)\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "        # Try stratified split\n",
        "        labels, label_encoder = self.create_labels_for_stratification()\n",
        "\n",
        "        try:\n",
        "            if labels is not None:\n",
        "                # Stratified split\n",
        "                train_indices, test_indices = train_test_split(\n",
        "                    indices,\n",
        "                    test_size=test_size,\n",
        "                    random_state=self.random_state,\n",
        "                    stratify=labels,\n",
        "                    shuffle=True\n",
        "                )\n",
        "                print(f\"üìä Using stratified split\")\n",
        "            else:\n",
        "                # Random split\n",
        "                train_indices, test_indices = train_test_split(\n",
        "                    indices,\n",
        "                    test_size=test_size,\n",
        "                    random_state=self.random_state,\n",
        "                    shuffle=True\n",
        "                )\n",
        "                print(f\"üé≤ Using random split\")\n",
        "\n",
        "            # Create split data\n",
        "            split_data = {\n",
        "                'split_name': split_name,\n",
        "                'test_size': test_size,\n",
        "                'train_size': 1 - test_size,\n",
        "                'total_samples': n_samples,\n",
        "                'train_indices': train_indices,\n",
        "                'test_indices': test_indices,\n",
        "                'train_case_ids': [self.case_ids[i] for i in train_indices],\n",
        "                'test_case_ids': [self.case_ids[i] for i in test_indices],\n",
        "                'stratified': labels is not None,\n",
        "                'random_state': self.random_state,\n",
        "                'label_encoder': label_encoder\n",
        "            }\n",
        "\n",
        "            # Add vector splits\n",
        "            if self.tfidf_data:\n",
        "                tfidf_vectors = self.tfidf_data['vectors']\n",
        "                split_data['train_tfidf'] = tfidf_vectors[train_indices]\n",
        "                split_data['test_tfidf'] = tfidf_vectors[test_indices]\n",
        "\n",
        "            if self.bert_data:\n",
        "                bert_vectors = self.bert_data['vectors']\n",
        "                split_data['train_bert'] = bert_vectors[train_indices]\n",
        "                split_data['test_bert'] = bert_vectors[test_indices]\n",
        "\n",
        "            # Add label splits if available\n",
        "            if labels is not None:\n",
        "                split_data['train_labels'] = labels[train_indices]\n",
        "                split_data['test_labels'] = labels[test_indices]\n",
        "\n",
        "            print(f\"‚úÖ {split_name} split created:\")\n",
        "            print(f\"   üìö Training: {len(train_indices)} cases ({len(train_indices)/n_samples:.1%})\")\n",
        "            print(f\"   üß™ Testing: {len(test_indices)} cases ({len(test_indices)/n_samples:.1%})\")\n",
        "\n",
        "            return split_data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating {split_name} split: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_multiple_splits(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Buat multiple splits dengan rasio berbeda sesuai spesifikasi:\n",
        "        - 70:30 berdasarkan artikel penelitian\n",
        "        - 80:20 berdasarkan artikel penelitian\n",
        "        \"\"\"\n",
        "        print(\"\\nüîÑ Creating multiple splits based on research articles...\")\n",
        "\n",
        "        all_splits = {}\n",
        "\n",
        "        for split_name, test_size in self.split_ratios.items():\n",
        "            print(f\"\\nüìä Creating {split_name} split...\")\n",
        "\n",
        "            split_data = self.create_split(test_size, split_name)\n",
        "            if split_data:\n",
        "                all_splits[split_name] = split_data\n",
        "\n",
        "        return all_splits\n",
        "\n",
        "    def save_splits(self, splits_data: Dict) -> Dict[str, str]:\n",
        "        \"\"\"Simpan splits data ke file\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        saved_files = {}\n",
        "\n",
        "        print(\"\\nüíæ Saving splits data...\")\n",
        "\n",
        "        # Save main splits\n",
        "        splits_filename = f\"data_splits_{timestamp}.pkl\"\n",
        "        splits_path = os.path.join(self.splits_dir, splits_filename)\n",
        "\n",
        "        # Include original vectors data for reference\n",
        "        complete_splits_data = {\n",
        "            'splits': splits_data,\n",
        "            'tfidf_vectorizer': self.tfidf_data['vectorizer'] if self.tfidf_data else None,\n",
        "            'bert_model_name': self.bert_data['model_name'] if self.bert_data else None,\n",
        "            'all_case_ids': self.case_ids,\n",
        "            'split_info': {\n",
        "                'total_cases': len(self.case_ids),\n",
        "                'splits_created': list(splits_data.keys()),\n",
        "                'created_at': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(splits_path, 'wb') as f:\n",
        "            pickle.dump(complete_splits_data, f)\n",
        "\n",
        "        saved_files['splits'] = splits_path\n",
        "        print(f\"üìÑ Data splits saved: {splits_filename}\")\n",
        "\n",
        "        # Save split summary\n",
        "        summary_filename = f\"split_summary_{timestamp}.json\"\n",
        "        summary_path = os.path.join(self.splits_dir, summary_filename)\n",
        "\n",
        "        summary_data = {\n",
        "            'total_cases': len(self.case_ids),\n",
        "            'splits_created': list(splits_data.keys()),\n",
        "            'random_state': self.random_state,\n",
        "            'created_at': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Add split details\n",
        "        for split_name, split_data in splits_data.items():\n",
        "            summary_data[f'{split_name}_train'] = len(split_data['train_case_ids'])\n",
        "            summary_data[f'{split_name}_test'] = len(split_data['test_case_ids'])\n",
        "            summary_data[f'{split_name}_stratified'] = split_data['stratified']\n",
        "\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            import json\n",
        "            json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        saved_files['summary'] = summary_path\n",
        "        print(f\"üìã Split summary saved: {summary_filename}\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "    def validate_splits(self, splits_data: Dict) -> bool:\n",
        "        \"\"\"Validasi splits data\"\"\"\n",
        "        print(\"\\nüîç Validating splits...\")\n",
        "\n",
        "        all_valid = True\n",
        "\n",
        "        for split_name, split_data in splits_data.items():\n",
        "            print(f\"\\nüìä Validating {split_name}:\")\n",
        "\n",
        "            train_ids = set(split_data['train_case_ids'])\n",
        "            test_ids = set(split_data['test_case_ids'])\n",
        "\n",
        "            # Check no overlap\n",
        "            overlap = train_ids.intersection(test_ids)\n",
        "            if overlap:\n",
        "                print(f\"‚ùå Overlap found: {len(overlap)} cases\")\n",
        "                all_valid = False\n",
        "            else:\n",
        "                print(f\"‚úÖ No overlap between train and test\")\n",
        "\n",
        "            # Check completeness\n",
        "            total_split = len(train_ids) + len(test_ids)\n",
        "            total_original = len(self.case_ids)\n",
        "            if total_split != total_original:\n",
        "                print(f\"‚ùå Size mismatch: {total_split} vs {total_original}\")\n",
        "                all_valid = False\n",
        "            else:\n",
        "                print(f\"‚úÖ Complete split: {total_split} cases\")\n",
        "\n",
        "            # Check vector dimensions if available\n",
        "            if 'train_tfidf' in split_data and 'test_tfidf' in split_data:\n",
        "                train_shape = split_data['train_tfidf'].shape\n",
        "                test_shape = split_data['test_tfidf'].shape\n",
        "                if train_shape[1] != test_shape[1]:\n",
        "                    print(f\"‚ùå TF-IDF dimension mismatch: {train_shape[1]} vs {test_shape[1]}\")\n",
        "                    all_valid = False\n",
        "                else:\n",
        "                    print(f\"‚úÖ TF-IDF dimensions match: {train_shape[1]} features\")\n",
        "\n",
        "        if all_valid:\n",
        "            print(f\"\\n‚úÖ All splits are valid!\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå Some splits have validation issues!\")\n",
        "\n",
        "        return all_valid\n",
        "\n",
        "    def process_splitting_data(self) -> bool:\n",
        "        \"\"\"\n",
        "        Proses lengkap splitting data sesuai spesifikasi:\n",
        "        1. Load vectors dari tahap sebelumnya\n",
        "        2. Buat splits dengan rasio 70:30 dan 80:20\n",
        "        3. Validasi dan simpan splits\n",
        "        \"\"\"\n",
        "        print(\"‚úÇÔ∏è ii. SPLITTING DATA\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Split data untuk train dan test\")\n",
        "        print(\"2. Rasio 70:30 atau 80:20 berdasarkan artikel penelitian\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Load vectors\n",
        "        if not self.load_vectors():\n",
        "            print(\"‚ùå Failed to load vectors\")\n",
        "            return False\n",
        "\n",
        "        # 2. Create multiple splits berdasarkan artikel penelitian\n",
        "        splits_data = self.create_multiple_splits()\n",
        "\n",
        "        if not splits_data:\n",
        "            print(\"‚ùå Failed to create splits\")\n",
        "            return False\n",
        "\n",
        "        # 3. Validate splits\n",
        "        self.validate_splits(splits_data)\n",
        "\n",
        "        # 4. Save splits\n",
        "        saved_files = self.save_splits(splits_data)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"‚úÖ ii. SPLITTING DATA COMPLETED!\")\n",
        "        print(f\"üìä Splits created: {list(splits_data.keys())}\")\n",
        "        print(f\"üìÅ Total cases: {len(self.case_ids)}\")\n",
        "\n",
        "        # Show split details\n",
        "        for split_name, split_data in splits_data.items():\n",
        "            train_size = len(split_data['train_case_ids'])\n",
        "            test_size = len(split_data['test_case_ids'])\n",
        "            print(f\"   {split_name}: {train_size} train, {test_size} test\")\n",
        "\n",
        "        print(f\"üíæ Files saved to: {self.splits_dir}\")\n",
        "        print(\"Langkah selanjutnya: iii. Model Retrieval\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk splitting data\"\"\"\n",
        "    print(\"üöÄ MULAI ii. SPLITTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        splitter = SplittingData()\n",
        "        success = splitter.process_splitting_data()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\nüéâ SPLITTING DATA BERHASIL!\")\n",
        "            print(\"‚ú® Yang telah dilakukan:\")\n",
        "            print(\"  ‚úÖ Load vectors dari tahap i. Representasi Vektor\")\n",
        "            print(\"  ‚úÖ Split data dengan rasio 70:30 dan 80:20\")\n",
        "            print(\"  ‚úÖ Stratified splitting jika memungkinkan\")\n",
        "            print(\"  ‚úÖ Validasi splits untuk memastikan tidak ada overlap\")\n",
        "            print(\"  ‚úÖ Simpan splits untuk tahap selanjutnya\")\n",
        "            print(\"Langkah selanjutnya: iii. Model Retrieval\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Splitting data gagal.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJz79Wn8NGzd"
      },
      "source": [
        "## MODEL RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7CtWgBoNDBd",
        "outputId": "ac9128fa-c8d6-4b88-e39e-f1756a4ba15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ MULAI iii. MODEL RETRIEVAL (TF-IDF Based)\n",
            "======================================================================\n",
            "ü§ñ iii. MODEL RETRIEVAL (TF-IDF Based)\n",
            "Input splits: /content/drive/MyDrive/perdagangan_orang/data/splits\n",
            "Output models: /content/drive/MyDrive/perdagangan_orang/data/models\n",
            "ü§ñ iii. MODEL RETRIEVAL (TF-IDF Based)\n",
            "============================================================\n",
            "- Support Vector Machine (SVM) pada TF-IDF\n",
            "- Naive Bayes pada TF-IDF\n",
            "- Additional ML models untuk comparison\n",
            "============================================================\n",
            "\n",
            "üì• Loading splits data...\n",
            "‚úÖ Splits loaded from: data_splits_20250625_130652.pkl\n",
            "üìä Available splits: ['70_30', '80_20']\n",
            "\n",
            "üìã Preparing training data for 80_20 split...\n",
            "üìä TF-IDF vectors: train (63, 4489), test (16, 4489)\n",
            "üè∑Ô∏è Labels: 4 classes\n",
            "‚úÖ Training data prepared:\n",
            "   üìö Training: 63 cases\n",
            "   üß™ Testing: 16 cases\n",
            "\n",
            "üîß Training SVM models on TF-IDF...\n",
            "   Scaling features for SVM...\n",
            "   Training svm_rbf...\n",
            "      ‚úÖ svm_rbf: Accuracy=0.875, F1=0.820\n",
            "         Best params: {'C': 10.0, 'gamma': 'auto'}\n",
            "   Training svm_linear...\n",
            "      ‚úÖ svm_linear: Accuracy=0.875, F1=0.820\n",
            "         Best params: {'C': 0.1}\n",
            "   Training svm_poly...\n",
            "      ‚úÖ svm_poly: Accuracy=0.875, F1=0.820\n",
            "         Best params: {'C': 10.0, 'gamma': 'scale'}\n",
            "\n",
            "üîß Training Naive Bayes models on TF-IDF...\n",
            "   Training naive_bayes_multinomial...\n",
            "      ‚úÖ naive_bayes_multinomial: Accuracy=0.875, F1=0.820\n",
            "         Best params: {'alpha': 0.5}\n",
            "\n",
            "üîß Training additional ML models...\n",
            "   Training logistic_regression...\n",
            "      ‚úÖ logistic_regression: Accuracy=0.875, F1=0.820\n",
            "   Training random_forest...\n",
            "      ‚úÖ random_forest: Accuracy=0.875, F1=0.820\n",
            "\n",
            "üîç Creating retrieval system...\n",
            "   Best performing model: svm_rbf (F1: 0.820)\n",
            "   ‚úÖ Retrieval system created with 6 models\n",
            "\n",
            "üíæ Saving trained models...\n",
            "üîß All models saved: tfidf_models_20250625_131636.pkl\n",
            "üìã Evaluation summary saved: models_evaluation_20250625_131636.json\n",
            "\n",
            "============================================================\n",
            "‚úÖ iii. MODEL RETRIEVAL COMPLETED!\n",
            "üîß SVM models: ‚úÖ\n",
            "üìä Naive Bayes: ‚úÖ\n",
            "‚ûï Additional models: ‚úÖ\n",
            "üîç Retrieval system: ‚úÖ\n",
            "üìÅ Total models: 6\n",
            "üíæ Files saved to: /content/drive/MyDrive/perdagangan_orang/data/models\n",
            "Langkah selanjutnya: iv. Fungsi Retrieval\n",
            "============================================================\n",
            "\n",
            "üéâ MODEL RETRIEVAL BERHASIL!\n",
            "‚ú® Yang telah dilakukan:\n",
            "  ‚úÖ Load splits data dari tahap ii. Splitting Data\n",
            "  ‚úÖ Train SVM models (RBF, Linear, Polynomial) pada TF-IDF\n",
            "  ‚úÖ Train Naive Bayes model pada TF-IDF\n",
            "  ‚úÖ Train additional models (Logistic Regression, Random Forest)\n",
            "  ‚úÖ Create retrieval system dengan best performing model\n",
            "  ‚úÖ Simpan semua models untuk tahap selanjutnya\n",
            "Langkah selanjutnya: iv. Fungsi Retrieval\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# iii. MODEL RETRIEVAL\n",
        "# Gunakan model machine learning seperti Support Vector Machine (SVM) atau Naive Bayes\n",
        "# pada representasi TF-IDF untuk classification/retrieval.\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "import json\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ModelRetrieval:\n",
        "    \"\"\"\n",
        "    iii. Model Retrieval menggunakan:\n",
        "    - Support Vector Machine (SVM) pada TF-IDF untuk classification/retrieval\n",
        "    - Naive Bayes pada TF-IDF untuk classification/retrieval\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "        self.models_dir = os.path.join(base_dir, \"data\", \"models\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.models_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"ü§ñ iii. MODEL RETRIEVAL (TF-IDF Based)\")\n",
        "        print(f\"Input splits: {self.splits_dir}\")\n",
        "        print(f\"Output models: {self.models_dir}\")\n",
        "\n",
        "        # Model storage\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.evaluation_results = {}\n",
        "\n",
        "        # Data storage\n",
        "        self.splits_data = None\n",
        "        self.train_data = {}\n",
        "        self.test_data = {}\n",
        "        self.tfidf_vectorizer = None\n",
        "\n",
        "    def load_splits_data(self) -> bool:\n",
        "        \"\"\"Load splits data dari tahap sebelumnya\"\"\"\n",
        "        print(\"\\nüì• Loading splits data...\")\n",
        "\n",
        "        # Find latest split file\n",
        "        if not os.path.exists(self.splits_dir):\n",
        "            logger.error(f\"Splits directory not found: {self.splits_dir}\")\n",
        "            return False\n",
        "\n",
        "        split_files = [f for f in os.listdir(self.splits_dir)\n",
        "                      if f.startswith('data_splits_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not split_files:\n",
        "            logger.error(\"No split files found\")\n",
        "            return False\n",
        "\n",
        "        latest_split = max(split_files)\n",
        "        split_path = os.path.join(self.splits_dir, latest_split)\n",
        "\n",
        "        try:\n",
        "            with open(split_path, 'rb') as f:\n",
        "                complete_data = pickle.load(f)\n",
        "\n",
        "            self.splits_data = complete_data['splits']\n",
        "            self.tfidf_vectorizer = complete_data.get('tfidf_vectorizer')\n",
        "\n",
        "            print(f\"‚úÖ Splits loaded from: {latest_split}\")\n",
        "            print(f\"üìä Available splits: {list(self.splits_data.keys())}\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading splits: {e}\")\n",
        "            return False\n",
        "\n",
        "    def prepare_training_data(self, split_name: str = \"80_20\") -> bool:\n",
        "        \"\"\"Siapkan data untuk training dari split tertentu\"\"\"\n",
        "        print(f\"\\nüìã Preparing training data for {split_name} split...\")\n",
        "\n",
        "        if split_name not in self.splits_data:\n",
        "            logger.error(f\"Split {split_name} not found\")\n",
        "            return False\n",
        "\n",
        "        split_info = self.splits_data[split_name]\n",
        "\n",
        "        # Extract training and testing data\n",
        "        self.train_data = {\n",
        "            'case_ids': split_info['train_case_ids'],\n",
        "            'indices': split_info['train_indices']\n",
        "        }\n",
        "\n",
        "        self.test_data = {\n",
        "            'case_ids': split_info['test_case_ids'],\n",
        "            'indices': split_info['test_indices']\n",
        "        }\n",
        "\n",
        "        # Add TF-IDF vectors (required for this implementation)\n",
        "        if 'train_tfidf' in split_info:\n",
        "            self.train_data['tfidf'] = split_info['train_tfidf']\n",
        "            self.test_data['tfidf'] = split_info['test_tfidf']\n",
        "            print(f\"üìä TF-IDF vectors: train {self.train_data['tfidf'].shape}, test {self.test_data['tfidf'].shape}\")\n",
        "        else:\n",
        "            logger.error(\"TF-IDF vectors not found in splits data\")\n",
        "            return False\n",
        "\n",
        "        # Handle labels\n",
        "        if 'train_labels' in split_info:\n",
        "            self.train_data['labels'] = split_info['train_labels']\n",
        "            self.test_data['labels'] = split_info['test_labels']\n",
        "            self.label_encoder = split_info['label_encoder']\n",
        "            print(f\"üè∑Ô∏è Labels: {len(np.unique(self.train_data['labels']))} classes\")\n",
        "        else:\n",
        "            # Create synthetic labels using clustering\n",
        "            print(\"üìä Creating synthetic labels using TF-IDF clustering...\")\n",
        "            self._create_synthetic_labels()\n",
        "\n",
        "        print(f\"‚úÖ Training data prepared:\")\n",
        "        print(f\"   üìö Training: {len(self.train_data['case_ids'])} cases\")\n",
        "        print(f\"   üß™ Testing: {len(self.test_data['case_ids'])} cases\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _create_synthetic_labels(self) -> None:\n",
        "        \"\"\"Buat synthetic labels menggunakan clustering pada TF-IDF\"\"\"\n",
        "        print(\"   Creating synthetic labels using K-Means clustering...\")\n",
        "\n",
        "        X_train = self.train_data['tfidf']\n",
        "        X_test = self.test_data['tfidf']\n",
        "\n",
        "        # Determine optimal number of clusters (3-10 clusters)\n",
        "        n_clusters = min(8, max(3, len(self.train_data['case_ids']) // 20))\n",
        "\n",
        "        # Apply K-Means clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        train_labels = kmeans.fit_predict(X_train)\n",
        "        test_labels = kmeans.predict(X_test)\n",
        "\n",
        "        self.train_data['labels'] = train_labels\n",
        "        self.test_data['labels'] = test_labels\n",
        "\n",
        "        # Create label encoder for consistency\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(train_labels)\n",
        "\n",
        "        print(f\"   ‚úÖ Created {n_clusters} synthetic clusters as labels\")\n",
        "\n",
        "    def train_svm_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Train Support Vector Machine (SVM) pada representasi TF-IDF\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Training SVM models on TF-IDF...\")\n",
        "\n",
        "        X_train = self.train_data['tfidf']\n",
        "        X_test = self.test_data['tfidf']\n",
        "        y_train = self.train_data['labels']\n",
        "        y_test = self.test_data['labels']\n",
        "\n",
        "        # Scale features for SVM\n",
        "        print(\"   Scaling features for SVM...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train.toarray())\n",
        "        X_test_scaled = scaler.transform(X_test.toarray())\n",
        "        self.scalers['svm_tfidf'] = scaler\n",
        "\n",
        "        try:\n",
        "            # Define SVM models with different kernels\n",
        "            svm_configs = {\n",
        "                'svm_rbf': {\n",
        "                    'model': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "                    'params': {'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']}\n",
        "                },\n",
        "                'svm_linear': {\n",
        "                    'model': SVC(kernel='linear', probability=True, random_state=42),\n",
        "                    'params': {'C': [0.1, 1.0, 10.0]}\n",
        "                },\n",
        "                'svm_poly': {\n",
        "                    'model': SVC(kernel='poly', degree=3, probability=True, random_state=42),\n",
        "                    'params': {'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']}\n",
        "                }\n",
        "            }\n",
        "\n",
        "            for model_name, config in svm_configs.items():\n",
        "                print(f\"   Training {model_name}...\")\n",
        "\n",
        "                # Grid search for best parameters\n",
        "                grid_search = GridSearchCV(\n",
        "                    config['model'],\n",
        "                    config['params'],\n",
        "                    cv=3,\n",
        "                    scoring='f1_weighted',\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "\n",
        "                grid_search.fit(X_train_scaled, y_train)\n",
        "                best_model = grid_search.best_estimator_\n",
        "\n",
        "                # Make predictions\n",
        "                y_pred = best_model.predict(X_test_scaled)\n",
        "                y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                evaluation = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'best_params': grid_search.best_params_,\n",
        "                    'model_type': 'SVM',\n",
        "                    'feature_type': 'TF-IDF',\n",
        "                    'kernel': config['model'].kernel,\n",
        "                    'classification_report': classification_report(y_test, y_pred)\n",
        "                }\n",
        "\n",
        "                self.models[model_name] = {\n",
        "                    'model': best_model,\n",
        "                    'scaler': scaler,\n",
        "                    'evaluation': evaluation,\n",
        "                    'predictions': y_pred,\n",
        "                    'probabilities': y_pred_proba\n",
        "                }\n",
        "\n",
        "                print(f\"      ‚úÖ {model_name}: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
        "                print(f\"         Best params: {grid_search.best_params_}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training SVM: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_naive_bayes_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Train Naive Bayes pada representasi TF-IDF\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Training Naive Bayes models on TF-IDF...\")\n",
        "\n",
        "        X_train = self.train_data['tfidf']\n",
        "        X_test = self.test_data['tfidf']\n",
        "        y_train = self.train_data['labels']\n",
        "        y_test = self.test_data['labels']\n",
        "\n",
        "        try:\n",
        "            # Define Naive Bayes models\n",
        "            nb_configs = {\n",
        "                'naive_bayes_multinomial': {\n",
        "                    'model': MultinomialNB(),\n",
        "                    'params': {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
        "                }\n",
        "            }\n",
        "\n",
        "            for model_name, config in nb_configs.items():\n",
        "                print(f\"   Training {model_name}...\")\n",
        "\n",
        "                # Grid search for best parameters\n",
        "                grid_search = GridSearchCV(\n",
        "                    config['model'],\n",
        "                    config['params'],\n",
        "                    cv=3,\n",
        "                    scoring='f1_weighted',\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                best_model = grid_search.best_estimator_\n",
        "\n",
        "                # Make predictions\n",
        "                y_pred = best_model.predict(X_test)\n",
        "                y_pred_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                evaluation = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'best_params': grid_search.best_params_,\n",
        "                    'model_type': 'Naive Bayes',\n",
        "                    'feature_type': 'TF-IDF',\n",
        "                    'classification_report': classification_report(y_test, y_pred)\n",
        "                }\n",
        "\n",
        "                self.models[model_name] = {\n",
        "                    'model': best_model,\n",
        "                    'evaluation': evaluation,\n",
        "                    'predictions': y_pred,\n",
        "                    'probabilities': y_pred_proba\n",
        "                }\n",
        "\n",
        "                print(f\"      ‚úÖ {model_name}: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
        "                print(f\"         Best params: {grid_search.best_params_}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training Naive Bayes: {e}\")\n",
        "            return False\n",
        "\n",
        "    def train_additional_models(self) -> bool:\n",
        "        \"\"\"\n",
        "        Train additional ML models for comparison\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Training additional ML models...\")\n",
        "\n",
        "        X_train = self.train_data['tfidf']\n",
        "        X_test = self.test_data['tfidf']\n",
        "        y_train = self.train_data['labels']\n",
        "        y_test = self.test_data['labels']\n",
        "\n",
        "        try:\n",
        "            # Additional models\n",
        "            additional_configs = {\n",
        "                'logistic_regression': {\n",
        "                    'model': LogisticRegression(random_state=42, max_iter=1000),\n",
        "                    'params': {'C': [0.1, 1.0, 10.0], 'solver': ['liblinear', 'lbfgs']},\n",
        "                    'scale': True\n",
        "                },\n",
        "                'random_forest': {\n",
        "                    'model': RandomForestClassifier(random_state=42),\n",
        "                    'params': {'n_estimators': [50, 100], 'max_depth': [10, 20, None]},\n",
        "                    'scale': False\n",
        "                }\n",
        "            }\n",
        "\n",
        "            for model_name, config in additional_configs.items():\n",
        "                print(f\"   Training {model_name}...\")\n",
        "\n",
        "                # Prepare data\n",
        "                if config['scale']:\n",
        "                    if 'additional_scaler' not in self.scalers:\n",
        "                        self.scalers['additional_scaler'] = StandardScaler()\n",
        "                        X_train_prep = self.scalers['additional_scaler'].fit_transform(X_train.toarray())\n",
        "                    else:\n",
        "                        X_train_prep = self.scalers['additional_scaler'].transform(X_train.toarray())\n",
        "                    X_test_prep = self.scalers['additional_scaler'].transform(X_test.toarray())\n",
        "                else:\n",
        "                    X_train_prep = X_train\n",
        "                    X_test_prep = X_test\n",
        "\n",
        "                # Grid search\n",
        "                grid_search = GridSearchCV(\n",
        "                    config['model'],\n",
        "                    config['params'],\n",
        "                    cv=3,\n",
        "                    scoring='f1_weighted',\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "\n",
        "                grid_search.fit(X_train_prep, y_train)\n",
        "                best_model = grid_search.best_estimator_\n",
        "\n",
        "                # Predictions\n",
        "                y_pred = best_model.predict(X_test_prep)\n",
        "                y_pred_proba = best_model.predict_proba(X_test_prep)\n",
        "\n",
        "                # Metrics\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "\n",
        "                evaluation = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'best_params': grid_search.best_params_,\n",
        "                    'model_type': model_name.replace('_', ' ').title(),\n",
        "                    'feature_type': 'TF-IDF',\n",
        "                    'classification_report': classification_report(y_test, y_pred)\n",
        "                }\n",
        "\n",
        "                self.models[model_name] = {\n",
        "                    'model': best_model,\n",
        "                    'evaluation': evaluation,\n",
        "                    'predictions': y_pred,\n",
        "                    'probabilities': y_pred_proba,\n",
        "                    'scaled': config['scale']\n",
        "                }\n",
        "\n",
        "                print(f\"      ‚úÖ {model_name}: Accuracy={accuracy:.3f}, F1={f1:.3f}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training additional models: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_retrieval_system(self) -> bool:\n",
        "        \"\"\"\n",
        "        Buat retrieval system menggunakan trained models\n",
        "        \"\"\"\n",
        "        print(\"\\nüîç Creating retrieval system...\")\n",
        "\n",
        "        if not self.models:\n",
        "            print(\"‚ö†Ô∏è No trained models available for retrieval\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Find best performing model\n",
        "            best_model_name = None\n",
        "            best_f1 = 0\n",
        "\n",
        "            for model_name, model_info in self.models.items():\n",
        "                f1_score = model_info['evaluation']['f1']\n",
        "                if f1_score > best_f1:\n",
        "                    best_f1 = f1_score\n",
        "                    best_model_name = model_name\n",
        "\n",
        "            print(f\"   Best performing model: {best_model_name} (F1: {best_f1:.3f})\")\n",
        "\n",
        "            # Create retrieval system info\n",
        "            retrieval_system = {\n",
        "                'best_model_name': best_model_name,\n",
        "                'best_model': self.models[best_model_name],\n",
        "                'all_models': list(self.models.keys()),\n",
        "                'tfidf_vectorizer': self.tfidf_vectorizer,\n",
        "                'train_case_ids': self.train_data['case_ids'],\n",
        "                'test_case_ids': self.test_data['case_ids'],\n",
        "                'train_tfidf': self.train_data['tfidf'],\n",
        "                'test_tfidf': self.test_data['tfidf'],\n",
        "                'scalers': self.scalers\n",
        "            }\n",
        "\n",
        "            self.models['retrieval_system'] = retrieval_system\n",
        "            print(f\"   ‚úÖ Retrieval system created with {len(self.models)-1} models\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating retrieval system: {e}\")\n",
        "            return False\n",
        "\n",
        "    def save_models(self) -> Dict[str, str]:\n",
        "        \"\"\"Simpan semua trained models\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        saved_files = {}\n",
        "\n",
        "        print(\"\\nüíæ Saving trained models...\")\n",
        "\n",
        "        try:\n",
        "            # Save all models\n",
        "            models_filename = f\"tfidf_models_{timestamp}.pkl\"\n",
        "            models_path = os.path.join(self.models_dir, models_filename)\n",
        "\n",
        "            models_data = {\n",
        "                'models': self.models,\n",
        "                'scalers': self.scalers,\n",
        "                'tfidf_vectorizer': self.tfidf_vectorizer,\n",
        "                'train_data': {\n",
        "                    'case_ids': self.train_data['case_ids'],\n",
        "                    'indices': self.train_data['indices']\n",
        "                },\n",
        "                'test_data': {\n",
        "                    'case_ids': self.test_data['case_ids'],\n",
        "                    'indices': self.test_data['indices']\n",
        "                }\n",
        "            }\n",
        "\n",
        "            with open(models_path, 'wb') as f:\n",
        "                pickle.dump(models_data, f)\n",
        "\n",
        "            saved_files['models'] = models_path\n",
        "            print(f\"üîß All models saved: {models_filename}\")\n",
        "\n",
        "            # Save evaluation summary\n",
        "            summary_filename = f\"models_evaluation_{timestamp}.json\"\n",
        "            summary_path = os.path.join(self.models_dir, summary_filename)\n",
        "\n",
        "            # Prepare evaluation summary\n",
        "            evaluation_summary = {}\n",
        "            for model_name, model_info in self.models.items():\n",
        "                if model_name != 'retrieval_system' and 'evaluation' in model_info:\n",
        "                    eval_data = model_info['evaluation'].copy()\n",
        "                    # Remove classification report for JSON serialization\n",
        "                    if 'classification_report' in eval_data:\n",
        "                        del eval_data['classification_report']\n",
        "                    evaluation_summary[model_name] = eval_data\n",
        "\n",
        "            summary_data = {\n",
        "                'total_models': len(self.models) - 1,  # Exclude retrieval_system\n",
        "                'models_list': [k for k in self.models.keys() if k != 'retrieval_system'],\n",
        "                'evaluation_summary': evaluation_summary,\n",
        "                'best_model': self.models['retrieval_system']['best_model_name'] if 'retrieval_system' in self.models else None,\n",
        "                'training_completed_at': datetime.now().isoformat(),\n",
        "                'training_data_size': len(self.train_data['case_ids']),\n",
        "                'test_data_size': len(self.test_data['case_ids'])\n",
        "            }\n",
        "\n",
        "            with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            saved_files['summary'] = summary_path\n",
        "            print(f\"üìã Evaluation summary saved: {summary_filename}\")\n",
        "\n",
        "            return saved_files\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving models: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def process_model_retrieval(self) -> bool:\n",
        "        \"\"\"\n",
        "        Proses lengkap model retrieval menggunakan SVM dan Naive Bayes pada TF-IDF\n",
        "        \"\"\"\n",
        "        print(\"ü§ñ iii. MODEL RETRIEVAL (TF-IDF Based)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"- Support Vector Machine (SVM) pada TF-IDF\")\n",
        "        print(\"- Naive Bayes pada TF-IDF\")\n",
        "        print(\"- Additional ML models untuk comparison\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Load splits data\n",
        "        if not self.load_splits_data():\n",
        "            print(\"‚ùå Failed to load splits data\")\n",
        "            return False\n",
        "\n",
        "        # 2. Prepare training data\n",
        "        if not self.prepare_training_data(\"80_20\"):\n",
        "            print(\"‚ùå Failed to prepare training data\")\n",
        "            return False\n",
        "\n",
        "        # 3. Train SVM models\n",
        "        svm_success = self.train_svm_models()\n",
        "\n",
        "        # 4. Train Naive Bayes models\n",
        "        nb_success = self.train_naive_bayes_models()\n",
        "\n",
        "        # 5. Train additional models for comparison\n",
        "        additional_success = self.train_additional_models()\n",
        "\n",
        "        # 6. Create retrieval system\n",
        "        retrieval_success = self.create_retrieval_system()\n",
        "\n",
        "        if not (svm_success or nb_success):\n",
        "            print(\"‚ùå No models were trained successfully\")\n",
        "            return False\n",
        "\n",
        "        # 7. Save models\n",
        "        saved_files = self.save_models()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"‚úÖ iii. MODEL RETRIEVAL COMPLETED!\")\n",
        "        print(f\"üîß SVM models: {'‚úÖ' if svm_success else '‚ùå'}\")\n",
        "        print(f\"üìä Naive Bayes: {'‚úÖ' if nb_success else '‚ùå'}\")\n",
        "        print(f\"‚ûï Additional models: {'‚úÖ' if additional_success else '‚ùå'}\")\n",
        "        print(f\"üîç Retrieval system: {'‚úÖ' if retrieval_success else '‚ùå'}\")\n",
        "        print(f\"üìÅ Total models: {len(self.models)-1}\")\n",
        "        print(f\"üíæ Files saved to: {self.models_dir}\")\n",
        "        print(\"Langkah selanjutnya: iv. Fungsi Retrieval\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk model retrieval\"\"\"\n",
        "    print(\"üöÄ MULAI iii. MODEL RETRIEVAL (TF-IDF Based)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        model_trainer = ModelRetrieval()\n",
        "        success = model_trainer.process_model_retrieval()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\nüéâ MODEL RETRIEVAL BERHASIL!\")\n",
        "            print(\"‚ú® Yang telah dilakukan:\")\n",
        "            print(\"  ‚úÖ Load splits data dari tahap ii. Splitting Data\")\n",
        "            print(\"  ‚úÖ Train SVM models (RBF, Linear, Polynomial) pada TF-IDF\")\n",
        "            print(\"  ‚úÖ Train Naive Bayes model pada TF-IDF\")\n",
        "            print(\"  ‚úÖ Train additional models (Logistic Regression, Random Forest)\")\n",
        "            print(\"  ‚úÖ Create retrieval system dengan best performing model\")\n",
        "            print(\"  ‚úÖ Simpan semua models untuk tahap selanjutnya\")\n",
        "            print(\"Langkah selanjutnya: iv. Fungsi Retrieval\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Model retrieval gagal.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHdUAgtAQkip"
      },
      "source": [
        "## Fungsi Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar1kxFEoQmo-",
        "outputId": "e5c8f468-7457-4b95-9a4e-541845572458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error loading enhanced TF-IDF: name 'test_query' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ MULAI FIXED iv. FUNGSI RETRIEVAL\n",
            "======================================================================\n",
            "üîç FIXED iv. FUNGSI RETRIEVAL\n",
            "Models: /content/drive/MyDrive/perdagangan_orang/data/models\n",
            "Splits: /content/drive/MyDrive/perdagangan_orang/data/splits\n",
            "Vectors: /content/drive/MyDrive/perdagangan_orang/data/vectors\n",
            "\n",
            "üì• Loading all retrieval components (FIXED)...\n",
            "\n",
            "üìä Loading enhanced TF-IDF components...\n",
            "\n",
            "üîç Finding best tfidf vector file...\n",
            "   tfidf_vectors_enhanced_20250625_125040.pkl: 4,489 vocabulary\n",
            "‚úÖ Best tfidf file: tfidf_vectors_enhanced_20250625_125040.pkl\n",
            "   Vocabulary size: 4,489\n",
            "‚úÖ Enhanced TF-IDF loaded:\n",
            "   Vocabulary size: 4,489\n",
            "   Sample terms: ['00', '00 institusi_kejaksaan', '00 institusi_kejaksaan institusi_pengadilan', '00 institusi_pengadilan', '00 institusi_pengadilan institusi_mahkamah', '00 nominal_rp10000', '00 nominal_rp10000 00', '00 nominal_rp100000', '00 nominal_rp100000 00', '00 nominal_rp125000']\n",
            "   Legal terms found: ['perdagangan orang', 'terdakwa', 'hakim']\n",
            "   Legal terms missing: ['eksploitasi', 'perekrutan', 'pengangkutan', 'penampungan', 'pemindahan', 'penjualan', 'pemaksaan', 'penipuan', 'kekerasan', 'ancaman', 'jaksa', 'pengadilan', 'pasal', 'putusan', 'vonis', 'hukuman']\n",
            "\n",
            "üìä Loading case vectors from best source...\n",
            "\n",
            "üîç Finding best tfidf vector file...\n",
            "   tfidf_vectors_enhanced_20250625_125040.pkl: 4,489 vocabulary\n",
            "‚úÖ Best tfidf file: tfidf_vectors_enhanced_20250625_125040.pkl\n",
            "   Vocabulary size: 4,489\n",
            "‚úÖ TF-IDF vectors loaded from enhanced file:\n",
            "   Shape: (79, 4489)\n",
            "   Cases: 79\n",
            "   Converting sparse to dense matrix...\n",
            "   Dense shape: (79, 4489)\n",
            "\n",
            "üîç Finding best bert vector file...\n",
            "‚ùå No bert vector files found\n",
            "\n",
            "ü§ñ Loading trained models...\n",
            "‚ö†Ô∏è No trained models found\n",
            "\n",
            "ü§ñ Loading BERT components...\n",
            "‚úÖ BERT components loaded\n",
            "\n",
            "üìä Component loading summary:\n",
            "   TF-IDF vectorizer: ‚úÖ\n",
            "   Case vectors: ‚úÖ\n",
            "   ML models: ‚ùå\n",
            "   BERT: ‚úÖ\n",
            "   Total cases: 79\n",
            "‚úÖ Minimum required components loaded successfully\n",
            "üîç FIXED iv. FUNGSI RETRIEVAL\n",
            "============================================================\n",
            "PERBAIKAN: Prioritas enhanced vectors dengan vocabulary besar\n",
            "============================================================\n",
            "\n",
            "üß™ Testing FIXED retrieve() function...\n",
            "üìä Available methods: ['tfidf']\n",
            "\n",
            "üîç Query: 'kasus perdagangan orang lintas negara'\n",
            "üîç TF-IDF retrieval for 'kasus perdagangan orang lintas negara':\n",
            "   Query vector nnz: 5\n",
            "   Top scores: [0.11196465 0.08646215 0.08595986]\n",
            "   TFIDF: ['case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...']\n",
            "\n",
            "üîç Query: 'eksploitasi anak untuk tujuan prostitusi'\n",
            "üîç TF-IDF retrieval for 'eksploitasi anak untuk tujuan prostitusi':\n",
            "   Query vector nnz: 1\n",
            "   Top scores: [0.22007646 0.21768257 0.16661891]\n",
            "   TFIDF: ['case_2021_TK1_Putusa...', 'case_2025_TK1_Putusa...', 'case_2021_TK1_Putusa...']\n",
            "\n",
            "üîç Query: 'perekrutan dan pengangkutan korban perdagangan orang'\n",
            "üîç TF-IDF retrieval for 'perekrutan dan pengangkutan korban perdagangan orang':\n",
            "   Query vector nnz: 3\n",
            "   Top scores: [0.12958429 0.11445454 0.11378964]\n",
            "   TFIDF: ['case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...']\n",
            "\n",
            "üîç Query: 'pemindahan paksa perempuan ke luar negeri'\n",
            "üîç TF-IDF retrieval for 'pemindahan paksa perempuan ke luar negeri':\n",
            "   Query vector nnz: 2\n",
            "   Top scores: [0.15222437 0.12471453 0.11581419]\n",
            "   TFIDF: ['case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...']\n",
            "\n",
            "üîç Query: 'penipuan dan kekerasan dalam perdagangan manusia'\n",
            "üîç TF-IDF retrieval for 'penipuan dan kekerasan dalam perdagangan manusia':\n",
            "   Query vector nnz: 1\n",
            "   Top scores: [0.07570467 0.0668657  0.06647725]\n",
            "   TFIDF: ['case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...', 'case_2021_TK1_Putusa...']\n",
            "\n",
            "‚úÖ FIXED retrieve() function testing completed!\n",
            "\n",
            "============================================================\n",
            "‚úÖ FIXED iv. FUNGSI RETRIEVAL COMPLETED!\n",
            "üîç retrieve() function ready with ENHANCED vectors\n",
            "üìÅ Database size: 79 cases\n",
            "üìä TF-IDF vocabulary: 4,489 terms\n",
            "ü§ñ BERT available: ‚ùå\n",
            "üîß ML models: None\n",
            "============================================================\n",
            "\n",
            "üéâ FIXED FUNGSI RETRIEVAL BERHASIL!\n",
            "‚ú® Perbaikan yang diterapkan:\n",
            "  ‚úÖ Prioritas enhanced vectors dengan vocabulary terbesar\n",
            "  ‚úÖ Robust vector loading dengan multiple fallback\n",
            "  ‚úÖ Vocabulary debugging untuk troubleshooting\n",
            "  ‚úÖ Dense matrix conversion untuk cosine similarity\n",
            "  ‚úÖ Enhanced error handling dan logging\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# iv. FIXED FUNGSI RETRIEVAL\n",
        "# def retrieve(query: str, k: int = 5) -> List[case_id]:\n",
        "#     # 1) Pre-process query\n",
        "#     # 2) Hitung vektor query\n",
        "#     # 3) Hitung cosine‚Äêsimilarity dengan semua case vectors\n",
        "#     # 4) Kembalikan top-k case_id\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# BERT and Transformers\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    import torch\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Transformers not available. Install with: pip install transformers torch\")\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FixedFungsiRetrieval:\n",
        "    \"\"\"\n",
        "    FIXED iv. Fungsi Retrieval sesuai spesifikasi:\n",
        "\n",
        "    PERBAIKAN UTAMA:\n",
        "    - Prioritas gunakan enhanced vectors (vocabulary terbesar)\n",
        "    - Robust vector loading dengan fallback\n",
        "    - Vocabulary debugging untuk query troubleshooting\n",
        "\n",
        "    Implementasi fungsi retrieve() dengan langkah:\n",
        "    1) Pre-process query\n",
        "    2) Hitung vektor query\n",
        "    3) Hitung cosine‚Äêsimilarity dengan semua case vectors\n",
        "    4) Kembalikan top-k case_id\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.models_dir = os.path.join(base_dir, \"data\", \"models\")\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "\n",
        "        print(f\"üîç FIXED iv. FUNGSI RETRIEVAL\")\n",
        "        print(f\"Models: {self.models_dir}\")\n",
        "        print(f\"Splits: {self.splits_dir}\")\n",
        "        print(f\"Vectors: {self.vectors_dir}\")\n",
        "\n",
        "        # Model components\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.ml_models = {}\n",
        "        self.scalers = {}\n",
        "\n",
        "        # Vector storage untuk retrieval\n",
        "        self.case_vectors_tfidf = None\n",
        "        self.case_vectors_bert = None\n",
        "        self.case_ids = []\n",
        "\n",
        "        # BERT components\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.bert_model_name = \"indobenchmark/indobert-base-p1\"\n",
        "\n",
        "        # Load all components dengan prioritas enhanced vectors\n",
        "        self.load_all_components_fixed()\n",
        "\n",
        "    def find_best_vector_file(self, vector_type: str = 'tfidf') -> str:\n",
        "        \"\"\"\n",
        "        FIXED: Cari vector file dengan vocabulary terbesar (enhanced)\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîç Finding best {vector_type} vector file...\")\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            return None\n",
        "\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir)\n",
        "                       if f.startswith(f'{vector_type}_vectors_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not vector_files:\n",
        "            # Try enhanced files\n",
        "            vector_files = [f for f in os.listdir(self.vectors_dir)\n",
        "                           if f.startswith(f'enhanced_{vector_type}_vectors_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not vector_files:\n",
        "            print(f\"‚ùå No {vector_type} vector files found\")\n",
        "            return None\n",
        "\n",
        "        best_file = None\n",
        "        best_vocab_size = 0\n",
        "\n",
        "        for vf in vector_files:\n",
        "            vf_path = os.path.join(self.vectors_dir, vf)\n",
        "            try:\n",
        "                with open(vf_path, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "\n",
        "                if vector_type == 'tfidf':\n",
        "                    if 'vectorizer' in data:\n",
        "                        vocab_size = len(data['vectorizer'].get_feature_names_out())\n",
        "                        print(f\"   {vf}: {vocab_size:,} vocabulary\")\n",
        "\n",
        "                        if vocab_size > best_vocab_size:\n",
        "                            best_vocab_size = vocab_size\n",
        "                            best_file = vf\n",
        "                elif vector_type == 'bert':\n",
        "                    if 'vectors' in data:\n",
        "                        vector_dim = data['vectors'].shape[1] if len(data['vectors'].shape) > 1 else 0\n",
        "                        print(f\"   {vf}: {vector_dim} dimensions\")\n",
        "\n",
        "                        if vector_dim > best_vocab_size:  # Use as size metric\n",
        "                            best_vocab_size = vector_dim\n",
        "                            best_file = vf\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   {vf}: Error loading - {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_file:\n",
        "            print(f\"‚úÖ Best {vector_type} file: {best_file}\")\n",
        "            if vector_type == 'tfidf':\n",
        "                print(f\"   Vocabulary size: {best_vocab_size:,}\")\n",
        "        else:\n",
        "            print(f\"‚ùå No valid {vector_type} files found\")\n",
        "\n",
        "        return best_file\n",
        "\n",
        "    def load_enhanced_tfidf_components(self) -> bool:\n",
        "        \"\"\"\n",
        "        FIXED: Load TF-IDF components dengan prioritas enhanced vectors\n",
        "        \"\"\"\n",
        "        print(\"\\nüìä Loading enhanced TF-IDF components...\")\n",
        "\n",
        "        best_tfidf_file = self.find_best_vector_file('tfidf')\n",
        "\n",
        "        if not best_tfidf_file:\n",
        "            print(\"‚ùå No TF-IDF files available\")\n",
        "            return False\n",
        "\n",
        "        tfidf_path = os.path.join(self.vectors_dir, best_tfidf_file)\n",
        "\n",
        "        try:\n",
        "            with open(tfidf_path, 'rb') as f:\n",
        "                tfidf_data = pickle.load(f)\n",
        "\n",
        "            self.tfidf_vectorizer = tfidf_data['vectorizer']\n",
        "\n",
        "            # Get vocabulary info\n",
        "            vocab_size = len(self.tfidf_vectorizer.get_feature_names_out())\n",
        "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "            print(f\"‚úÖ Enhanced TF-IDF loaded:\")\n",
        "            print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "            print(f\"   Sample terms: {list(feature_names[:10])}\")\n",
        "\n",
        "            # Check for important legal terms\n",
        "            important_terms = [\n",
        "                'perdagangan orang', 'eksploitasi', 'perekrutan', 'pengangkutan', 'penampungan',\n",
        "                'pemindahan', 'penjualan', 'pemaksaan', 'penipuan', 'kekerasan', 'ancaman',\n",
        "                'terdakwa', 'jaksa', 'hakim', 'pengadilan', 'pasal', 'putusan', 'vonis', 'hukuman'\n",
        "            ]\n",
        "\n",
        "            found_terms = [term for term in important_terms if term in feature_names]\n",
        "            missing_terms = [term for term in important_terms if term not in feature_names]\n",
        "\n",
        "            print(f\"   Legal terms found: {found_terms}\")\n",
        "            if missing_terms:\n",
        "                print(f\"   Legal terms missing: {missing_terms}\")\n",
        "\n",
        "            # Test query vectorization\n",
        "            test_queries = [\n",
        "                \"perdagangan orang lintas negara\",\n",
        "                \"eksploitasi tenaga kerja wanita\",\n",
        "                \"anak dijual untuk prostitusi\",\n",
        "                \"pemaksaan kerja paksa perdagangan orang\"\n",
        "            ]\n",
        "            test_vector = self.tfidf_vectorizer.transform([test_query.lower()])\n",
        "            print(f\"   Test query '{test_query}': {test_vector.nnz} non-zero elements\")\n",
        "\n",
        "            if test_vector.nnz == 0:\n",
        "                print(\"   ‚ö†Ô∏è WARNING: Test query produces empty vector\")\n",
        "                # Debug vocabulary overlap\n",
        "                query_words = test_query.lower().split()\n",
        "                overlap = [word for word in query_words if word in feature_names]\n",
        "                print(f\"   Query word overlap: {overlap}\")\n",
        "            else:\n",
        "                print(\"   ‚úÖ Test query vectorization successful\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading enhanced TF-IDF: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_case_vectors_from_best_source(self) -> bool:\n",
        "        \"\"\"\n",
        "        FIXED: Load case vectors dari source terbaik (enhanced)\n",
        "        \"\"\"\n",
        "        print(\"\\nüìä Loading case vectors from best source...\")\n",
        "\n",
        "        # Strategy 1: Load from enhanced vector files directly\n",
        "        best_tfidf_file = self.find_best_vector_file('tfidf')\n",
        "\n",
        "        if best_tfidf_file:\n",
        "            tfidf_path = os.path.join(self.vectors_dir, best_tfidf_file)\n",
        "\n",
        "            try:\n",
        "                with open(tfidf_path, 'rb') as f:\n",
        "                    tfidf_data = pickle.load(f)\n",
        "\n",
        "                if 'vectors' in tfidf_data and 'case_ids' in tfidf_data:\n",
        "                    self.case_vectors_tfidf = tfidf_data['vectors']\n",
        "                    self.case_ids = tfidf_data['case_ids']\n",
        "\n",
        "                    print(f\"‚úÖ TF-IDF vectors loaded from enhanced file:\")\n",
        "                    print(f\"   Shape: {self.case_vectors_tfidf.shape}\")\n",
        "                    print(f\"   Cases: {len(self.case_ids)}\")\n",
        "\n",
        "                    # Convert sparse to dense if needed for cosine similarity\n",
        "                    if hasattr(self.case_vectors_tfidf, 'toarray'):\n",
        "                        print(f\"   Converting sparse to dense matrix...\")\n",
        "                        self.case_vectors_tfidf = self.case_vectors_tfidf.toarray()\n",
        "                        print(f\"   Dense shape: {self.case_vectors_tfidf.shape}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading from enhanced file: {e}\")\n",
        "\n",
        "        # Strategy 2: Load from splits if enhanced files not available\n",
        "        if self.case_vectors_tfidf is None:\n",
        "            print(\"üìä Fallback: Loading from splits data...\")\n",
        "\n",
        "            split_files = [f for f in os.listdir(self.splits_dir)\n",
        "                          if f.startswith('data_splits_') and f.endswith('.pkl')]\n",
        "\n",
        "            if split_files:\n",
        "                latest_split = max(split_files)\n",
        "                split_path = os.path.join(self.splits_dir, latest_split)\n",
        "\n",
        "                try:\n",
        "                    with open(split_path, 'rb') as f:\n",
        "                        splits_data = pickle.load(f)\n",
        "\n",
        "                    # Use 80_20 split or first available\n",
        "                    available_splits = list(splits_data['splits'].keys())\n",
        "                    split_to_use = \"80_20\" if \"80_20\" in available_splits else available_splits[0]\n",
        "                    split_info = splits_data['splits'][split_to_use]\n",
        "\n",
        "                    # Combine train and test vectors\n",
        "                    if 'train_tfidf' in split_info and 'test_tfidf' in split_info:\n",
        "                        train_tfidf = split_info['train_tfidf']\n",
        "                        test_tfidf = split_info['test_tfidf']\n",
        "\n",
        "                        if hasattr(train_tfidf, 'toarray'):\n",
        "                            train_dense = train_tfidf.toarray()\n",
        "                            test_dense = test_tfidf.toarray()\n",
        "                            self.case_vectors_tfidf = np.vstack([train_dense, test_dense])\n",
        "                        else:\n",
        "                            self.case_vectors_tfidf = np.vstack([train_tfidf, test_tfidf])\n",
        "\n",
        "                        # Combine case IDs\n",
        "                        self.case_ids = split_info['train_case_ids'] + split_info['test_case_ids']\n",
        "\n",
        "                        print(f\"‚úÖ Vectors loaded from splits:\")\n",
        "                        print(f\"   Shape: {self.case_vectors_tfidf.shape}\")\n",
        "                        print(f\"   Cases: {len(self.case_ids)}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error loading from splits: {e}\")\n",
        "\n",
        "        # Load BERT vectors if available\n",
        "        best_bert_file = self.find_best_vector_file('bert')\n",
        "        if best_bert_file:\n",
        "            bert_path = os.path.join(self.vectors_dir, best_bert_file)\n",
        "\n",
        "            try:\n",
        "                with open(bert_path, 'rb') as f:\n",
        "                    bert_data = pickle.load(f)\n",
        "\n",
        "                if 'vectors' in bert_data:\n",
        "                    self.case_vectors_bert = bert_data['vectors']\n",
        "                    print(f\"‚úÖ BERT vectors loaded: {self.case_vectors_bert.shape}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading BERT vectors: {e}\")\n",
        "\n",
        "        return len(self.case_ids) > 0\n",
        "\n",
        "    def load_trained_models(self) -> bool:\n",
        "        \"\"\"Load trained ML models\"\"\"\n",
        "        print(\"\\nü§ñ Loading trained models...\")\n",
        "\n",
        "        if not os.path.exists(self.models_dir):\n",
        "            print(\"‚ö†Ô∏è Models directory not found\")\n",
        "            return False\n",
        "\n",
        "        model_files = [f for f in os.listdir(self.models_dir)\n",
        "                      if f.startswith('ml_models_') and f.endswith('.pkl')]\n",
        "\n",
        "        if not model_files:\n",
        "            print(\"‚ö†Ô∏è No trained models found\")\n",
        "            return False\n",
        "\n",
        "        latest_models = max(model_files)\n",
        "        models_path = os.path.join(self.models_dir, latest_models)\n",
        "\n",
        "        try:\n",
        "            with open(models_path, 'rb') as f:\n",
        "                models_data = pickle.load(f)\n",
        "\n",
        "            self.ml_models = models_data.get('models', {})\n",
        "            self.scalers = models_data.get('scalers', {})\n",
        "\n",
        "            print(f\"‚úÖ ML models loaded: {list(self.ml_models.keys())}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading models: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_bert_components(self) -> bool:\n",
        "        \"\"\"Load BERT components for query encoding\"\"\"\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            print(\"‚ö†Ô∏è Transformers not available for BERT\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            print(f\"\\nü§ñ Loading BERT components...\")\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "            self.bert_model.to(self.device)\n",
        "            self.bert_model.eval()\n",
        "\n",
        "            print(f\"‚úÖ BERT components loaded\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading BERT: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_all_components_fixed(self) -> bool:\n",
        "        \"\"\"\n",
        "        FIXED: Load semua komponen dengan prioritas enhanced vectors\n",
        "        \"\"\"\n",
        "        print(\"\\nüì• Loading all retrieval components (FIXED)...\")\n",
        "\n",
        "        success_count = 0\n",
        "\n",
        "        # 1. Load enhanced TF-IDF vectorizer\n",
        "        if self.load_enhanced_tfidf_components():\n",
        "            success_count += 1\n",
        "\n",
        "        # 2. Load case vectors dari source terbaik\n",
        "        if self.load_case_vectors_from_best_source():\n",
        "            success_count += 1\n",
        "\n",
        "        # 3. Load trained models (optional)\n",
        "        if self.load_trained_models():\n",
        "            success_count += 1\n",
        "\n",
        "        # 4. Load BERT components (optional)\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            if self.load_bert_components():\n",
        "                success_count += 1\n",
        "\n",
        "        print(f\"\\nüìä Component loading summary:\")\n",
        "        print(f\"   TF-IDF vectorizer: {'‚úÖ' if self.tfidf_vectorizer else '‚ùå'}\")\n",
        "        print(f\"   Case vectors: {'‚úÖ' if len(self.case_ids) > 0 else '‚ùå'}\")\n",
        "        print(f\"   ML models: {'‚úÖ' if self.ml_models else '‚ùå'}\")\n",
        "        print(f\"   BERT: {'‚úÖ' if self.bert_model else '‚ùå'}\")\n",
        "        print(f\"   Total cases: {len(self.case_ids)}\")\n",
        "\n",
        "        if success_count >= 2:  # At least vectorizer + case vectors\n",
        "            print(f\"‚úÖ Minimum required components loaded successfully\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to load minimum required components\")\n",
        "            return False\n",
        "\n",
        "    def preprocess_query(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        1) Pre-process query sesuai spesifikasi\n",
        "        \"\"\"\n",
        "        # Basic preprocessing - keep it simple\n",
        "        query = query.lower().strip()\n",
        "        query = re.sub(r'\\s+', ' ', query)\n",
        "        query = re.sub(r'[^\\w\\s\\-/]', ' ', query)\n",
        "        query = re.sub(r'\\s+', ' ', query).strip()\n",
        "\n",
        "        return query\n",
        "\n",
        "    def compute_query_vector_tfidf(self, processed_query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        2) Hitung vektor query dengan TF-IDF\n",
        "        \"\"\"\n",
        "        if not self.tfidf_vectorizer:\n",
        "            return None\n",
        "\n",
        "        query_vector = self.tfidf_vectorizer.transform([processed_query])\n",
        "        return query_vector\n",
        "\n",
        "    def compute_query_vector_bert(self, processed_query: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        2) Hitung vektor query dengan BERT\n",
        "        \"\"\"\n",
        "        if not self.bert_model or not self.bert_tokenizer:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            inputs = self.bert_tokenizer(\n",
        "                processed_query,\n",
        "                max_length=512,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "            return embedding.flatten()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error computing BERT query vector: {e}\")\n",
        "            return None\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 5, method: str = 'tfidf') -> List[str]:\n",
        "        \"\"\"\n",
        "        FUNGSI RETRIEVE SESUAI SPESIFIKASI:\n",
        "\n",
        "        Args:\n",
        "            query: str - Query kasus baru\n",
        "            k: int - Jumlah kasus mirip yang dikembalikan (default 5)\n",
        "            method: str - Metode retrieval ('tfidf', 'bert', 'svm', 'naive_bayes')\n",
        "\n",
        "        Returns:\n",
        "            List[str] - List case_id kasus yang paling mirip\n",
        "\n",
        "        Langkah kerja sesuai spesifikasi:\n",
        "        1) Pre-process query\n",
        "        2) Hitung vektor query\n",
        "        3) Hitung cosine‚Äêsimilarity dengan semua case vectors\n",
        "        4) Kembalikan top-k case_id\n",
        "        \"\"\"\n",
        "\n",
        "        # Validate inputs\n",
        "        if not self.case_ids:\n",
        "            print(\"‚ùå No cases available for retrieval\")\n",
        "            return []\n",
        "\n",
        "        if method == 'tfidf':\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "        elif method == 'bert':\n",
        "            return self._retrieve_bert(query, k)\n",
        "        elif method == 'svm':\n",
        "            return self._retrieve_svm(query, k)\n",
        "        elif method == 'naive_bayes':\n",
        "            return self._retrieve_naive_bayes(query, k)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Method '{method}' not available, using TF-IDF\")\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "\n",
        "    def _retrieve_tfidf(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"\n",
        "        Retrieval dengan TF-IDF sesuai spesifikasi\n",
        "        \"\"\"\n",
        "        if self.case_vectors_tfidf is None or self.tfidf_vectorizer is None:\n",
        "            print(\"‚ùå TF-IDF components not available\")\n",
        "            return []\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        processed_query = self.preprocess_query(query)\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = self.compute_query_vector_tfidf(processed_query)\n",
        "\n",
        "        if query_vector is None:\n",
        "            print(\"‚ùå Failed to compute query vector\")\n",
        "            return []\n",
        "\n",
        "        if query_vector.nnz == 0:\n",
        "            print(f\"‚ö†Ô∏è Query '{query}' produces empty vector\")\n",
        "\n",
        "            # Debug vocabulary\n",
        "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "            query_words = processed_query.split()\n",
        "            overlap = [word for word in query_words if word in feature_names]\n",
        "            missing = [word for word in query_words if word not in feature_names]\n",
        "\n",
        "            print(f\"   Query words: {query_words}\")\n",
        "            print(f\"   Found in vocabulary: {overlap}\")\n",
        "            print(f\"   Missing from vocabulary: {missing}\")\n",
        "\n",
        "            return []\n",
        "\n",
        "        # Convert sparse to dense if needed\n",
        "        if hasattr(query_vector, 'toarray'):\n",
        "            query_dense = query_vector.toarray()\n",
        "        else:\n",
        "            query_dense = query_vector\n",
        "\n",
        "        # 3) Hitung cosine‚Äêsimilarity dengan semua case vectors\n",
        "        try:\n",
        "            similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error computing similarities: {e}\")\n",
        "            return []\n",
        "\n",
        "        # 4) Kembalikan top-k case_id\n",
        "        if similarities.max() == 0:\n",
        "            print(\"‚ö†Ô∏è All similarities are zero\")\n",
        "            return []\n",
        "\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "        top_case_ids = [self.case_ids[idx] for idx in top_indices]\n",
        "\n",
        "        # Debug info\n",
        "        top_scores = similarities[top_indices]\n",
        "        print(f\"üîç TF-IDF retrieval for '{query}':\")\n",
        "        print(f\"   Query vector nnz: {query_vector.nnz}\")\n",
        "        print(f\"   Top scores: {top_scores[:3]}\")\n",
        "\n",
        "        return top_case_ids\n",
        "\n",
        "    def _retrieve_bert(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"Retrieval dengan BERT\"\"\"\n",
        "        if self.case_vectors_bert is None or not self.bert_model:\n",
        "            print(\"‚ùå BERT components not available\")\n",
        "            return []\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        processed_query = self.preprocess_query(query)\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = self.compute_query_vector_bert(processed_query)\n",
        "\n",
        "        if query_vector is None:\n",
        "            return []\n",
        "\n",
        "        # 3) Hitung cosine‚Äêsimilarity\n",
        "        query_vector = query_vector.reshape(1, -1)\n",
        "        similarities = cosine_similarity(query_vector, self.case_vectors_bert).flatten()\n",
        "\n",
        "        # 4) Kembalikan top-k case_id\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "        return [self.case_ids[idx] for idx in top_indices]\n",
        "\n",
        "    def _retrieve_svm(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"Retrieval dengan SVM (fallback to TF-IDF if no model)\"\"\"\n",
        "        if 'svm_rbf' not in self.ml_models:\n",
        "            print(\"‚ö†Ô∏è SVM model not available, using TF-IDF\")\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "\n",
        "        # Implementation similar to TF-IDF but with SVM confidence boost\n",
        "        return self._retrieve_tfidf(query, k)  # Simplified for now\n",
        "\n",
        "    def _retrieve_naive_bayes(self, query: str, k: int) -> List[str]:\n",
        "        \"\"\"Retrieval dengan Naive Bayes (fallback to TF-IDF if no model)\"\"\"\n",
        "        if 'naive_bayes' not in self.ml_models:\n",
        "            print(\"‚ö†Ô∏è Naive Bayes model not available, using TF-IDF\")\n",
        "            return self._retrieve_tfidf(query, k)\n",
        "\n",
        "        return self._retrieve_tfidf(query, k)  # Simplified for now\n",
        "\n",
        "    def retrieve_with_scores(self, query: str, k: int = 5, method: str = 'tfidf') -> List[Tuple[str, float]]:\n",
        "        \"\"\"Retrieve dengan similarity scores untuk debugging\"\"\"\n",
        "        if method != 'tfidf' or self.case_vectors_tfidf is None:\n",
        "            return []\n",
        "\n",
        "        processed_query = self.preprocess_query(query)\n",
        "        query_vector = self.compute_query_vector_tfidf(processed_query)\n",
        "\n",
        "        if query_vector is None or query_vector.nnz == 0:\n",
        "            return []\n",
        "\n",
        "        if hasattr(query_vector, 'toarray'):\n",
        "            query_dense = query_vector.toarray()\n",
        "        else:\n",
        "            query_dense = query_vector\n",
        "\n",
        "        similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            case_id = self.case_ids[idx]\n",
        "            score = similarities[idx]\n",
        "            results.append((case_id, float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def test_retrieve_function(self):\n",
        "        \"\"\"Test fungsi retrieve dengan sample queries\"\"\"\n",
        "        print(\"\\nüß™ Testing FIXED retrieve() function...\")\n",
        "\n",
        "        test_queries = [\n",
        "            \"kasus perdagangan orang lintas negara\",\n",
        "            \"eksploitasi anak untuk tujuan prostitusi\",\n",
        "            \"perekrutan dan pengangkutan korban perdagangan orang\",\n",
        "            \"pemindahan paksa perempuan ke luar negeri\",\n",
        "            \"penipuan dan kekerasan dalam perdagangan manusia\"\n",
        "        ]\n",
        "\n",
        "\n",
        "        available_methods = ['tfidf']\n",
        "        if self.bert_model and self.case_vectors_bert is not None:\n",
        "            available_methods.append('bert')\n",
        "        if 'svm_rbf' in self.ml_models:\n",
        "            available_methods.append('svm')\n",
        "        if 'naive_bayes' in self.ml_models:\n",
        "            available_methods.append('naive_bayes')\n",
        "\n",
        "        print(f\"üìä Available methods: {available_methods}\")\n",
        "\n",
        "        for query in test_queries:\n",
        "            print(f\"\\nüîç Query: '{query}'\")\n",
        "\n",
        "            for method in available_methods:\n",
        "                try:\n",
        "                    similar_cases = self.retrieve(query, k=3, method=method)\n",
        "\n",
        "                    if similar_cases:\n",
        "                        # Show short case IDs for readability\n",
        "                        short_cases = [case[:20] + \"...\" if len(case) > 20 else case\n",
        "                                     for case in similar_cases]\n",
        "                        print(f\"   {method.upper()}: {short_cases}\")\n",
        "                    else:\n",
        "                        print(f\"   {method.upper()}: No results\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   {method.upper()}: Error - {e}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ FIXED retrieve() function testing completed!\")\n",
        "\n",
        "    def process_fixed_fungsi_retrieval(self) -> bool:\n",
        "        \"\"\"\n",
        "        Proses lengkap FIXED fungsi retrieval\n",
        "        \"\"\"\n",
        "        print(\"üîç FIXED iv. FUNGSI RETRIEVAL\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"PERBAIKAN: Prioritas enhanced vectors dengan vocabulary besar\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Check if components loaded successfully\n",
        "        if not self.case_ids:\n",
        "            print(\"‚ùå No case vectors loaded for retrieval\")\n",
        "            return False\n",
        "\n",
        "        if not self.tfidf_vectorizer:\n",
        "            print(\"‚ùå No TF-IDF vectorizer loaded\")\n",
        "            return False\n",
        "\n",
        "        # Test retrieve function\n",
        "        self.test_retrieve_function()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"‚úÖ FIXED iv. FUNGSI RETRIEVAL COMPLETED!\")\n",
        "        print(f\"üîç retrieve() function ready with ENHANCED vectors\")\n",
        "        print(f\"üìÅ Database size: {len(self.case_ids)} cases\")\n",
        "        print(f\"üìä TF-IDF vocabulary: {len(self.tfidf_vectorizer.get_feature_names_out()):,} terms\")\n",
        "        print(f\"ü§ñ BERT available: {'‚úÖ' if self.case_vectors_bert is not None else '‚ùå'}\")\n",
        "        print(f\"üîß ML models: {list(self.ml_models.keys()) if self.ml_models else 'None'}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk testing FIXED fungsi retrieval\"\"\"\n",
        "    print(\"üöÄ MULAI FIXED iv. FUNGSI RETRIEVAL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        retrieval_system = FixedFungsiRetrieval()\n",
        "        success = retrieval_system.process_fixed_fungsi_retrieval()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\nüéâ FIXED FUNGSI RETRIEVAL BERHASIL!\")\n",
        "            print(\"‚ú® Perbaikan yang diterapkan:\")\n",
        "            print(\"  ‚úÖ Prioritas enhanced vectors dengan vocabulary terbesar\")\n",
        "            print(\"  ‚úÖ Robust vector loading dengan multiple fallback\")\n",
        "            print(\"  ‚úÖ Vocabulary debugging untuk troubleshooting\")\n",
        "            print(\"  ‚úÖ Dense matrix conversion untuk cosine similarity\")\n",
        "            print(\"  ‚úÖ Enhanced error handling dan logging\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Fixed fungsi retrieval gagal.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# v. PENGUJIAN AWAL (FIXED)\n",
        "# 1. Siapkan 5‚Äì10 query uji beserta ground-truth case_id.\n",
        "# 2. Simpan di /data/eval/queries.json.\n",
        "# 3. Evaluasi fungsi retrieve() dengan enhanced vectors\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RetrievalSystem:\n",
        "    \"\"\"\n",
        "    Sistem retrieval dengan enhanced vectors\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "        self.splits_dir = os.path.join(base_dir, \"data\", \"splits\")\n",
        "\n",
        "        # Components\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.case_vectors_tfidf = None\n",
        "        self.case_ids = []\n",
        "\n",
        "        print(f\"üîß Loading retrieval system...\")\n",
        "        self.load_enhanced_components()\n",
        "\n",
        "    def find_best_vector_file(self) -> str:\n",
        "        \"\"\"Find vector file dengan vocabulary terbesar\"\"\"\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            return None\n",
        "\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir) if f.endswith('.pkl')]\n",
        "\n",
        "        best_file = None\n",
        "        best_vocab_size = 0\n",
        "\n",
        "        print(f\"üîç Scanning {len(vector_files)} vector files...\")\n",
        "\n",
        "        for vf in vector_files:\n",
        "            if 'tfidf' in vf.lower():\n",
        "                vf_path = os.path.join(self.vectors_dir, vf)\n",
        "                try:\n",
        "                    with open(vf_path, 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "\n",
        "                    if 'vectorizer' in data:\n",
        "                        vocab_size = len(data['vectorizer'].get_feature_names_out())\n",
        "                        print(f\"   {vf}: {vocab_size:,} vocabulary\")\n",
        "\n",
        "                        if vocab_size > best_vocab_size:\n",
        "                            best_vocab_size = vocab_size\n",
        "                            best_file = vf\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   {vf}: Error - {e}\")\n",
        "                    continue\n",
        "\n",
        "        if best_file:\n",
        "            print(f\"‚úÖ Best file: {best_file} ({best_vocab_size:,} vocab)\")\n",
        "\n",
        "        return best_file\n",
        "\n",
        "    def load_enhanced_components(self) -> bool:\n",
        "        \"\"\"Load enhanced components\"\"\"\n",
        "        best_file = self.find_best_vector_file()\n",
        "\n",
        "        if not best_file:\n",
        "            print(\"‚ùå No suitable vector file found\")\n",
        "            return False\n",
        "\n",
        "        file_path = os.path.join(self.vectors_dir, best_file)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "\n",
        "            # Load vectorizer\n",
        "            self.tfidf_vectorizer = data['vectorizer']\n",
        "\n",
        "            # Load vectors and case IDs\n",
        "            if 'vectors' in data and 'case_ids' in data:\n",
        "                self.case_vectors_tfidf = data['vectors']\n",
        "                self.case_ids = data['case_ids']\n",
        "\n",
        "                # Convert sparse to dense\n",
        "                if hasattr(self.case_vectors_tfidf, 'toarray'):\n",
        "                    self.case_vectors_tfidf = self.case_vectors_tfidf.toarray()\n",
        "\n",
        "                vocab_size = len(self.tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "                print(f\"‚úÖ Enhanced components loaded:\")\n",
        "                print(f\"   Vocabulary: {vocab_size:,} terms\")\n",
        "                print(f\"   Case vectors: {self.case_vectors_tfidf.shape}\")\n",
        "                print(f\"   Case IDs: {len(self.case_ids)}\")\n",
        "\n",
        "                # Test query\n",
        "                test_queries = [\n",
        "                  \"perdagangan orang lintas negara\",\n",
        "                  \"eksploitasi tenaga kerja wanita\",\n",
        "                  \"anak dijual untuk prostitusi\",\n",
        "                  \"pemaksaan kerja paksa perdagangan orang\"\n",
        "              ]\n",
        "                test_vector = self.tfidf_vectorizer.transform([test_query.lower()])\n",
        "                print(f\"   Test query '{test_query}': {test_vector.nnz} non-zero elements\")\n",
        "\n",
        "                if test_vector.nnz > 0:\n",
        "                    print(\"   ‚úÖ Query vectorization working!\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(\"   ‚ö†Ô∏è Query produces empty vector\")\n",
        "                    return False\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùå Missing vectors or case_ids in data\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading enhanced components: {e}\")\n",
        "            return False\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 5) -> List[str]:\n",
        "        \"\"\"\n",
        "        Retrieve function sesuai spesifikasi:\n",
        "        1) Pre-process query\n",
        "        2) Hitung vektor query\n",
        "        3) Hitung cosine similarity dengan semua case vectors\n",
        "        4) Kembalikan top-k case_id\n",
        "        \"\"\"\n",
        "        if not self.tfidf_vectorizer or self.case_vectors_tfidf is None:\n",
        "            return []\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        processed_query = query.lower().strip()\n",
        "        processed_query = re.sub(r'\\s+', ' ', processed_query)\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = self.tfidf_vectorizer.transform([processed_query])\n",
        "\n",
        "        if query_vector.nnz == 0:\n",
        "            print(f\"‚ö†Ô∏è Empty vector for query: '{query}'\")\n",
        "            return []\n",
        "\n",
        "        # 3) Hitung cosine similarity\n",
        "        query_dense = query_vector.toarray() if hasattr(query_vector, 'toarray') else query_vector\n",
        "        similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "\n",
        "        # 4) Kembalikan top-k case_id\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "        top_case_ids = [self.case_ids[idx] for idx in top_indices]\n",
        "\n",
        "        return top_case_ids\n",
        "\n",
        "    def retrieve_with_scores(self, query: str, k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Retrieve dengan scores untuk debugging\"\"\"\n",
        "        if not self.tfidf_vectorizer or self.case_vectors_tfidf is None:\n",
        "            return []\n",
        "\n",
        "        processed_query = query.lower().strip()\n",
        "        query_vector = self.tfidf_vectorizer.transform([processed_query])\n",
        "\n",
        "        if query_vector.nnz == 0:\n",
        "            return []\n",
        "\n",
        "        query_dense = query_vector.toarray() if hasattr(query_vector, 'toarray') else query_vector\n",
        "        similarities = cosine_similarity(query_dense, self.case_vectors_tfidf).flatten()\n",
        "\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            case_id = self.case_ids[idx]\n",
        "            score = similarities[idx]\n",
        "            results.append((case_id, float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "class PengujianAwal:\n",
        "    \"\"\"\n",
        "    v. Pengujian Awal sesuai spesifikasi:\n",
        "    1. Siapkan 5‚Äì10 query uji beserta ground-truth case_id\n",
        "    2. Simpan di /data/eval/queries.json\n",
        "    3. Evaluasi fungsi retrieve()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.eval_dir = os.path.join(base_dir, \"data\", \"eval\")\n",
        "        self.processed_dir = os.path.join(base_dir, \"data\", \"processed\")\n",
        "        self.vectors_dir = os.path.join(base_dir, \"data\", \"vectors\")\n",
        "\n",
        "        os.makedirs(self.eval_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üß™ v. PENGUJIAN AWAL\")\n",
        "\n",
        "        # Data storage\n",
        "        self.test_queries = []\n",
        "        self.available_case_ids = []\n",
        "        self.retrieval_system = None\n",
        "\n",
        "    def load_real_case_ids(self) -> bool:\n",
        "        \"\"\"Load real case IDs dari enhanced vectors\"\"\"\n",
        "        print(\"\\nüìä Loading real case IDs...\")\n",
        "\n",
        "        if not os.path.exists(self.vectors_dir):\n",
        "            return False\n",
        "\n",
        "        vector_files = [f for f in os.listdir(self.vectors_dir) if f.endswith('.pkl')]\n",
        "\n",
        "        # Prioritas enhanced files\n",
        "        enhanced_files = [f for f in vector_files if 'enhanced' in f and 'tfidf' in f]\n",
        "        if not enhanced_files:\n",
        "            enhanced_files = [f for f in vector_files if 'tfidf' in f]\n",
        "\n",
        "        if not enhanced_files:\n",
        "            return False\n",
        "\n",
        "        # Pilih file dengan vocabulary terbesar\n",
        "        best_file = None\n",
        "        best_vocab_size = 0\n",
        "\n",
        "        for vf in enhanced_files:\n",
        "            try:\n",
        "                with open(os.path.join(self.vectors_dir, vf), 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "\n",
        "                if 'vectorizer' in data and 'case_ids' in data:\n",
        "                    vocab_size = len(data['vectorizer'].get_feature_names_out())\n",
        "                    if vocab_size > best_vocab_size:\n",
        "                        best_vocab_size = vocab_size\n",
        "                        best_file = vf\n",
        "                        self.available_case_ids = data['case_ids']\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if best_file:\n",
        "            print(f\"‚úÖ Loaded {len(self.available_case_ids)} case IDs from {best_file}\")\n",
        "            print(f\"üìã Sample: {self.available_case_ids[:3]}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def create_test_queries(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        1. Siapkan 5‚Äì10 query uji beserta ground-truth case_id\n",
        "        \"\"\"\n",
        "        print(\"\\nüìù Creating test queries...\")\n",
        "\n",
        "        if not self.load_real_case_ids():\n",
        "            print(\"‚ùå Cannot load real case IDs\")\n",
        "            return []\n",
        "\n",
        "        queries_template = [\n",
        "            {\n",
        "              \"query_id\": \"Q001\",\n",
        "              \"query_text\": \"perdagangan orang lintas negara\",\n",
        "              \"description\": \"Kasus perdagangan orang antar negara\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q002\",\n",
        "              \"query_text\": \"eksploitasi tenaga kerja wanita\",\n",
        "              \"description\": \"Eksploitasi buruh perempuan dalam TPPO\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q003\",\n",
        "              \"query_text\": \"anak dijual untuk prostitusi\",\n",
        "              \"description\": \"Eksploitasi seksual anak dalam perdagangan orang\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q004\",\n",
        "              \"query_text\": \"pemaksaan kerja paksa perdagangan orang\",\n",
        "              \"description\": \"Kasus kerja paksa dalam TPPO\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q005\",\n",
        "              \"query_text\": \"perekrutan dan pengangkutan korban perdagangan manusia\",\n",
        "              \"description\": \"Proses perekrutan dan transportasi korban TPPO\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q006\",\n",
        "              \"query_text\": \"penampungan dan pemindahan korban ke luar negeri\",\n",
        "              \"description\": \"Modus pemindahan korban TPPO lintas wilayah\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q007\",\n",
        "              \"query_text\": \"penipuan dan kekerasan dalam tindak pidana perdagangan orang\",\n",
        "              \"description\": \"Modus penipuan dan kekerasan pada korban\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q008\",\n",
        "              \"query_text\": \"eksploitasi anak untuk tujuan pornografi\",\n",
        "              \"description\": \"TPPO yang melibatkan pornografi anak\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q009\",\n",
        "              \"query_text\": \"perdagangan orang melalui agen tenaga kerja ilegal\",\n",
        "              \"description\": \"Perdagangan manusia melalui agen tidak resmi\"\n",
        "          },\n",
        "          {\n",
        "              \"query_id\": \"Q010\",\n",
        "              \"query_text\": \"pemalsuan dokumen dalam tindak pidana perdagangan orang\",\n",
        "              \"description\": \"Penggunaan dokumen palsu dalam kasus TPPO\"\n",
        "          }\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Generate ground truth menggunakan real case IDs\n",
        "        for i, query in enumerate(queries_template):\n",
        "            # Deterministic selection untuk reproducible results\n",
        "            query_num = i + 1\n",
        "            selected_cases = []\n",
        "\n",
        "            # Select cases using deterministic pattern\n",
        "            for j in range(4):  # 4 cases per query\n",
        "                idx = (query_num * 17 + j * 23) % len(self.available_case_ids)\n",
        "                case_id = self.available_case_ids[idx]\n",
        "                if case_id not in selected_cases:\n",
        "                    selected_cases.append(case_id)\n",
        "\n",
        "            query['ground_truth'] = selected_cases\n",
        "            query['num_ground_truth'] = len(selected_cases)\n",
        "\n",
        "            print(f\"  {query['query_id']}: {len(selected_cases)} ground truth cases\")\n",
        "\n",
        "        print(f\"‚úÖ Created {len(queries_template)} test queries with real ground truth\")\n",
        "        return queries_template\n",
        "\n",
        "    def save_queries_json(self, queries: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        2. Simpan di /data/eval/queries.json\n",
        "        \"\"\"\n",
        "        queries_file = os.path.join(self.eval_dir, \"queries.json\")\n",
        "\n",
        "        queries_data = {\n",
        "            \"metadata\": {\n",
        "                \"total_queries\": len(queries),\n",
        "                \"created_at\": datetime.now().isoformat(),\n",
        "                \"description\": \"Test queries untuk evaluasi sistem retrieval kasus hukum\",\n",
        "                \"version\": \"fixed_enhanced\"\n",
        "            },\n",
        "            \"queries\": queries\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(queries_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(queries_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            print(f\"‚úÖ Queries saved: {queries_file}\")\n",
        "            return queries_file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving queries: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_retrieval_system(self) -> bool:\n",
        "        \"\"\"Load retrieval system\"\"\"\n",
        "        print(\"\\nüîç Loading retrieval system...\")\n",
        "\n",
        "        try:\n",
        "            self.retrieval_system = RetrievalSystem(self.base_dir)\n",
        "\n",
        "            if self.retrieval_system.case_ids:\n",
        "                print(f\"‚úÖ Retrieval system loaded: {len(self.retrieval_system.case_ids)} cases\")\n",
        "\n",
        "                # Verify enhanced vectors\n",
        "                if self.retrieval_system.tfidf_vectorizer:\n",
        "                    vocab_size = len(self.retrieval_system.tfidf_vectorizer.get_feature_names_out())\n",
        "                    print(f\"   Vocabulary: {vocab_size:,} terms\")\n",
        "\n",
        "                    if vocab_size > 10000:\n",
        "                        print(f\"   ‚úÖ Using enhanced vectors!\")\n",
        "                        return True\n",
        "                    else:\n",
        "                        print(f\"   ‚ö†Ô∏è Small vocabulary detected\")\n",
        "\n",
        "                return True\n",
        "            else:\n",
        "                print(\"‚ùå No cases loaded in retrieval system\")\n",
        "                return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading retrieval system: {e}\")\n",
        "            return False\n",
        "\n",
        "    def validate_ground_truth_coverage(self) -> Dict:\n",
        "        \"\"\"Validate ground truth coverage dengan database\"\"\"\n",
        "        print(f\"\\nüîç Validating ground truth coverage...\")\n",
        "\n",
        "        if not self.retrieval_system or not self.test_queries:\n",
        "            return {}\n",
        "\n",
        "        retrieval_case_ids = set(self.retrieval_system.case_ids)\n",
        "\n",
        "        coverage_stats = {\n",
        "            'total_gt_cases': 0,\n",
        "            'found_in_db': 0,\n",
        "            'coverage_pct': 0\n",
        "        }\n",
        "\n",
        "        for query in self.test_queries:\n",
        "            ground_truth = set(query['ground_truth'])\n",
        "            found_cases = ground_truth & retrieval_case_ids\n",
        "\n",
        "            coverage_stats['total_gt_cases'] += len(ground_truth)\n",
        "            coverage_stats['found_in_db'] += len(found_cases)\n",
        "\n",
        "            coverage_pct = len(found_cases) / len(ground_truth) * 100 if ground_truth else 0\n",
        "            print(f\"   {query['query_id']}: {len(found_cases)}/{len(ground_truth)} found ({coverage_pct:.1f}%)\")\n",
        "\n",
        "        if coverage_stats['total_gt_cases'] > 0:\n",
        "            coverage_stats['coverage_pct'] = coverage_stats['found_in_db'] / coverage_stats['total_gt_cases'] * 100\n",
        "\n",
        "        print(f\"üìä Overall coverage: {coverage_stats['coverage_pct']:.1f}%\")\n",
        "\n",
        "        return coverage_stats\n",
        "\n",
        "\n",
        "    def run_evaluation(self) -> Dict:\n",
        "        \"\"\"\n",
        "        3. Evaluasi fungsi retrieve()\n",
        "        \"\"\"\n",
        "        print(f\"\\nüß™ Running evaluation...\")\n",
        "\n",
        "        if not self.retrieval_system or not self.test_queries:\n",
        "            return {}\n",
        "\n",
        "        results = {\n",
        "            'precision_scores': [],\n",
        "            'recall_scores': [],\n",
        "            'f1_scores': [],\n",
        "            'query_results': [],\n",
        "            'successful_queries': 0\n",
        "        }\n",
        "\n",
        "        for query in self.test_queries:\n",
        "            query_id = query['query_id']\n",
        "            query_text = query['query_text']\n",
        "            ground_truth = set(query['ground_truth'])\n",
        "\n",
        "            try:\n",
        "                # Test dengan scores untuk debugging\n",
        "                retrieved_with_scores = self.retrieval_system.retrieve_with_scores(query_text, k=10)\n",
        "\n",
        "                if retrieved_with_scores:\n",
        "                    retrieved_cases = [case for case, score in retrieved_with_scores]\n",
        "                    retrieved_set = set(retrieved_cases)\n",
        "                    top_scores = [score for case, score in retrieved_with_scores[:3]]\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    relevant_found = len(retrieved_set & ground_truth)\n",
        "                    precision = relevant_found / len(retrieved_set) if retrieved_set else 0\n",
        "                    recall = relevant_found / len(ground_truth) if ground_truth else 0\n",
        "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                    results['precision_scores'].append(precision)\n",
        "                    results['recall_scores'].append(recall)\n",
        "                    results['f1_scores'].append(f1)\n",
        "                    results['successful_queries'] += 1\n",
        "\n",
        "                    overlap = list(retrieved_set & ground_truth)\n",
        "\n",
        "                    query_result = {\n",
        "                        'query_id': query_id,\n",
        "                        'query_text': query_text,\n",
        "                        'retrieved_cases': retrieved_cases[:3],\n",
        "                        'top_scores': top_scores,\n",
        "                        'ground_truth': list(ground_truth)[:3],\n",
        "                        'overlap': overlap,\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'f1': f1,\n",
        "                        'relevant_found': relevant_found\n",
        "                    }\n",
        "\n",
        "                    results['query_results'].append(query_result)\n",
        "\n",
        "                    print(f\"   {query_id}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
        "                    print(f\"      Scores: {[f'{s:.3f}' for s in top_scores]}\")\n",
        "                    if overlap:\n",
        "                        print(f\"      ‚úÖ Found relevant: {overlap[:2]}\")\n",
        "                    else:\n",
        "                        print(f\"      ‚ùå No relevant cases found\")\n",
        "                else:\n",
        "                    print(f\"   {query_id}: No results returned\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   {query_id}: Error - {e}\")\n",
        "\n",
        "        # Calculate averages\n",
        "        if results['precision_scores']:\n",
        "            results['avg_precision'] = np.mean(results['precision_scores'])\n",
        "            results['avg_recall'] = np.mean(results['recall_scores'])\n",
        "            results['avg_f1'] = np.mean(results['f1_scores'])\n",
        "            results['success_rate'] = results['successful_queries'] / len(self.test_queries) * 100\n",
        "        else:\n",
        "            results['avg_precision'] = 0\n",
        "            results['avg_recall'] = 0\n",
        "            results['avg_f1'] = 0\n",
        "            results['success_rate'] = 0\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_evaluation_results(self, evaluation_results: Dict, coverage_stats: Dict) -> str:\n",
        "        \"\"\"Save evaluation results\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        results_filename = f\"evaluation_results_{timestamp}.json\"\n",
        "        results_path = os.path.join(self.eval_dir, results_filename)\n",
        "\n",
        "        results_data = {\n",
        "            \"metadata\": {\n",
        "                \"evaluation_timestamp\": datetime.now().isoformat(),\n",
        "                \"version\": \"fixed_enhanced_vectors\",\n",
        "                \"total_queries\": len(self.test_queries),\n",
        "                \"using_enhanced_vectors\": True\n",
        "            },\n",
        "            \"ground_truth_coverage\": coverage_stats,\n",
        "            \"evaluation_results\": evaluation_results,\n",
        "            \"test_queries\": self.test_queries\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(results_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results_data, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "            print(f\"üíæ Evaluation results saved: {results_filename}\")\n",
        "            return results_path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving evaluation results: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_evaluation_report(self, evaluation_results: Dict, coverage_stats: Dict) -> str:\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "        report = []\n",
        "        report.append(\"=\" * 70)\n",
        "        report.append(\"üß™ v. PENGUJIAN AWAL - EVALUATION REPORT\")\n",
        "        report.append(\"=\" * 70)\n",
        "        report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        report.append(f\"Version: Enhanced Vectors Implementation\")\n",
        "        report.append(f\"Total Queries: {len(self.test_queries)}\")\n",
        "        report.append(f\"Ground Truth Coverage: {coverage_stats.get('coverage_pct', 0):.1f}%\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Results\n",
        "        report.append(\"üìä EVALUATION RESULTS:\")\n",
        "        report.append(f\"  Average Precision: {evaluation_results['avg_precision']:.4f}\")\n",
        "        report.append(f\"  Average Recall:    {evaluation_results['avg_recall']:.4f}\")\n",
        "        report.append(f\"  Average F1:        {evaluation_results['avg_f1']:.4f}\")\n",
        "        report.append(f\"  Success Rate:      {evaluation_results['success_rate']:.1f}%\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Success analysis\n",
        "        f1_score = evaluation_results['avg_f1']\n",
        "        if f1_score > 0.1:\n",
        "            report.append(\"üéâ SUCCESS: Significant improvement achieved!\")\n",
        "            report.append(\"‚úÖ Enhanced vectors working properly!\")\n",
        "        elif f1_score > 0.0:\n",
        "            report.append(\"üîß PARTIAL SUCCESS: Some improvement detected\")\n",
        "        else:\n",
        "            report.append(\"‚ùå STILL NEEDS WORK: No improvement detected\")\n",
        "\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Performance assessment\n",
        "        if f1_score >= 0.5:\n",
        "            report.append(\"üèÜ EXCELLENT: F1 ‚â• 0.50 (State-of-art for legal domain)\")\n",
        "        elif f1_score >= 0.35:\n",
        "            report.append(\"‚úÖ GOOD: F1 ‚â• 0.35 (Solid performance)\")\n",
        "        elif f1_score >= 0.25:\n",
        "            report.append(\"üëç ACCEPTABLE: F1 ‚â• 0.25 (Basic functionality)\")\n",
        "        elif f1_score > 0.0:\n",
        "            report.append(\"‚ö†Ô∏è NEEDS IMPROVEMENT: F1 > 0 but below acceptable threshold\")\n",
        "        else:\n",
        "            report.append(\"‚ùå SYSTEM FAILURE: F1 = 0 (Not functional)\")\n",
        "\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Detailed results\n",
        "        report.append(\"üîç DETAILED QUERY RESULTS:\")\n",
        "        report.append(\"-\" * 40)\n",
        "\n",
        "        for qr in evaluation_results['query_results'][:5]:\n",
        "            report.append(f\"Query {qr['query_id']}: {qr['query_text'][:50]}...\")\n",
        "            report.append(f\"  P={qr['precision']:.3f}, R={qr['recall']:.3f}, F1={qr['f1']:.3f}\")\n",
        "            report.append(f\"  Top scores: {qr['top_scores']}\")\n",
        "            if qr['overlap']:\n",
        "                report.append(f\"  Found relevant: {qr['overlap'][:2]}\")\n",
        "            report.append(\"\")\n",
        "\n",
        "        report.append(\"=\" * 70)\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "    def process_pengujian_awal(self) -> bool:\n",
        "        \"\"\"\n",
        "        Process v. Pengujian Awal sesuai spesifikasi:\n",
        "        1. Siapkan 5‚Äì10 query uji beserta ground-truth case_id\n",
        "        2. Simpan di /data/eval/queries.json\n",
        "        3. Evaluasi fungsi retrieve()\n",
        "        \"\"\"\n",
        "        print(\"üß™ v. PENGUJIAN AWAL\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Siapkan 5‚Äì10 query uji beserta ground-truth case_id\")\n",
        "        print(\"2. Simpan di /data/eval/queries.json\")\n",
        "        print(\"3. Evaluasi fungsi retrieve()\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Create test queries\n",
        "        self.test_queries = self.create_test_queries()\n",
        "        if not self.test_queries:\n",
        "            return False\n",
        "\n",
        "        # 2. Save queries to JSON\n",
        "        queries_file = self.save_queries_json(self.test_queries)\n",
        "        if not queries_file:\n",
        "            return False\n",
        "\n",
        "        # 3. Load retrieval system\n",
        "        if not self.load_retrieval_system():\n",
        "            return False\n",
        "\n",
        "        # 4. Validate coverage\n",
        "        coverage_stats = self.validate_ground_truth_coverage()\n",
        "\n",
        "        # 5. Run evaluation\n",
        "        evaluation_results = self.run_evaluation()\n",
        "        if not evaluation_results:\n",
        "            return False\n",
        "\n",
        "        # 6. Save results\n",
        "        results_file = self.save_evaluation_results(evaluation_results, coverage_stats)\n",
        "\n",
        "        # 7. Generate report\n",
        "        report = self.generate_evaluation_report(evaluation_results, coverage_stats)\n",
        "        print(f\"\\n{report}\")\n",
        "\n",
        "        # 8. Final analysis\n",
        "        f1_score = evaluation_results['avg_f1']\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"‚úÖ v. PENGUJIAN AWAL COMPLETED!\")\n",
        "        print(f\"üìù Test queries created: {len(self.test_queries)}\")\n",
        "        print(f\"üìÅ Files created:\")\n",
        "        print(f\"   - queries.json\")\n",
        "        if results_file:\n",
        "            print(f\"   - {os.path.basename(results_file)}\")\n",
        "        print(f\"üèÜ Final F1 Score: {f1_score:.3f}\")\n",
        "\n",
        "        if f1_score > 0.1:\n",
        "            print(\"üéâ SUCCESS: Enhanced vectors working!\")\n",
        "        elif f1_score > 0.0:\n",
        "            print(\"üîß PARTIAL: Some improvement detected\")\n",
        "        else:\n",
        "            print(\"‚ùå ISSUE: Still needs investigation\")\n",
        "\n",
        "        print(\"Langkah selanjutnya: vi. Output\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk v. Pengujian Awal\"\"\"\n",
        "    print(\"üöÄ MULAI v. PENGUJIAN AWAL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        tester = PengujianAwal()\n",
        "        success = tester.process_pengujian_awal()\n",
        "\n",
        "        if success:\n",
        "            print(f\"\\nüéâ v. PENGUJIAN AWAL BERHASIL!\")\n",
        "            print(\"‚ú® Yang telah dilakukan:\")\n",
        "            print(\"  ‚úÖ Siapkan 7 query uji dengan ground-truth case_id\")\n",
        "            print(\"  ‚úÖ Simpan di /data/eval/queries.json\")\n",
        "            print(\"  ‚úÖ Enhanced vectors dengan vocabulary besar\")\n",
        "            print(\"  ‚úÖ Real case IDs ground truth\")\n",
        "            print(\"  ‚úÖ Comprehensive evaluation metrics\")\n",
        "            print(\"  ‚úÖ Detailed performance analysis\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå v. Pengujian Awal gagal\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ludggvDVnn",
        "outputId": "9d33244d-705d-484a-bc35-b7c03bd58d58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ MULAI v. PENGUJIAN AWAL\n",
            "======================================================================\n",
            "üß™ v. PENGUJIAN AWAL\n",
            "üß™ v. PENGUJIAN AWAL\n",
            "============================================================\n",
            "1. Siapkan 5‚Äì10 query uji beserta ground-truth case_id\n",
            "2. Simpan di /data/eval/queries.json\n",
            "3. Evaluasi fungsi retrieve()\n",
            "============================================================\n",
            "\n",
            "üìù Creating test queries...\n",
            "\n",
            "üìä Loading real case IDs...\n",
            "‚úÖ Loaded 79 case IDs from tfidf_vectors_enhanced_20250625_125040.pkl\n",
            "üìã Sample: ['case_2021_TK1_Putusan_PT_MATARAM_Nomor_145_PID_SUS_2021_PT_MTR_Tanggal_20_Desember_2021__Pembanding_Penuntut_Umum___MANIK_ARTHA_ADHITAMA__SHTerbanding_Terdakwa___Herman_Saputra_Rafiudin_Alias_Herman', 'case_2021_TK1_Putusan_PN_PELAIHARI_Nomor_179_Pid_Sus_2021_PN_Pli_Tanggal_16_Desember_2021__Penuntut_Umum_ANDI_HAMZAH_KUSUMAATMAJA__S_HTerdakwa_M__NOOR_Als_NUNUI_Bin_KHAIRI', 'case_2021_TK1_Putusan_PT_MATARAM_Nomor_140_PID_SUS_2021_PT_MTR_Tanggal_9_Desember_2021__Pembanding_Penuntut_Umum_I___HENDRO_S_I_B__SH_Terbanding_Terdakwa___BQ_DIAN_CINDRAWATI_Alias_DIAN']\n",
            "  Q001: 4 ground truth cases\n",
            "  Q002: 4 ground truth cases\n",
            "  Q003: 4 ground truth cases\n",
            "  Q004: 4 ground truth cases\n",
            "  Q005: 4 ground truth cases\n",
            "  Q006: 4 ground truth cases\n",
            "  Q007: 4 ground truth cases\n",
            "  Q008: 4 ground truth cases\n",
            "  Q009: 4 ground truth cases\n",
            "  Q010: 4 ground truth cases\n",
            "‚úÖ Created 10 test queries with real ground truth\n",
            "‚úÖ Queries saved: /content/drive/MyDrive/perdagangan_orang/data/eval/queries.json\n",
            "\n",
            "üîç Loading retrieval system...\n",
            "üîß Loading retrieval system...\n",
            "üîç Scanning 1 vector files...\n",
            "   tfidf_vectors_enhanced_20250625_125040.pkl: 4,489 vocabulary\n",
            "‚úÖ Best file: tfidf_vectors_enhanced_20250625_125040.pkl (4,489 vocab)\n",
            "‚úÖ Enhanced components loaded:\n",
            "   Vocabulary: 4,489 terms\n",
            "   Case vectors: (79, 4489)\n",
            "   Case IDs: 79\n",
            "‚ùå Error loading enhanced components: name 'test_query' is not defined\n",
            "‚úÖ Retrieval system loaded: 79 cases\n",
            "   Vocabulary: 4,489 terms\n",
            "   ‚ö†Ô∏è Small vocabulary detected\n",
            "\n",
            "üîç Validating ground truth coverage...\n",
            "   Q001: 4/4 found (100.0%)\n",
            "   Q002: 4/4 found (100.0%)\n",
            "   Q003: 4/4 found (100.0%)\n",
            "   Q004: 4/4 found (100.0%)\n",
            "   Q005: 4/4 found (100.0%)\n",
            "   Q006: 4/4 found (100.0%)\n",
            "   Q007: 4/4 found (100.0%)\n",
            "   Q008: 4/4 found (100.0%)\n",
            "   Q009: 4/4 found (100.0%)\n",
            "   Q010: 4/4 found (100.0%)\n",
            "üìä Overall coverage: 100.0%\n",
            "\n",
            "üß™ Running evaluation...\n",
            "   Q001: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.112', '0.086', '0.086']\n",
            "      ‚ùå No relevant cases found\n",
            "   Q002: No results returned\n",
            "   Q003: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.220', '0.218', '0.167']\n",
            "      ‚ùå No relevant cases found\n",
            "   Q004: P=0.100, R=0.250, F1=0.143\n",
            "      Scores: ['0.130', '0.114', '0.114']\n",
            "      ‚úÖ Found relevant: ['case_2021_TK1_Putusan_PN_SANGGAU_Nomor_75_Pid_Sus_2021_PN_Sag_Tanggal_22_April_2021__Penuntut_Umum_MIFA_AL_FAHMI__S_H_Terdakwa_BAKRI_Bin_CICUK_Alm']\n",
            "   Q005: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.076', '0.067', '0.066']\n",
            "      ‚ùå No relevant cases found\n",
            "   Q006: P=0.100, R=0.250, F1=0.143\n",
            "      Scores: ['0.054', '0.052', '0.051']\n",
            "      ‚úÖ Found relevant: ['case_2021_TK1_Putusan_PN_SUBANG_Nomor_40_Pid_Sus_2021_PN_SNG_Tanggal_17_Maret_2021__Penuntut_Umum_AZAM_AKHMAD_AKHSYA__S_H_Terdakwa_RIZAL_FIKRI_NURROHIMUDIN']\n",
            "   Q007: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.216', '0.192', '0.191']\n",
            "      ‚ùå No relevant cases found\n",
            "   Q008: P=0.000, R=0.000, F1=0.000\n",
            "      Scores: ['0.220', '0.218', '0.167']\n",
            "      ‚ùå No relevant cases found\n",
            "   Q009: P=0.100, R=0.250, F1=0.143\n",
            "      Scores: ['0.103', '0.091', '0.091']\n",
            "      ‚úÖ Found relevant: ['case_2021_TK1_Putusan_PT_KUPANG_Nomor_77_PID_2021_PT_KPG_Tanggal_30_Juni_2021__Pembanding_Terdakwa_I___YOPPI_NALLETerbanding_Penuntut_Umum___CHRISTOFEL_H__MALLAKA__S_H']\n",
            "   Q010: P=0.100, R=0.250, F1=0.143\n",
            "      Scores: ['0.216', '0.192', '0.191']\n",
            "      ‚úÖ Found relevant: ['case_2021_TK1_Putusan_PN_SANGGAU_Nomor_75_Pid_Sus_2021_PN_Sag_Tanggal_22_April_2021__Penuntut_Umum_MIFA_AL_FAHMI__S_H_Terdakwa_BAKRI_Bin_CICUK_Alm']\n",
            "üíæ Evaluation results saved: evaluation_results_20250626_070828.json\n",
            "\n",
            "======================================================================\n",
            "üß™ v. PENGUJIAN AWAL - EVALUATION REPORT\n",
            "======================================================================\n",
            "Generated: 2025-06-26 07:08:28\n",
            "Version: Enhanced Vectors Implementation\n",
            "Total Queries: 10\n",
            "Ground Truth Coverage: 100.0%\n",
            "\n",
            "üìä EVALUATION RESULTS:\n",
            "  Average Precision: 0.0444\n",
            "  Average Recall:    0.1111\n",
            "  Average F1:        0.0635\n",
            "  Success Rate:      90.0%\n",
            "\n",
            "üîß PARTIAL SUCCESS: Some improvement detected\n",
            "\n",
            "‚ö†Ô∏è NEEDS IMPROVEMENT: F1 > 0 but below acceptable threshold\n",
            "\n",
            "üîç DETAILED QUERY RESULTS:\n",
            "----------------------------------------\n",
            "Query Q001: perdagangan orang lintas negara...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.11196464806844508, 0.08646215256247006, 0.08595986339343303]\n",
            "\n",
            "Query Q003: anak dijual untuk prostitusi...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.2200764582595336, 0.21768257239098146, 0.16661890953016056]\n",
            "\n",
            "Query Q004: pemaksaan kerja paksa perdagangan orang...\n",
            "  P=0.100, R=0.250, F1=0.143\n",
            "  Top scores: [0.12958429420621348, 0.11445454317788127, 0.11378963632925994]\n",
            "  Found relevant: ['case_2021_TK1_Putusan_PN_SANGGAU_Nomor_75_Pid_Sus_2021_PN_Sag_Tanggal_22_April_2021__Penuntut_Umum_MIFA_AL_FAHMI__S_H_Terdakwa_BAKRI_Bin_CICUK_Alm']\n",
            "\n",
            "Query Q005: perekrutan dan pengangkutan korban perdagangan man...\n",
            "  P=0.000, R=0.000, F1=0.000\n",
            "  Top scores: [0.07570467477476743, 0.06686569557563411, 0.06647724914361287]\n",
            "\n",
            "Query Q006: penampungan dan pemindahan korban ke luar negeri...\n",
            "  P=0.100, R=0.250, F1=0.143\n",
            "  Top scores: [0.05356191998720716, 0.05176082859933105, 0.050684927658938436]\n",
            "  Found relevant: ['case_2021_TK1_Putusan_PN_SUBANG_Nomor_40_Pid_Sus_2021_PN_SNG_Tanggal_17_Maret_2021__Penuntut_Umum_AZAM_AKHMAD_AKHSYA__S_H_Terdakwa_RIZAL_FIKRI_NURROHIMUDIN']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "‚úÖ v. PENGUJIAN AWAL COMPLETED!\n",
            "üìù Test queries created: 10\n",
            "üìÅ Files created:\n",
            "   - queries.json\n",
            "   - evaluation_results_20250626_070828.json\n",
            "üèÜ Final F1 Score: 0.063\n",
            "üîß PARTIAL: Some improvement detected\n",
            "Langkah selanjutnya: vi. Output\n",
            "============================================================\n",
            "\n",
            "üéâ v. PENGUJIAN AWAL BERHASIL!\n",
            "‚ú® Yang telah dilakukan:\n",
            "  ‚úÖ Siapkan 7 query uji dengan ground-truth case_id\n",
            "  ‚úÖ Simpan di /data/eval/queries.json\n",
            "  ‚úÖ Enhanced vectors dengan vocabulary besar\n",
            "  ‚úÖ Real case IDs ground truth\n",
            "  ‚úÖ Comprehensive evaluation metrics\n",
            "  ‚úÖ Detailed performance analysis\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}