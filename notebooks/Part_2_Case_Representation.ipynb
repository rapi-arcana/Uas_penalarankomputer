{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIzV9-y1pk1H",
        "outputId": "36b0c15f-e2d6-4c19-8a5b-6f3b6886a85d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional"
      ],
      "metadata": {
        "id": "qp7R1o3K9fql"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pengaturan logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MetadataExtractor:\n",
        "    \"\"\"Ekstrak metadata terstruktur dari dokumen putusan pengadilan\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.cleaned_dir = \"/data/raw\"  # Input: file teks yang sudah dibersihkan\n",
        "        self.gdrive_cleaned_dir = os.path.join(base_dir, \"CLEANED\")  # Input alternatif\n",
        "        self.output_dir = \"/data/processed\"  # Output directory lokal\n",
        "        self.gdrive_output_dir = os.path.join(base_dir, \"data\", \"processed\")  # Output Google Drive\n",
        "        self.metadata_dir = os.path.join(base_dir, \"METADATA\")  # Backup output\n",
        "        self.logs_dir = \"/logs\"\n",
        "\n",
        "        # Buat direktori\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.gdrive_output_dir, exist_ok=True)  # Buat di Google Drive\n",
        "        os.makedirs(self.metadata_dir, exist_ok=True)\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üîç EKSTRAKSI METADATA\")\n",
        "        print(f\"Input: {self.cleaned_dir} atau {self.gdrive_cleaned_dir}\")\n",
        "        print(f\"Output Lokal: {self.output_dir}\")\n",
        "        print(f\"Output GDrive: {self.gdrive_output_dir}\")\n",
        "\n",
        "        # Setup ekstraksi patterns\n",
        "        self.setup_extraction_patterns()\n",
        "\n",
        "        # Mapping bulan Indonesia\n",
        "        self.month_mapping = {\n",
        "            'januari': '01', 'februari': '02', 'maret': '03', 'april': '04',\n",
        "            'mei': '05', 'juni': '06', 'juli': '07', 'agustus': '08',\n",
        "            'september': '09', 'oktober': '10', 'november': '11', 'desember': '12'\n",
        "        }\n",
        "\n",
        "    def setup_extraction_patterns(self):\n",
        "        \"\"\"Definisikan pola regex yang lebih spesifik untuk ekstraksi metadata\"\"\"\n",
        "\n",
        "        # 1. POLA NOMOR PERKARA (lebih spesifik)\n",
        "        self.case_number_patterns = [\n",
        "            r'(?:nomor|register)\\s*(?:perkara)?\\s*:\\s*(\\d+/pid\\.?(?:sus|b)?/?(?:\\d+)?/?pn\\.?\\w+)',  # Pidana\n",
        "            r'(?:nomor|register)\\s*(?:perkara)?\\s*:\\s*(\\d+/pdt\\.?g?/?(?:\\d+)?/?pn\\.?\\w+)',  # Perdata\n",
        "            r'(?:nomor|register)\\s*(?:perkara)?\\s*:\\s*(\\d+/\\w+/\\d{4}/pn\\.?\\w+)',  # Format lengkap\n",
        "            r'perkara\\s+nomor\\s*:\\s*(\\d+/[\\w\\./]+)',\n",
        "            r'dalam\\s+perkara\\s+nomor\\s*:\\s*([^\\n\\r]+?)(?:\\s|$)',\n",
        "        ]\n",
        "\n",
        "        # 2. POLA TANGGAL (lebih kontekstual)\n",
        "        self.date_patterns = [\n",
        "            r'diputuskan?\\s+(?:pada\\s+(?:hari\\s+\\w+\\s+)?)?tanggal\\s*:?\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "            r'dibacakan\\s+(?:pada\\s+(?:hari\\s+\\w+\\s+)?)?tanggal\\s*:?\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "            r'pada\\s+hari\\s+\\w+\\s+tanggal\\s+(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "            r'(?:jakarta|surabaya|bandung|medan|semarang|yogyakarta|makassar|palembang|denpasar|malang),?\\s*(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',\n",
        "        ]\n",
        "\n",
        "        # 3. POLA JENIS PERKARA (lebih spesifik dengan konteks)\n",
        "        self.case_type_patterns = [\n",
        "            r'(?:dalam\\s+)?perkara\\s+tindak\\s+pidana\\s+perdagangan\\s+orang',\n",
        "            r'(?:dalam\\s+)?perkara\\s+pidana\\s+(khusus|umum)',\n",
        "            r'(?:dalam\\s+)?perkara\\s+(pidana|perdata)(?:\\s+(?:khusus|umum))?',\n",
        "            r'tindak\\s+pidana\\s+perdagangan\\s+orang',\n",
        "        ]\n",
        "\n",
        "\n",
        "        # 4. POLA PASAL HUKUM (dengan konteks UU)\n",
        "        self.legal_article_patterns = [\n",
        "            r'pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?(?:\\s+(?:jo\\.?|juncto|dan)\\s+pasal\\s+\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)*)\\s+(?:undang[- ]undang|uu)',\n",
        "            r'melanggar\\s+pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)',\n",
        "            r'berdasarkan\\s+pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)',\n",
        "            r'undang[- ]undang\\s+(?:republik\\s+indonesia\\s+)?nomor\\s+(\\d+)\\s+tahun\\s+(\\d{4})',\n",
        "            r'uu\\s+(?:no\\.?\\s*|nomor\\s+)?(\\d+)/?(\\d{4})',\n",
        "        ]\n",
        "\n",
        "        # 5. POLA PIHAK-PIHAK (dengan delimiter yang jelas)\n",
        "        self.parties_patterns = [\n",
        "            # Terdakwa (format yang umum)\n",
        "            r'terdakwa\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*alias|,\\s*yang\\s+selanjutnya|;|\\n))',\n",
        "            r'nama\\s+lengkap\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "\n",
        "            # Penuntut Umum\n",
        "            r'(?:jaksa\\s+)?penuntut\\s+umum\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*S\\.H|,|\\n))',\n",
        "\n",
        "            # Perkara Perdata\n",
        "            r'penggugat\\s*(?:I|1)?\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "            r'tergugat\\s*(?:I|1)?\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "            r'pemohon\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "            r'termohon\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,|\\n))',\n",
        "\n",
        "            # Hakim\n",
        "            r'hakim\\s+ketua\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*S\\.H|,|\\n))',\n",
        "            r'hakim\\s+anggota\\s*:\\s*([A-Z][^,;\\n\\r]+?)(?:\\s*(?:,\\s*S\\.H|,|\\n))',\n",
        "        ]\n",
        "\n",
        "        # 6. POLA PENGADILAN (dengan nama lokasi yang jelas)\n",
        "        self.court_patterns = [\n",
        "            r'pengadilan\\s+negeri\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
        "            r'pengadilan\\s+tinggi\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
        "            r'mahkamah\\s+agung\\s+republik\\s+indonesia',\n",
        "            r'pengadilan\\s+(?:tata\\s+usaha\\s+negara|agama|militer)\\s+([A-Z][a-z]+)',\n",
        "        ]\n",
        "\n",
        "    def extract_case_number(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Ekstrak nomor perkara dengan validasi format\"\"\"\n",
        "        for pattern in self.case_number_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                case_number = match.group(1).strip()\n",
        "                # Validasi format nomor perkara\n",
        "                if re.match(r'\\d+/', case_number) and len(case_number) > 5:\n",
        "                    return case_number\n",
        "        return None\n",
        "\n",
        "    def extract_dates(self, text: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"Ekstrak tanggal-tanggal penting\"\"\"\n",
        "        dates = {'decision_date': None, 'hearing_date': None}\n",
        "\n",
        "        for pattern in self.date_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    day = match.group(1).zfill(2)\n",
        "                    month_name = match.group(2).lower()\n",
        "                    year = match.group(3)\n",
        "\n",
        "                    if month_name in self.month_mapping:\n",
        "                        month = self.month_mapping[month_name]\n",
        "                        formatted_date = f\"{year}-{month}-{day}\"\n",
        "\n",
        "                        context = text[max(0, match.start()-50):match.end()+50].lower()\n",
        "                        if 'diputuskan' in context or 'dibacakan' in context:\n",
        "                            dates['decision_date'] = formatted_date\n",
        "                        else:\n",
        "                            dates['hearing_date'] = formatted_date\n",
        "                except (IndexError, KeyError):\n",
        "                    continue\n",
        "\n",
        "        return dates\n",
        "\n",
        "    def extract_case_type(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Ekstrak jenis perkara\"\"\"\n",
        "        for pattern in self.case_type_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(0).strip()\n",
        "        return None\n",
        "\n",
        "    def extract_legal_articles(self, text: str) -> List[str]:\n",
        "        \"\"\"Ekstrak pasal dan undang-undang dengan validasi\"\"\"\n",
        "        articles = []\n",
        "        for pattern in self.legal_article_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                article = match.group(0).strip()\n",
        "\n",
        "                # Validasi artikel hukum (harus mengandung angka)\n",
        "                if re.search(r'\\d+', article) and len(article) > 5:\n",
        "                    # Bersihkan artikel dari kata yang tidak perlu\n",
        "                    article = re.sub(r'\\s+', ' ', article)\n",
        "                    if article not in articles:\n",
        "                        articles.append(article)\n",
        "\n",
        "        return articles[:10]  # Batasi maksimal 10 pasal untuk menghindari noise\n",
        "\n",
        "    def clean_party_name(self, name: str) -> str:\n",
        "        \"\"\"Membersihkan nama dari gelar dan informasi tambahan\"\"\"\n",
        "        if not name:\n",
        "            return \"\"\n",
        "\n",
        "        # Daftar pattern untuk dibersihkan\n",
        "        clean_patterns = [\n",
        "            r',\\s*S\\.H\\.?.*$',\n",
        "            r',\\s*S\\.E\\.?.*$',\n",
        "            r',\\s*M\\.H\\.?.*$',\n",
        "            r',\\s*alias.*$',\n",
        "            r',\\s*bin\\s+.*$',\n",
        "            r',\\s*binti\\s+.*$',\n",
        "            r'\\s+yang\\s+selanjutnya.*$'\n",
        "        ]\n",
        "\n",
        "        cleaned_name = name.strip()\n",
        "        for pattern in clean_patterns:\n",
        "            cleaned_name = re.sub(pattern, '', cleaned_name, flags=re.IGNORECASE)\n",
        "\n",
        "        return cleaned_name.strip()\n",
        "\n",
        "    def extract_parties(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Ekstrak pihak-pihak yang terlibat dengan validasi nama\"\"\"\n",
        "        parties = {\n",
        "            'defendants': [],      # Terdakwa\n",
        "            'prosecutors': [],     # JPU\n",
        "            'plaintiffs': [],      # Penggugat/Pemohon\n",
        "            'respondents': [],     # Tergugat/Termohon\n",
        "            'judges': []           # Hakim\n",
        "        }\n",
        "\n",
        "        for pattern in self.parties_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                party_name = match.group(1).strip()\n",
        "                party_context = match.group(0).lower()\n",
        "\n",
        "                # Validasi nama (harus dimulai dengan huruf kapital dan minimal 3 karakter)\n",
        "                if not party_name or len(party_name) < 3 or not party_name[0].isupper():\n",
        "                    continue\n",
        "\n",
        "                # Bersihkan nama dari gelar dan informasi tambahan\n",
        "                party_name = self.clean_party_name(party_name)\n",
        "\n",
        "                if len(party_name) < 3:\n",
        "                    continue\n",
        "\n",
        "                # Kategorisasi berdasarkan konteks\n",
        "                if 'terdakwa' in party_context or 'nama lengkap' in party_context:\n",
        "                    parties['defendants'].append(party_name)\n",
        "                elif any(word in party_context for word in ['jaksa', 'penuntut']):\n",
        "                    parties['prosecutors'].append(party_name)\n",
        "                elif 'penggugat' in party_context or 'pemohon' in party_context:\n",
        "                    parties['plaintiffs'].append(party_name)\n",
        "                elif 'tergugat' in party_context or 'termohon' in party_context:\n",
        "                    parties['respondents'].append(party_name)\n",
        "                elif 'hakim' in party_context:\n",
        "                    parties['judges'].append(party_name)\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in parties:\n",
        "            parties[key] = list(dict.fromkeys(parties[key]))\n",
        "\n",
        "        return parties\n",
        "\n",
        "    def extract_court_info(self, text: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"Ekstrak informasi pengadilan\"\"\"\n",
        "        court_info = {'court_name': None, 'court_type': None, 'court_location': None}\n",
        "\n",
        "        for pattern in self.court_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                court_text = match.group(0).strip()\n",
        "                court_info['court_name'] = court_text\n",
        "\n",
        "                if 'negeri' in court_text.lower():\n",
        "                    court_info['court_type'] = 'Pengadilan Negeri'\n",
        "                    if match.groups():\n",
        "                        court_info['court_location'] = match.group(1).strip()\n",
        "                elif 'tinggi' in court_text.lower():\n",
        "                    court_info['court_type'] = 'Pengadilan Tinggi'\n",
        "                    if match.groups():\n",
        "                        court_info['court_location'] = match.group(1).strip()\n",
        "                elif 'mahkamah agung' in court_text.lower():\n",
        "                    court_info['court_type'] = 'Mahkamah Agung'\n",
        "\n",
        "                break\n",
        "\n",
        "        return court_info\n",
        "\n",
        "    def extract_metadata_from_text(self, text: str, filename: str) -> Dict:\n",
        "        \"\"\"Ekstrak semua metadata dari teks\"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return {}\n",
        "\n",
        "        metadata = {\n",
        "            'filename': filename,\n",
        "            'extraction_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        }\n",
        "\n",
        "        # Ekstrak komponen metadata\n",
        "        metadata['case_number'] = self.extract_case_number(text)\n",
        "        dates = self.extract_dates(text)\n",
        "        metadata.update(dates)\n",
        "        metadata['case_type'] = self.extract_case_type(text)\n",
        "        metadata['legal_articles'] = self.extract_legal_articles(text)\n",
        "        metadata['parties'] = self.extract_parties(text)\n",
        "        metadata['court_info'] = self.extract_court_info(text)\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def process_single_file(self, filename: str, source_dir: str) -> Optional[Dict]:\n",
        "        \"\"\"Proses file tunggal dengan validasi hasil\"\"\"\n",
        "        file_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.error(f\"File tidak ditemukan: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            if not text.strip():\n",
        "                logger.warning(f\"File kosong: {filename}\")\n",
        "                return None\n",
        "\n",
        "            metadata = self.extract_metadata_from_text(text, filename)\n",
        "\n",
        "            if metadata:\n",
        "                # Validasi minimal data yang diperlukan\n",
        "                has_case_number = bool(metadata.get('case_number'))\n",
        "                has_parties = any(metadata.get('parties', {}).values())\n",
        "                has_dates = bool(metadata.get('decision_date') or metadata.get('hearing_date'))\n",
        "\n",
        "                status = \"‚úÖ\" if (has_case_number or has_parties or has_dates) else \"‚ö†Ô∏è\"\n",
        "                print(f\"{status} {filename}\")\n",
        "                return metadata\n",
        "            else:\n",
        "                print(f\"‚ùå {filename}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error memproses {filename}: {str(e)}\")\n",
        "            print(f\"‚ùå {filename}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_text_files(self, directory: str) -> List[str]:\n",
        "        \"\"\"Dapatkan daftar file teks\"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            return []\n",
        "        return [f for f in os.listdir(directory)\n",
        "                if f.endswith('.txt') and os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    def save_metadata_to_csv(self, metadata_list: List[Dict]):\n",
        "        \"\"\"Simpan metadata ke CSV terstruktur di kedua lokasi\"\"\"\n",
        "        flattened_data = []\n",
        "\n",
        "        for metadata in metadata_list:\n",
        "            parties = metadata.get('parties', {})\n",
        "            court_info = metadata.get('court_info', {})\n",
        "\n",
        "            flat_record = {\n",
        "                # IDENTITAS DOKUMEN\n",
        "                'nama_file': metadata.get('filename'),\n",
        "                'tanggal_ekstraksi': metadata.get('extraction_timestamp'),\n",
        "\n",
        "                # IDENTITAS PERKARA\n",
        "                'nomor_perkara': metadata.get('case_number'),\n",
        "                'tanggal_putusan': metadata.get('decision_date'),\n",
        "                'tanggal_sidang': metadata.get('hearing_date'),\n",
        "                'jenis_perkara': metadata.get('case_type'),\n",
        "\n",
        "                # INFORMASI PENGADILAN\n",
        "                'nama_pengadilan': court_info.get('court_name'),\n",
        "                'jenis_pengadilan': court_info.get('court_type'),\n",
        "                'lokasi_pengadilan': court_info.get('court_location'),\n",
        "\n",
        "                # PIHAK-PIHAK TERKAIT\n",
        "                'pihak_penggugat': '; '.join(parties.get('plaintiffs', [])),\n",
        "                'pihak_tergugat': '; '.join(parties.get('respondents', [])),\n",
        "                'terdakwa': '; '.join(parties.get('defendants', [])),\n",
        "                'jaksa_penuntut_umum': '; '.join(parties.get('prosecutors', [])),\n",
        "                'hakim': '; '.join(parties.get('judges', [])),\n",
        "\n",
        "                # ASPEK HUKUM\n",
        "                'pasal_yang_dilanggar': '; '.join(metadata.get('legal_articles', [])),\n",
        "                'jumlah_pasal': len(metadata.get('legal_articles', [])),\n",
        "\n",
        "                # HITUNGAN PIHAK\n",
        "                'jumlah_penggugat': len(parties.get('plaintiffs', [])),\n",
        "                'jumlah_tergugat': len(parties.get('respondents', [])),\n",
        "                'jumlah_terdakwa': len(parties.get('defendants', [])),\n",
        "                'jumlah_hakim': len(parties.get('judges', []))\n",
        "            }\n",
        "            flattened_data.append(flat_record)\n",
        "\n",
        "        # Buat DataFrame\n",
        "        df = pd.DataFrame(flattened_data)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        csv_filename = f\"cases.csv\"\n",
        "\n",
        "        # Simpan ke direktori lokal (/data/processed)\n",
        "        csv_path_local = os.path.join(self.output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_local, index=False, encoding='utf-8')\n",
        "        print(f\"üìÑ CSV Lokal: {csv_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive (/content/drive/MyDrive/perdagangan_orang/data/processed)\n",
        "        csv_path_gdrive = os.path.join(self.gdrive_output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_gdrive, index=False, encoding='utf-8')\n",
        "        print(f\"üíæ CSV GDrive: {csv_path_gdrive}\")\n",
        "\n",
        "        return csv_path_local, csv_path_gdrive\n",
        "\n",
        "    def save_metadata_to_json(self, metadata_list: List[Dict]):\n",
        "        \"\"\"Simpan metadata ke JSON lengkap di kedua lokasi\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        json_filename = f\"cases.json\"\n",
        "\n",
        "        # Simpan ke direktori lokal\n",
        "        json_path_local = os.path.join(self.output_dir, json_filename)\n",
        "        with open(json_path_local, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üìÑ JSON Lokal: {json_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive\n",
        "        json_path_gdrive = os.path.join(self.gdrive_output_dir, json_filename)\n",
        "        with open(json_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üíæ JSON GDrive: {json_path_gdrive}\")\n",
        "\n",
        "        return json_path_local, json_path_gdrive\n",
        "\n",
        "    def process_all_files(self) -> List[Dict]:\n",
        "        \"\"\"Proses semua file untuk ekstraksi metadata\"\"\"\n",
        "        print(\"üîç i. EKSTRAKSI METADATA\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Mengambil metadata: Nomor Perkara, Tanggal, Jenis Perkara, Pasal, Pihak, dll.\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Cari file dari kedua lokasi\n",
        "        data_raw_files = self.get_text_files(self.cleaned_dir)\n",
        "        gdrive_files = self.get_text_files(self.gdrive_cleaned_dir)\n",
        "\n",
        "        if data_raw_files:\n",
        "            files_to_process = data_raw_files\n",
        "            source_directory = self.cleaned_dir\n",
        "            print(f\"üìÇ Menggunakan file dari: {self.cleaned_dir}\")\n",
        "        elif gdrive_files:\n",
        "            files_to_process = gdrive_files\n",
        "            source_directory = self.gdrive_cleaned_dir\n",
        "            print(f\"üìÇ Menggunakan file dari: {self.gdrive_cleaned_dir}\")\n",
        "        else:\n",
        "            print(\"‚ùå Tidak ada file teks yang ditemukan!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"üìÅ Ditemukan {len(files_to_process)} file untuk diproses\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Proses setiap file\n",
        "        all_metadata = []\n",
        "        success_count = 0\n",
        "\n",
        "        for i, filename in enumerate(files_to_process, 1):\n",
        "            print(f\"[{i:3d}/{len(files_to_process)}] \", end=\"\")\n",
        "            metadata = self.process_single_file(filename, source_directory)\n",
        "\n",
        "            if metadata:\n",
        "                all_metadata.append(metadata)\n",
        "                success_count += 1\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"‚úÖ BERHASIL: {success_count} file\")\n",
        "        print(f\"‚ùå GAGAL: {len(files_to_process) - success_count} file\")\n",
        "\n",
        "        if all_metadata:\n",
        "            # Simpan ke CSV dan JSON di kedua lokasi\n",
        "            csv_paths = self.save_metadata_to_csv(all_metadata)\n",
        "            json_paths = self.save_metadata_to_json(all_metadata)\n",
        "\n",
        "            print(f\"üìä Total metadata berhasil diekstrak: {len(all_metadata)} record\")\n",
        "            print(f\"üíæ File tersimpan di 2 lokasi: lokal & Google Drive\")\n",
        "\n",
        "        return all_metadata\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk menjalankan ekstraksi metadata\"\"\"\n",
        "    print(\"üöÄ MULAI EKSTRAKSI METADATA PUTUSAN PENGADILAN\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        extractor = MetadataExtractor()\n",
        "        metadata_results = extractor.process_all_files()\n",
        "\n",
        "        if metadata_results:\n",
        "            print(\"\\nüéâ EKSTRAKSI METADATA SELESAI!\")\n",
        "            print(f\"Total metadata: {len(metadata_results)} record\")\n",
        "            print(\"File output tersimpan di:\")\n",
        "            print(\"  - Lokal: /data/processed/\")\n",
        "            print(\"  - GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Tidak ada metadata yang berhasil diekstrak.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd__tIwK9wZ5",
        "outputId": "409a8633-78ff-4d0c-c585-83d0ec95c175"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ MULAI EKSTRAKSI METADATA PUTUSAN PENGADILAN\n",
            "======================================================================\n",
            "üîç EKSTRAKSI METADATA\n",
            "Input: /data/raw atau /content/drive/MyDrive/perdagangan_orang/CLEANED\n",
            "Output Lokal: /data/processed\n",
            "Output GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed\n",
            "üîç i. EKSTRAKSI METADATA\n",
            "============================================================\n",
            "Mengambil metadata: Nomor Perkara, Tanggal, Jenis Perkara, Pasal, Pihak, dll.\n",
            "============================================================\n",
            "üìÇ Menggunakan file dari: /content/drive/MyDrive/perdagangan_orang/CLEANED\n",
            "üìÅ Ditemukan 79 file untuk diproses\n",
            "------------------------------------------------------------\n",
            "[  1/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_145_PID_SUS_2021_PT_MTR_Tanggal_20_Desember_2021__Pembanding_Penuntut_Umum___MANIK_ARTHA_ADHITAMA__SHTerbanding_Terdakwa___Herman_Saputra_Rafiudin_Alias_Herman.txt\n",
            "[  2/79] ‚úÖ case_2021_TK1_Putusan_PN_PELAIHARI_Nomor_179_Pid_Sus_2021_PN_Pli_Tanggal_16_Desember_2021__Penuntut_Umum_ANDI_HAMZAH_KUSUMAATMAJA__S_HTerdakwa_M__NOOR_Als_NUNUI_Bin_KHAIRI.txt\n",
            "[  3/79] ‚ö†Ô∏è case_2021_TK1_Putusan_PT_MATARAM_Nomor_140_PID_SUS_2021_PT_MTR_Tanggal_9_Desember_2021__Pembanding_Penuntut_Umum_I___HENDRO_S_I_B__SH_Terbanding_Terdakwa___BQ_DIAN_CINDRAWATI_Alias_DIAN.txt\n",
            "[  4/79] ‚úÖ case_2021_TK1_Putusan_PN_BALIKPAPAN_Nomor_412_Pid_Sus_2021_PN_Bpp_Tanggal_30_Nopember_2021__Penuntut_Umum_Ita_Wahyuning_Lestari__SH_Terdakwa_JEKSON_RAJAGUKGUK_Alias_JECO_Anak_dari_ALBERT_RAJAGUKGUK.txt\n",
            "[  5/79] ‚úÖ case_2021_TK1_Putusan_PT_SAMARINDA_Nomor_240_PID_2021_PT_SMR_Tanggal_26_Nopember_2021__Pembanding_Terbanding_Terdakwa___DEVITA_ARIYANI_Als_DORA_Binti_MUSTOPA_Diwakili_Oleh___Nunung_Tri_Sulistiawa__S_H_.txt\n",
            "[  6/79] ‚úÖ case_2021_TK1_Putusan_PT_PEKANBARU_Nomor_494_PID_SUS_2021_PT_PBR_Tanggal_17_Nopember_2021__Pembanding_Terbanding_Terdakwa___EKO_SUMBARA_Alias_EKO_Bin_MUHMMAD_NASIR_Alm_Diwakili_Oleh___ANDI_NUGRAHG__SH_.txt\n",
            "[  7/79] ‚úÖ case_2021_TK1_Putusan_PN_MAKALE_Nomor_92_Pid_Sus_2021_PN_Mak_Tanggal_15_Nopember_2021__Penuntut_Umum_MARGARETHA_H__PATURU__S_H_Terdakwa_SRI_SUNARTI_alias_MAMI.txt\n",
            "[  8/79] ‚úÖ case_2021_TK1_Putusan_PN_MAKALE_Nomor_93_Pid_Sus_2021_PN_Mak_Tanggal_15_Nopember_2021__Penuntut_Umum_MARGARETHA_H__PATURU__S_H_Terdakwa_WIWIN_ALIAS_VALEN.txt\n",
            "[  9/79] ‚úÖ case_2021_TK1_Putusan_PN_SURABAYA_Nomor_1969_Pid_Sus_2021_PN_Sby_Tanggal_8_Nopember_2021__Penuntut_Umum_DEDDY_ARISANDI__SH__MHTerdakwa_HENDRI_YULIANSYAH_BIN_ALM_BUTRI_SYAMSI.txt\n",
            "[ 10/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_120_PID_SUS_2021_PT_MTR_Tanggal_8_Nopember_2021__Pembanding_Terbanding_Terdakwa___RATNI__SH_Alias_RANITerbanding_Pembanding_Penuntut_Umum___FEDDY_HANTYO_NUG__M_H_.txt\n",
            "[ 11/79] ‚úÖ case_2021_TK1_Putusan_PN_TANGERANG_Nomor_1614_Pid_Sus_2021_PN_Tng_Tanggal_3_Nopember_2021__Penuntut_Umum_HADI_WIDODO__SHTerdakwa_1_SUBUR_RAHARJO_Bin_SUGITO2_AMAR_SAHIDIN_Als_ABANG_Bin_WARSITO.txt\n",
            "[ 12/79] ‚úÖ case_2021_TK1_Putusan_PN_INDRAMAYU_Nomor_214_Pid_Sus_2021_PN_Idm_Tanggal_21_Oktober_2021__Penuntut_Umum_1_M__ICHSAN__S_H___M_H_2_TISNA_P__WIJAYA__SHTerdakwa_1_ISMAEL_IBRAHIM_KHALEEL__alias_ISMAILI_AJAT.txt\n",
            "[ 13/79] ‚úÖ case_2021_TK1_Putusan_PN_INDRAMAYU_Nomor_213_Pid_Sus_2021_PN_Idm_Tanggal_21_Oktober_2021__Penuntut_Umum_JIHANTO_NUR_RACHMAN__SHTerdakwa_1_ULFIYATI_Alias_ULFI_Binti_SUTIMAN2_MAHFUDZ_SIDDIQ_Alias_M_DAKIM.txt\n",
            "[ 14/79] ‚úÖ case_2021_TK1_Putusan_PN_INDRAMAYU_Nomor_215_Pid_Sus_2021_PN_Idm_Tanggal_21_Oktober_2021__Penuntut_Umum_1_M__ICHSAN__S_H___M_H_2_TISNA_P__WIJAYA__SHTerdakwa_1_ANDI_SOPANDI_alias_ANDI_BIN_UUM_2_DAUSTADI.txt\n",
            "[ 15/79] ‚úÖ case_2025_TK1_Putusan_PA_LUBUK_PAKAM_Nomor_2063_Pdt_G_2025_PA_Lpk_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 16/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_2898_Pdt_G_2025_PA_Tgrs_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 17/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_3022_Pdt_G_2025_PA_Tgrs_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 18/79] ‚úÖ case_2021_TK1_Putusan_PT_BANTEN_Nomor_108_PID_SUS_2021_PT_BTN_Tanggal_18_Oktober_2021__Pembanding_Penuntut_Umum___AGUSTRI_HARTONO__SH__MHTerbanding_Terdakwa___TOFIK_TRIYATNO_Bin_TASMIARJO_ALM.txt\n",
            "[ 19/79] ‚úÖ case_2021_TK1_Putusan_PN_PALOPO_Nomor_114_Pid_Sus_2021_PN_Plp_Tanggal_12_Oktober_2021__Penuntut_Umum_1_YANUAR_FIHAWIANO_SH2_AHMAD_SULHAN_S_H3_Erlysa_Said__S_H_Terdakwa_MELFI_INDIRIATI_PUTRI_Als__S_BATO.txt\n",
            "[ 20/79] ‚úÖ case_2021_TK1_Putusan_PN_MATARAM_Nomor_467_Pid_Sus_2021_PN_Mtr_Tanggal_30_September_2021__Penuntut_Umum_1_HENDRO_SAYEKTI_SH_2_M_BUSTANUL__ARIFIN_SH_MH_3_MOCH__TAUFIQ_ISMAIL__SHTerdakwa_PANDRI__AZ_ANDRE.txt\n",
            "[ 21/79] ‚úÖ case_2021_TK1_Putusan_PN_CILACAP_Nomor_197_Pid_Sus_2021_PN_Clp_Tanggal_30_September_2021__Penuntut_Umum_Santa_Novena_Christy_SHTerdakwa_GULIYAH_Binti_Alm_TEGUH_SUPARDI.txt\n",
            "[ 22/79] ‚úÖ case_2021_TK1_Putusan_PN_PURWOKERTO_Nomor_124_Pid_Sus_2021_PN_Pwt_Tanggal_7_September_2021__Penuntut_Umum_MARYANI_WIDIYASTUTITerdakwa_OSI_NAVITALIA_ALS_OCI_BINTI_SUNARTO.txt\n",
            "[ 23/79] ‚úÖ case_2021_TK1_Putusan_PN_PURWOKERTO_Nomor_126_Pid_Sus_2021_PN_Pwt_Tanggal_7_September_2021__Penuntut_Umum_MARYANI_WIDIYASTUTITerdakwa_KARMANESA_FEBRIARI_ALS_ESA_BIN_NIAT_NGUDIANTO.txt\n",
            "[ 24/79] ‚úÖ case_2025_TK1_Putusan_PA_Ngamprah_Nomor_1378_Pdt_G_2025_PA_Nph_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 25/79] ‚úÖ case_2025_TK1_Putusan_PA_Ngamprah_Nomor_1205_Pdt_G_2025_PA_Nph_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 26/79] ‚úÖ case_2021_TK1_Putusan_PN_PURWOKERTO_Nomor_125_Pid_Sus_2021_PN_Pwt_Tanggal_7_September_2021__Penuntut_Umum_MARYANI_WIDIYASTUTITerdakwa_JEFRI_TOMS_PARDIANTO_ALS_JEFRI_ALS_GATEL_BIN_PARDIMAN.txt\n",
            "[ 27/79] ‚úÖ case_2021_TK1_Putusan_PN_ROKAN_HILIR_Nomor_234_Pid_Sus_2021_PN_Rhl_Tanggal_6_September_2021__Penuntut_Umum_1_MARULITUA_J__SITANGGANG__SH_2_YONGKI_ARVIUS__S_H_MHTerdakwa_EKO_SUMBARA_Alias_EKO_Bin_IR_Alm.txt\n",
            "[ 28/79] ‚úÖ case_2021_TK1_Putusan_PT_BANTEN_Nomor_92_PID_SUS_2021_PT_BTN_Tanggal_31_Agustus_2021__Pembanding_Penuntut_Umum___GORUT_PERTHIKA__SHTerbanding_Terdakwa_I___MAYANG_APRILLA_RAHMAYANTI_als_MAMI_APRILA_CHOI.txt\n",
            "[ 29/79] ‚úÖ case_2021_TK1_Putusan_PN_Ngabang_Nomor_65_Pid_Sus_2021_PN_Nba_Tanggal_30_Agustus_2021__Penuntut_Umum_Pewira_Saputra_SHTerdakwa_Wan_Wan_Anak_dari_Alm_Liu_Po_Fha.txt\n",
            "[ 30/79] ‚úÖ case_2021_TK1_Putusan_PN_Ngabang_Nomor_64_Pid_Sus_2021_PN_Nba_Tanggal_30_Agustus_2021__Penuntut_Umum_Pewira_Saputra_SHTerdakwa_Susanti_Alias_Aling_Anak_Dari_Siau_Ket_Loy.txt\n",
            "[ 31/79] ‚úÖ case_2021_TK1_Putusan_PN_JAKARTA_UTARA_Nomor_596_Pid_Sus_2021_PN_Jkt_Utr_Tanggal_16_Agustus_2021__Penuntut_Umum_DYOFA_YUDHISTIRA__SHTerdakwa_ALI_NURUDIN_ALIAS_ALI_.txt\n",
            "[ 32/79] ‚úÖ case_2021_TK1_Putusan_PN_JAKARTA_UTARA_Nomor_595_Pid_Sus_2021_PN_Jkt_Utr_Tanggal_16_Agustus_2021__Penuntut_Umum_DYOFA_YUDHISTIRA__SHTerdakwa_YUDHISTIRA_ARMIN_ALIAS_YUDI.txt\n",
            "[ 33/79] ‚úÖ case_2021_TK1_Putusan_PN_MANDAILING_NATAL_Nomor_87_Pid_Sus_2021_PN_Mdl_Tanggal_9_Agustus_2021__Penuntut_Umum_1_NURHAYATI_PULUNGAN__SH2_HERIYANTO_MANURUNG__SHTerdakwa_FITRIANI_ALIAS_JABUKE.txt\n",
            "[ 34/79] ‚úÖ case_2021_TK1_Putusan_PN_BENGKULU_Nomor_227_Pid_Sus_2021_PN_Bgl_Tanggal_22_Juli_2021__Penuntut_Umum_SRI_RAHMITerdakwa_RIWANSYAH__S_Pd_Als_RIWAN_Als_CIN_Als_MAMI_Bin_YASUR_I.txt\n",
            "[ 35/79] ‚ö†Ô∏è case_2025_TK1_Putusan_PA_Sei_Rampah_Nomor_648_Pdt_G_2025_PA_Srh_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 36/79] ‚ö†Ô∏è case_2025_TK1_Putusan_PA_Sei_Rampah_Nomor_655_Pdt_G_2025_PA_Srh_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 37/79] ‚ö†Ô∏è case_2025_TK1_Putusan_PA_Sei_Rampah_Nomor_660_Pdt_G_2025_PA_Srh_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 38/79] ‚úÖ case_2021_TK1_Putusan_PN_PASANGKAYU_Nomor_78_Pid_Sus_2021_PN_Pky_Tanggal_21_Juli_2021__Penuntut_Umum_HAFIZ_AKBAR_RITONGA__SHTerdakwa_NURSANTI_alias_BUNDA_binti_TASWIN_MOITA.txt\n",
            "[ 39/79] ‚úÖ case_2021_TK1_Putusan_PN_PASANGKAYU_Nomor_79_Pid_Sus_2021_PN_Pky_Tanggal_21_Juli_2021__Penuntut_Umum_FRI_HARMOKO__SH__MHTerdakwa_RUHANI_alias_RIFKA_binti_ABD__LATIF.txt\n",
            "[ 40/79] ‚úÖ case_2021_TK1_Putusan_PN_SURABAYA_Nomor_1135_Pid_Sus_2021_PN_Sby_Tanggal_1_Juli_2021__Penuntut_Umum_SULFIKAR__SHTerdakwa_NUR_RAHMAT_KISWO_PRANGGONO_BIN_SENO_BT_PRANGGONO.txt\n",
            "[ 41/79] ‚úÖ case_2021_TK1_Putusan_PT_KUPANG_Nomor_82_PID_2021_PT_KPG_Tanggal_30_Juni_2021__Pembanding_Terbanding_Terdakwa_II___YOPPI_NALLETerbanding_Pembanding_Penuntut_Umum___CHRISTOFEL_H__MALLAKA__S_HTerbaSEMUEL.txt\n",
            "[ 42/79] ‚ö†Ô∏è case_2021_TK1_Putusan_PT_KUPANG_Nomor_77_PID_2021_PT_KPG_Tanggal_30_Juni_2021__Pembanding_Terdakwa_I___YOPPI_NALLETerbanding_Penuntut_Umum___CHRISTOFEL_H__MALLAKA__S_H.txt\n",
            "[ 43/79] ‚úÖ case_2021_TK1_Putusan_PN_JAKARTA_UTARA_Nomor_341_Pid_Sus_2021_PN_Jkt_Utr_Tanggal_9_Juni_2021__Penuntut_Umum_ERNI_PRAMOTI__SHTerdakwa_ARDIAN_FIRMANSYAH_BIN_IWONG_TASWAN_.txt\n",
            "[ 44/79] ‚úÖ case_2021_TK1_Putusan_PN_MAJALENGKA_Nomor_59_Pid_Sus_2021_PN_Mjl_Tanggal_2_Juni_2021__Penuntut_Umum_ADE_MULYANI__SHTerdakwa_AHMAD_MUAMAR_Alias_AMAR_bin_MUHDLOR.txt\n",
            "[ 45/79] ‚úÖ case_2021_TK1_Putusan_PN_MAJALENGKA_Nomor_58_Pid_Sus_2021_PN_Mjl_Tanggal_2_Juni_2021__Penuntut_Umum_DANU_TRISNAWANTO__S_H_Terdakwa_AGUNG_SUBEKTI_Bin_DURIA.txt\n",
            "[ 46/79] ‚úÖ case_2025_TK1_Putusan_PA_PONOROGO_Nomor_883_Pdt_G_2025_PA_Po_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 47/79] ‚úÖ case_2025_TK1_Putusan_PA_PONOROGO_Nomor_188_Pdt_P_2025_PA_Po_Tanggal_25_Juni_2025__Pemohon_melawan_Termohon.txt\n",
            "[ 48/79] ‚úÖ case_2025_TK1_Putusan_PA_PONOROGO_Nomor_746_Pdt_G_2025_PA_Po_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 49/79] ‚úÖ case_2021_TK1_Putusan_PN_DENPASAR_Nomor_211_Pid_Sus_2021_PN_Dps_Tanggal_25_Mei_2021__Penuntut_Umum_Dewi_Agustin_Adiputri__SH_MHTerdakwa_Maulana_Aldi.txt\n",
            "[ 50/79] ‚ö†Ô∏è case_2021_TK1_Putusan_PN_SURABAYA_Nomor_575_Pid_Sus_2021_PN_Sby_Tanggal_24_Mei_2021__Penuntut_Umum_DZULKIFLY_NENTO__SHTerdakwa_RICO_LINGGAR_JAYA_BIN_EDI_WAHYONO.txt\n",
            "[ 51/79] ‚úÖ case_2021_TK1_Putusan_PN_TENGGARONG_Nomor_141_Pid_Sus_2021_PN_Trg_Tanggal_4_Mei_2021__Penuntut_Umum_FITRI_IRA_P__SH_Terdakwa_SUNIYE_Als_SOIMAH_Binti_ARIF.txt\n",
            "[ 52/79] ‚úÖ case_2021_TK1_Putusan_PN_BENGKULU_Nomor_87_Pid_Sus_2021_PN_Bgl_Tanggal_4_Mei_2021__Penuntut_Umum_J_HUTAGAOL_SH_MHTerdakwa_HIKMAT_DEKI_Als_DEKI_Als_MAMI_SHISI_Bin_WAHIDIN.txt\n",
            "[ 53/79] ‚úÖ case_2021_TK1_Putusan_PN_TENGGARONG_Nomor_142_Pid_Sus_2021_PN_Trg_Tanggal_4_Mei_2021__Penuntut_Umum_FITRI_IRA_P__SH_Terdakwa_RANI_Als_RANI_RAHAYU_Alias_DIFA_Binti_IPAR.txt\n",
            "[ 54/79] ‚úÖ case_2021_TK1_Putusan_PN_TEGAL_Nomor_22_Pid_Sus_2021_PN_Tgl_Tanggal_4_Mei_2021__Penuntut_Umum_1_Haerati__SH2_GRETA_ANASTASIA__S_H__M_H_3_Intan_Kafa_Arbina__SH_MHTerdakwa_MUAMAR_KADAFI.txt\n",
            "[ 55/79] ‚úÖ case_2021_TK1_Putusan_PN_TANJUNG_SELOR_Nomor_19_Pid_Sus_2021_PN_Tjs_Tanggal_28_April_2021__Penuntut_Umum_DANU_BAGUS_PRATAMA__S_HTerdakwa_MUHAMAD_SAFRIANSYAH_Als_BULOT_Bin_MILI.txt\n",
            "[ 56/79] ‚úÖ case_2021_TK1_Putusan_PT_PADANG_Nomor_70_PID_SUS_2021_PT_PDG_Tanggal_27_April_2021__Pembanding_Penuntut_Umum_I___MEILYA_TRISNA__SH__MHTerbanding_Terdakwa___DIAN_EKA_PUTRA_Pgl__DIAN.txt\n",
            "[ 57/79] ‚úÖ case_2021_TK1_Putusan_PN_SUBANG_Nomor_66_Pid_Sus_2021_PN_SNG_Tanggal_27_April_2021__Penuntut_Umum_ADITYO_ISMUTOMO__SH_Terdakwa_WANAP_als_MANAP_bin_TAKIM.txt\n",
            "[ 58/79] ‚úÖ case_2021_TK1_Putusan_PN_AMURANG_Nomor_5_Pid_Sus_2021_PN_Amr_Tanggal_22_April_2021__Penuntut_Umum_M__REZA_PAHLEPI__SHTerdakwa_VICKY_FERNANDO_BAHIHI_alias_VIKI.txt\n",
            "[ 59/79] ‚úÖ case_2021_TK1_Putusan_PN_SANGGAU_Nomor_75_Pid_Sus_2021_PN_Sag_Tanggal_22_April_2021__Penuntut_Umum_MIFA_AL_FAHMI__S_H_Terdakwa_BAKRI_Bin_CICUK_Alm.txt\n",
            "[ 60/79] ‚úÖ case_2021_TK1_Putusan_PN_AMURANG_Nomor_6_Pid_Sus_2021_PN_Amr_Tanggal_22_April_2021__Penuntut_Umum_M__REZA_PAHLEPI__SHTerdakwa_RICKY_JUNIOR_TUMBELAKA_alias_RIKI.txt\n",
            "[ 61/79] ‚úÖ case_2021_TK1_Putusan_PN_AMURANG_Nomor_7_Pid_Sus_2021_PN_Amr_Tanggal_22_April_2021__Penuntut_Umum_M__REZA_PAHLEPI__SHTerdakwa_RIJAL_SUMAMPOW_alias_JAL.txt\n",
            "[ 62/79] ‚úÖ case_2025_TK1_Putusan_PA_LAMONGAN_Nomor_1419_Pdt_G_2025_PA_Lmg_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 63/79] ‚úÖ case_2025_TK1_Putusan_PA_LAMONGAN_Nomor_1375_Pdt_G_2025_PA_Lmg_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 64/79] ‚úÖ case_2025_TK1_Putusan_PA_LAMONGAN_Nomor_1436_Pdt_G_2025_PA_Lmg_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 65/79] ‚ö†Ô∏è case_2021_TK1_Putusan_PN_ENDE_Nomor_11_Pid_Sus_2021_PN_End_Tanggal_19_April_2021__Penuntut_Umum_1_OKKY_PRASETYO_AJIE2_TERESIA_WEKO__SHTerdakwa_STEFANUS_KUASA_Alias_EFAN.txt\n",
            "[ 66/79] ‚úÖ case_2021_TK1_Putusan_PN_ENDE_Nomor_10_Pid_Sus_2021_PN_End_Tanggal_19_April_2021__Penuntut_Umum_1_OKKY_PRASETYO_AJIE2_TERESIA_WEKO__SHTerdakwa_MARIA_YUNIANTI_JAGONG_Alias_YUNI.txt\n",
            "[ 67/79] ‚úÖ case_2021_TK1_Putusan_PN_BANYUWANGI_Nomor_76_Pid_Sus_2021_PN_Byw_Tanggal_12_April_2021__Penuntut_Umum_1_I_KETUT_GDE_DAME_NEGARA__SH2_GANDHI_MUCHLISIN__S_H_Terdakwa_SUWITO__Als__PAK_TO_Bin_MUSIMAN.txt\n",
            "[ 68/79] ‚úÖ case_2021_TK1_Putusan_PN_PANGKALAN_BUN_Nomor_57_Pid_Sus_2021_PN_Pbu_Tanggal_7_April_2021__Penuntut_Umum_1_GOMGOMAN_H_SIMBOLON__S_H___M_H_2_GANES_ADI_KUSUMA__S_H_Terdakwa_RINDA_EVANNA_HOTMAULI_SIAIAHAAN.txt\n",
            "[ 69/79] ‚úÖ case_2021_TK1_Putusan_PN_SAMBAS_Nomor_22_Pid_Sus_2021_PN_Sbs_Tanggal_22_Maret_2021__Penuntut_Umum_1_Muhammad_Nur_Faisal_Wijaya__S_H_2_I_in_Lindayani__S_H___M_H_Terdakwa_RIKKY_OKTADO_Als_RIKI_Bin_N__Alm.txt\n",
            "[ 70/79] ‚úÖ case_2021_TK1_Putusan_PN_SUBANG_Nomor_40_Pid_Sus_2021_PN_SNG_Tanggal_17_Maret_2021__Penuntut_Umum_AZAM_AKHMAD_AKHSYA__S_H_Terdakwa_RIZAL_FIKRI_NURROHIMUDIN.txt\n",
            "[ 71/79] ‚ö†Ô∏è case_2021_TK1_Putusan_PN_KUALA_SIMPANG_Nomor_22_Pid_Sus_2021_PN_Ksp_Tanggal_8_Maret_2021__Penuntut_Umum_MARIONO__SH_MHTerdakwa_HENGKIE_BIN_EFENDI.txt\n",
            "[ 72/79] ‚ö†Ô∏è case_2021_TK1_Putusan_PN_KUALA_SIMPANG_Nomor_23_Pid_Sus_2021_PN_Ksp_Tanggal_8_Maret_2021__Penuntut_Umum_MARIONO__SH_MHTerdakwa_2_SUPRAYETNI_Binti_Alm_KASIMIN3_RAZALI_Bin_alm_SULAIMAN4_ROSMINI_BinTUBARA.txt\n",
            "[ 73/79] ‚úÖ case_2021_TK1_Putusan_PT_MANADO_Nomor_10_PID_SUS_2021_PT_MND_Tanggal_25_Februari_2021__Pembanding_Penuntut_Umum___JENNY_R_WAYONG__SHTerbanding_Terdakwa___MICHAEL_UMBOH.txt\n",
            "[ 74/79] ‚úÖ case_2021_TK1_Putusan_PT_YOGYAKARTA_Nomor_12_PID_SUS_2021_PT_YYK_Tanggal_18_Februari_2021__Pembanding_Penuntut_Umum___AGUS_KURNIAWAN_SHTerbanding_Terdakwa___SITI_FATIMAH_Als__NADIRA_Als__MBAK_MBUL.txt\n",
            "[ 75/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_12_PID_SUS_2021_PT_MTR_Tanggal_16_Februari_2021__Pembanding_Penuntut_Umum_I___SAHDI_SH_Terbanding_Terdakwa___H_HUSNUL_ANSORI_ALIAS_H_ANSORI.txt\n",
            "[ 76/79] ‚úÖ case_2021_TK1_Putusan_PT_MANADO_Nomor_4_PID_2021_PT_MND_Tanggal_10_Februari_2021__Identitas_Pihak_Tidak_Dipublikasi.txt\n",
            "[ 77/79] ‚úÖ case_2021_TK1_Putusan_PN_SAMBAS_Nomor_7_Pid_Sus_2021_PN_Sbs_Tanggal_9_Februari_2021__Penuntut_Umum_1_Meirita_Pakpahan__S_H_2_Salomo_Saing__S_H___M_H_Terdakwa_ELISA_BINTI_NAJIR.txt\n",
            "[ 78/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_2726_Pdt_G_2025_PA_Tgrs_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "[ 79/79] ‚ö†Ô∏è case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_3026_Pdt_G_2025_PA_Tgrs_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt\n",
            "------------------------------------------------------------\n",
            "‚úÖ BERHASIL: 79 file\n",
            "‚ùå GAGAL: 0 file\n",
            "üìÑ CSV Lokal: /data/processed/cases.csv\n",
            "üíæ CSV GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/cases.csv\n",
            "üìÑ JSON Lokal: /data/processed/cases.json\n",
            "üíæ JSON GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/cases.json\n",
            "üìä Total metadata berhasil diekstrak: 79 record\n",
            "üíæ File tersimpan di 2 lokasi: lokal & Google Drive\n",
            "\n",
            "üéâ EKSTRAKSI METADATA SELESAI!\n",
            "Total metadata: 79 record\n",
            "File output tersimpan di:\n",
            "  - Lokal: /data/processed/\n",
            "  - GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ekstraksi Konten Kunci**"
      ],
      "metadata": {
        "id": "aEh7BzxNAG1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import logging"
      ],
      "metadata": {
        "id": "RrwPcpwWAKAV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class KontenKunciExtractor:\n",
        "    \"\"\"Ekstrak konten kunci dari dokumen putusan pengadilan\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.cleaned_dir = \"/data/raw\"\n",
        "        self.gdrive_cleaned_dir = os.path.join(base_dir, \"CLEANED\")\n",
        "        self.output_dir = \"/data/processed\"  # Output lokal\n",
        "        self.gdrive_output_dir = os.path.join(base_dir, \"data\", \"processed\")  # Output Google Drive\n",
        "        self.logs_dir = \"/logs\"\n",
        "\n",
        "        # Buat direktori\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.gdrive_output_dir, exist_ok=True)  # Buat di Google Drive\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üìù EKSTRAKSI KONTEN KUNCI\")\n",
        "        print(f\"Input: {self.cleaned_dir} atau {self.gdrive_cleaned_dir}\")\n",
        "        print(f\"Output Lokal: {self.output_dir}\")\n",
        "        print(f\"Output GDrive: {self.gdrive_output_dir}\")\n",
        "\n",
        "        # Setup pola ekstraksi konten\n",
        "        self.setup_content_patterns()\n",
        "\n",
        "    def setup_content_patterns(self):\n",
        "        \"\"\"Setup pola regex untuk ekstraksi konten kunci\"\"\"\n",
        "\n",
        "        # 1. POLA RINGKASAN FAKTA\n",
        "        self.fact_summary_patterns = {\n",
        "            'dakwaan': [\n",
        "                r'dakwaan\\s*:?\\s*(.*?)(?=\\n\\n|\\nterdakwa|\\npenuntut|\\nhakim|\\Z)',\n",
        "                r'terdakwa\\s+didakwa\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'melakukan\\s+perbuatan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'barang_bukti': [\n",
        "                r'barang\\s+bukti\\s*:?\\s*(.*?)(?=\\n\\n|\\nsaksi|\\nhakim|\\Z)',\n",
        "                r'bukti.*?yang\\s+diajukan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'alat\\s+bukti\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'fakta_persidangan': [\n",
        "                r'fakta.*?persidangan\\s*:?\\s*(.*?)(?=\\n\\n|\\npertimbangan|\\Z)',\n",
        "                r'berdasarkan\\s+fakta.*?persidangan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'dari\\s+fakta.*?terungkap\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'kronologi': [\n",
        "                r'kronologi\\s*:?\\s*(.*?)(?=\\n\\n|\\npertimbangan|\\Z)',\n",
        "                r'peristiwa\\s+terjadi\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'pada\\s+tanggal.*?terdakwa\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'kerugian': [\n",
        "                r'kerugian.*?negara\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'merugikan.*?keuangan.*?negara\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'nilai\\s+kerugian\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 2. POLA ARGUMEN HUKUM UTAMA\n",
        "        self.legal_argument_patterns = {\n",
        "            'pertimbangan_hakim': [\n",
        "                r'pertimbangan\\s*:?\\s*(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'menimbang\\s*:?\\s*(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'bahwa.*?hakim\\s+berpendapat\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'pasal_yang_terbukti': [\n",
        "                r'terbukti.*?melanggar\\s+pasal\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'perbuatan.*?memenuhi.*?pasal\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'dakwaan.*?terbukti.*?pasal\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'alasan_putusan': [\n",
        "                r'oleh\\s+karena\\s+itu\\s+(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'dengan\\s+demikian\\s+(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "                r'berdasarkan.*?pertimbangan\\s+(.*?)(?=\\nmengadili|\\namar|\\Z)',\n",
        "            ],\n",
        "            'amar_putusan': [\n",
        "                r'mengadili\\s*:?\\s*(.*?)(?=\\Z)',\n",
        "                r'amar\\s*:?\\s*(.*?)(?=\\Z)',\n",
        "                r'memutuskan\\s*:?\\s*(.*?)(?=\\Z)',\n",
        "            ],\n",
        "            'putusan_hukuman': [\n",
        "                r'menjatuhkan\\s+pidana\\s+(.*?)(?=\\n[0-9]|\\Z)',\n",
        "                r'menghukum\\s+terdakwa\\s+(.*?)(?=\\n[0-9]|\\Z)',\n",
        "                r'pidana\\s+penjara\\s+selama\\s+(.*?)(?=\\n|\\Z)',\n",
        "                r'pidana\\s+denda\\s+sebesar\\s+(.*?)(?=\\n|\\Z)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 3. POLA ELEMEN PENTING LAINNYA\n",
        "        self.other_patterns = {\n",
        "            'saksi': [\n",
        "                r'saksi\\s*:?\\s*(.*?)(?=\\nterdakwa|\\npenuntut|\\Z)',\n",
        "                r'keterangan\\s+saksi\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'ahli': [\n",
        "                r'keterangan\\s+ahli\\s*:?\\s*(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'ahli\\s+menerangkan\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ],\n",
        "            'pengakuan_terdakwa': [\n",
        "                r'terdakwa\\s+mengaku\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "                r'keterangan\\s+terdakwa\\s+(.*?)(?=\\n\\n|\\Z)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def clean_extracted_text(self, text: str) -> str:\n",
        "        \"\"\"Bersihkan teks yang diekstrak\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove common legal formatting\n",
        "        text = re.sub(r'\\b(?:pasal|ayat|huruf|angka)\\s+\\([^)]+\\)', '', text)\n",
        "        # Remove line numbers\n",
        "        text = re.sub(r'^\\d+\\.?\\s*', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_fact_summary(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Ekstrak ringkasan fakta dari teks\"\"\"\n",
        "        facts = {}\n",
        "\n",
        "        for category, patterns in self.fact_summary_patterns.items():\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    if match.group(1):\n",
        "                        content = self.clean_extracted_text(match.group(1))\n",
        "                        if len(content) > len(extracted_text):\n",
        "                            extracted_text = content\n",
        "\n",
        "            facts[category] = extracted_text[:1000] if extracted_text else \"\"  # Limit to 1000 chars\n",
        "\n",
        "        return facts\n",
        "\n",
        "    def extract_legal_arguments(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Ekstrak argumen hukum utama\"\"\"\n",
        "        arguments = {}\n",
        "\n",
        "        for category, patterns in self.legal_argument_patterns.items():\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    if match.group(1):\n",
        "                        content = self.clean_extracted_text(match.group(1))\n",
        "                        if len(content) > len(extracted_text):\n",
        "                            extracted_text = content\n",
        "\n",
        "            arguments[category] = extracted_text[:1000] if extracted_text else \"\"\n",
        "\n",
        "        return arguments\n",
        "\n",
        "    def extract_other_elements(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Ekstrak elemen penting lainnya\"\"\"\n",
        "        others = {}\n",
        "\n",
        "        for category, patterns in self.other_patterns.items():\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    if match.group(1):\n",
        "                        content = self.clean_extracted_text(match.group(1))\n",
        "                        if len(content) > len(extracted_text):\n",
        "                            extracted_text = content\n",
        "\n",
        "            others[category] = extracted_text[:500] if extracted_text else \"\"\n",
        "\n",
        "        return others\n",
        "\n",
        "    def extract_key_content_from_text(self, text: str, filename: str) -> Dict:\n",
        "        \"\"\"Ekstrak semua konten kunci dari teks\"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return {}\n",
        "\n",
        "        content = {\n",
        "            'filename': filename,\n",
        "            'extraction_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'text_length': len(text),\n",
        "            'word_count': len(text.split())\n",
        "        }\n",
        "\n",
        "        # 1. Ekstrak ringkasan fakta\n",
        "        fact_summary = self.extract_fact_summary(text)\n",
        "        content['fact_summary'] = fact_summary\n",
        "\n",
        "        # 2. Ekstrak argumen hukum\n",
        "        legal_arguments = self.extract_legal_arguments(text)\n",
        "        content['legal_arguments'] = legal_arguments\n",
        "\n",
        "        # 3. Ekstrak elemen lainnya\n",
        "        other_elements = self.extract_other_elements(text)\n",
        "        content['other_elements'] = other_elements\n",
        "\n",
        "        # 4. Hitung kelengkapan konten\n",
        "        content['content_completeness'] = self.calculate_content_completeness(content)\n",
        "\n",
        "        return content\n",
        "\n",
        "    def calculate_content_completeness(self, content: Dict) -> float:\n",
        "        \"\"\"Hitung persentase kelengkapan konten kunci\"\"\"\n",
        "        essential_elements = [\n",
        "            'dakwaan', 'barang_bukti', 'pertimbangan_hakim',\n",
        "            'amar_putusan', 'putusan_hukuman'\n",
        "        ]\n",
        "\n",
        "        score = 0\n",
        "        fact_summary = content.get('fact_summary', {})\n",
        "        legal_arguments = content.get('legal_arguments', {})\n",
        "\n",
        "        for element in essential_elements:\n",
        "            if element in fact_summary and fact_summary[element]:\n",
        "                score += 1\n",
        "            elif element in legal_arguments and legal_arguments[element]:\n",
        "                score += 1\n",
        "\n",
        "        return (score / len(essential_elements)) * 100\n",
        "\n",
        "    def process_single_file(self, filename: str, source_dir: str) -> Optional[Dict]:\n",
        "        \"\"\"Proses file tunggal untuk ekstraksi konten kunci\"\"\"\n",
        "        file_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            logger.error(f\"File tidak ditemukan: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            if not text.strip():\n",
        "                logger.warning(f\"File kosong: {filename}\")\n",
        "                return None\n",
        "\n",
        "            content = self.extract_key_content_from_text(text, filename)\n",
        "\n",
        "            if content:\n",
        "                completeness = content.get('content_completeness', 0)\n",
        "                print(f\"‚úÖ {filename} (Kelengkapan: {completeness:.1f}%)\")\n",
        "                return content\n",
        "            else:\n",
        "                print(f\"‚ùå {filename}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error memproses {filename}: {str(e)}\")\n",
        "            print(f\"‚ùå {filename}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_text_files(self, directory: str) -> List[str]:\n",
        "        \"\"\"Dapatkan daftar file teks\"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            return []\n",
        "        return [f for f in os.listdir(directory)\n",
        "                if f.endswith('.txt') and os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    def save_content_to_csv(self, content_list: List[Dict]):\n",
        "        \"\"\"Simpan konten kunci ke CSV di kedua lokasi\"\"\"\n",
        "        flattened_data = []\n",
        "\n",
        "        for content in content_list:\n",
        "            fact_summary = content.get('fact_summary', {})\n",
        "            legal_arguments = content.get('legal_arguments', {})\n",
        "            other_elements = content.get('other_elements', {})\n",
        "\n",
        "            flat_record = {\n",
        "                # IDENTITAS DOKUMEN\n",
        "                'nama_file': content.get('filename'),\n",
        "                'tanggal_ekstraksi': content.get('extraction_timestamp'),\n",
        "                'panjang_teks': content.get('text_length'),\n",
        "                'jumlah_kata': content.get('word_count'),\n",
        "                'kelengkapan_konten_persen': content.get('content_completeness'),\n",
        "\n",
        "                # RINGKASAN FAKTA\n",
        "                'dakwaan': fact_summary.get('dakwaan', ''),\n",
        "                'barang_bukti': fact_summary.get('barang_bukti', ''),\n",
        "                'fakta_persidangan': fact_summary.get('fakta_persidangan', ''),\n",
        "                'kronologi': fact_summary.get('kronologi', ''),\n",
        "                'kerugian': fact_summary.get('kerugian', ''),\n",
        "\n",
        "                # ARGUMEN HUKUM UTAMA\n",
        "                'pertimbangan_hakim': legal_arguments.get('pertimbangan_hakim', ''),\n",
        "                'pasal_yang_terbukti': legal_arguments.get('pasal_yang_terbukti', ''),\n",
        "                'alasan_putusan': legal_arguments.get('alasan_putusan', ''),\n",
        "                'amar_putusan': legal_arguments.get('amar_putusan', ''),\n",
        "                'putusan_hukuman': legal_arguments.get('putusan_hukuman', ''),\n",
        "\n",
        "                # ELEMEN LAINNYA\n",
        "                'keterangan_saksi': other_elements.get('saksi', ''),\n",
        "                'keterangan_ahli': other_elements.get('ahli', ''),\n",
        "                'pengakuan_terdakwa': other_elements.get('pengakuan_terdakwa', '')\n",
        "            }\n",
        "            flattened_data.append(flat_record)\n",
        "\n",
        "        # Buat DataFrame\n",
        "        df = pd.DataFrame(flattened_data)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        csv_filename = f\"konten_kunci_{timestamp}.csv\"\n",
        "\n",
        "        # Simpan ke direktori lokal\n",
        "        csv_path_local = os.path.join(self.output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_local, index=False, encoding='utf-8')\n",
        "        print(f\"üìÑ CSV Lokal: {csv_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive\n",
        "        csv_path_gdrive = os.path.join(self.gdrive_output_dir, csv_filename)\n",
        "        df.to_csv(csv_path_gdrive, index=False, encoding='utf-8')\n",
        "        print(f\"üíæ CSV GDrive: {csv_path_gdrive}\")\n",
        "\n",
        "        return csv_path_local, csv_path_gdrive\n",
        "\n",
        "    def save_content_to_json(self, content_list: List[Dict]):\n",
        "        \"\"\"Simpan konten kunci ke JSON lengkap di kedua lokasi\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        json_filename = f\"konten_kunci_{timestamp}.json\"\n",
        "\n",
        "        # Simpan ke direktori lokal\n",
        "        json_path_local = os.path.join(self.output_dir, json_filename)\n",
        "        with open(json_path_local, 'w', encoding='utf-8') as f:\n",
        "            json.dump(content_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üìÑ JSON Lokal: {json_path_local}\")\n",
        "\n",
        "        # Simpan ke Google Drive\n",
        "        json_path_gdrive = os.path.join(self.gdrive_output_dir, json_filename)\n",
        "        with open(json_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "            json.dump(content_list, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üíæ JSON GDrive: {json_path_gdrive}\")\n",
        "\n",
        "        return json_path_local, json_path_gdrive\n",
        "\n",
        "    def create_summary_report(self, content_list: List[Dict]):\n",
        "        \"\"\"Buat laporan ringkasan ekstraksi konten\"\"\"\n",
        "        if not content_list:\n",
        "            return\n",
        "\n",
        "        # Statistik dasar\n",
        "        total_files = len(content_list)\n",
        "        avg_completeness = sum(c.get('content_completeness', 0) for c in content_list) / total_files\n",
        "        avg_length = sum(c.get('text_length', 0) for c in content_list) / total_files\n",
        "\n",
        "        # Hitung coverage untuk setiap elemen\n",
        "        coverage = {}\n",
        "        elements = ['dakwaan', 'barang_bukti', 'pertimbangan_hakim', 'amar_putusan', 'putusan_hukuman']\n",
        "\n",
        "        for element in elements:\n",
        "            count = 0\n",
        "            for content in content_list:\n",
        "                fact_summary = content.get('fact_summary', {})\n",
        "                legal_arguments = content.get('legal_arguments', {})\n",
        "\n",
        "                if (fact_summary.get(element) or legal_arguments.get(element)):\n",
        "                    count += 1\n",
        "\n",
        "            coverage[element] = (count / total_files) * 100\n",
        "\n",
        "        # Buat laporan\n",
        "        report = f\"\"\"\n",
        "üìä LAPORAN EKSTRAKSI KONTEN KUNCI\n",
        "====================================\n",
        "Tanggal: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "STATISTIK UMUM:\n",
        "- Total file diproses: {total_files}\n",
        "- Rata-rata kelengkapan: {avg_completeness:.1f}%\n",
        "- Rata-rata panjang teks: {avg_length:,.0f} karakter\n",
        "\n",
        "COVERAGE ELEMEN KUNCI:\n",
        "- Dakwaan: {coverage.get('dakwaan', 0):.1f}%\n",
        "- Barang Bukti: {coverage.get('barang_bukti', 0):.1f}%\n",
        "- Pertimbangan Hakim: {coverage.get('pertimbangan_hakim', 0):.1f}%\n",
        "- Amar Putusan: {coverage.get('amar_putusan', 0):.1f}%\n",
        "- Putusan Hukuman: {coverage.get('putusan_hukuman', 0):.1f}%\n",
        "\n",
        "FILE DENGAN KELENGKAPAN TINGGI (>80%):\n",
        "\"\"\"\n",
        "\n",
        "        high_quality = [c for c in content_list if c.get('content_completeness', 0) > 80]\n",
        "        for content in high_quality[:10]:\n",
        "            filename = content.get('filename', 'Unknown')\n",
        "            completeness = content.get('content_completeness', 0)\n",
        "            report += f\"- {filename}: {completeness:.1f}%\\n\"\n",
        "\n",
        "        if len(high_quality) > 10:\n",
        "            report += f\"... dan {len(high_quality) - 10} file lainnya\\n\"\n",
        "\n",
        "        # Simpan laporan\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        report_filename = f\"laporan_konten_{timestamp}.txt\"\n",
        "        report_path = os.path.join(self.output_dir, report_filename)\n",
        "\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(f\"üìã Laporan disimpan: {report_path}\")\n",
        "\n",
        "    def process_all_files(self) -> List[Dict]:\n",
        "        \"\"\"Proses semua file untuk ekstraksi konten kunci\"\"\"\n",
        "        print(\"üìù ii. EKSTRAKSI KONTEN KUNCI\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"1. Ringkasan fakta (barang bukti, dakwaan)\")\n",
        "        print(\"2. Argumen hukum utama (putusan, pasal)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Cari file dari kedua lokasi\n",
        "        data_raw_files = self.get_text_files(self.cleaned_dir)\n",
        "        gdrive_files = self.get_text_files(self.gdrive_cleaned_dir)\n",
        "\n",
        "        if data_raw_files:\n",
        "            files_to_process = data_raw_files\n",
        "            source_directory = self.cleaned_dir\n",
        "            print(f\"üìÇ Menggunakan file dari: {self.cleaned_dir}\")\n",
        "        elif gdrive_files:\n",
        "            files_to_process = gdrive_files\n",
        "            source_directory = self.gdrive_cleaned_dir\n",
        "            print(f\"üìÇ Menggunakan file dari: {self.gdrive_cleaned_dir}\")\n",
        "        else:\n",
        "            print(\"‚ùå Tidak ada file teks yang ditemukan!\")\n",
        "            return []\n",
        "\n",
        "        print(f\"üìÅ Ditemukan {len(files_to_process)} file untuk diproses\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Proses setiap file\n",
        "        all_content = []\n",
        "        success_count = 0\n",
        "\n",
        "        for i, filename in enumerate(files_to_process, 1):\n",
        "            print(f\"[{i:3d}/{len(files_to_process)}] \", end=\"\")\n",
        "            content = self.process_single_file(filename, source_directory)\n",
        "\n",
        "            if content:\n",
        "                all_content.append(content)\n",
        "                success_count += 1\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"‚úÖ BERHASIL: {success_count} file\")\n",
        "        print(f\"‚ùå GAGAL: {len(files_to_process) - success_count} file\")\n",
        "\n",
        "        if all_content:\n",
        "            # Simpan ke CSV dan JSON di kedua lokasi\n",
        "            csv_paths = self.save_content_to_csv(all_content)\n",
        "            json_paths = self.save_content_to_json(all_content)\n",
        "\n",
        "            # Buat laporan ringkasan\n",
        "            self.create_summary_report(all_content)\n",
        "\n",
        "            print(f\"üìä Total konten kunci berhasil diekstrak: {len(all_content)} record\")\n",
        "            print(f\"üíæ File tersimpan di 2 lokasi: lokal & Google Drive\")\n",
        "\n",
        "        return all_content\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk menjalankan ekstraksi konten kunci\"\"\"\n",
        "    print(\"üöÄ MULAI EKSTRAKSI KONTEN KUNCI\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        extractor = KontenKunciExtractor()\n",
        "        content_results = extractor.process_all_files()\n",
        "\n",
        "        if content_results:\n",
        "            print(\"\\nüéâ EKSTRAKSI KONTEN KUNCI SELESAI!\")\n",
        "            print(f\"Total konten: {len(content_results)} record\")\n",
        "            print(\"File output tersimpan di: /data/processed/\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Tidak ada konten kunci yang berhasil diekstrak.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbJTW62TAMxo",
        "outputId": "a7276544-8be5-4003-db0f-2908eae7cf71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ MULAI EKSTRAKSI KONTEN KUNCI\n",
            "==================================================\n",
            "üìù EKSTRAKSI KONTEN KUNCI\n",
            "Input: /data/raw atau /content/drive/MyDrive/perdagangan_orang/CLEANED\n",
            "Output Lokal: /data/processed\n",
            "Output GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed\n",
            "üìù ii. EKSTRAKSI KONTEN KUNCI\n",
            "============================================================\n",
            "1. Ringkasan fakta (barang bukti, dakwaan)\n",
            "2. Argumen hukum utama (putusan, pasal)\n",
            "============================================================\n",
            "üìÇ Menggunakan file dari: /content/drive/MyDrive/perdagangan_orang/CLEANED\n",
            "üìÅ Ditemukan 79 file untuk diproses\n",
            "------------------------------------------------------------\n",
            "[  1/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_145_PID_SUS_2021_PT_MTR_Tanggal_20_Desember_2021__Pembanding_Penuntut_Umum___MANIK_ARTHA_ADHITAMA__SHTerbanding_Terdakwa___Herman_Saputra_Rafiudin_Alias_Herman.txt (Kelengkapan: 100.0%)\n",
            "[  2/79] ‚úÖ case_2021_TK1_Putusan_PN_PELAIHARI_Nomor_179_Pid_Sus_2021_PN_Pli_Tanggal_16_Desember_2021__Penuntut_Umum_ANDI_HAMZAH_KUSUMAATMAJA__S_HTerdakwa_M__NOOR_Als_NUNUI_Bin_KHAIRI.txt (Kelengkapan: 100.0%)\n",
            "[  3/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_140_PID_SUS_2021_PT_MTR_Tanggal_9_Desember_2021__Pembanding_Penuntut_Umum_I___HENDRO_S_I_B__SH_Terbanding_Terdakwa___BQ_DIAN_CINDRAWATI_Alias_DIAN.txt (Kelengkapan: 100.0%)\n",
            "[  4/79] ‚úÖ case_2021_TK1_Putusan_PN_BALIKPAPAN_Nomor_412_Pid_Sus_2021_PN_Bpp_Tanggal_30_Nopember_2021__Penuntut_Umum_Ita_Wahyuning_Lestari__SH_Terdakwa_JEKSON_RAJAGUKGUK_Alias_JECO_Anak_dari_ALBERT_RAJAGUKGUK.txt (Kelengkapan: 100.0%)\n",
            "[  5/79] ‚úÖ case_2021_TK1_Putusan_PT_SAMARINDA_Nomor_240_PID_2021_PT_SMR_Tanggal_26_Nopember_2021__Pembanding_Terbanding_Terdakwa___DEVITA_ARIYANI_Als_DORA_Binti_MUSTOPA_Diwakili_Oleh___Nunung_Tri_Sulistiawa__S_H_.txt (Kelengkapan: 100.0%)\n",
            "[  6/79] ‚úÖ case_2021_TK1_Putusan_PT_PEKANBARU_Nomor_494_PID_SUS_2021_PT_PBR_Tanggal_17_Nopember_2021__Pembanding_Terbanding_Terdakwa___EKO_SUMBARA_Alias_EKO_Bin_MUHMMAD_NASIR_Alm_Diwakili_Oleh___ANDI_NUGRAHG__SH_.txt (Kelengkapan: 100.0%)\n",
            "[  7/79] ‚úÖ case_2021_TK1_Putusan_PN_MAKALE_Nomor_92_Pid_Sus_2021_PN_Mak_Tanggal_15_Nopember_2021__Penuntut_Umum_MARGARETHA_H__PATURU__S_H_Terdakwa_SRI_SUNARTI_alias_MAMI.txt (Kelengkapan: 100.0%)\n",
            "[  8/79] ‚úÖ case_2021_TK1_Putusan_PN_MAKALE_Nomor_93_Pid_Sus_2021_PN_Mak_Tanggal_15_Nopember_2021__Penuntut_Umum_MARGARETHA_H__PATURU__S_H_Terdakwa_WIWIN_ALIAS_VALEN.txt (Kelengkapan: 100.0%)\n",
            "[  9/79] ‚úÖ case_2021_TK1_Putusan_PN_SURABAYA_Nomor_1969_Pid_Sus_2021_PN_Sby_Tanggal_8_Nopember_2021__Penuntut_Umum_DEDDY_ARISANDI__SH__MHTerdakwa_HENDRI_YULIANSYAH_BIN_ALM_BUTRI_SYAMSI.txt (Kelengkapan: 100.0%)\n",
            "[ 10/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_120_PID_SUS_2021_PT_MTR_Tanggal_8_Nopember_2021__Pembanding_Terbanding_Terdakwa___RATNI__SH_Alias_RANITerbanding_Pembanding_Penuntut_Umum___FEDDY_HANTYO_NUG__M_H_.txt (Kelengkapan: 100.0%)\n",
            "[ 11/79] ‚úÖ case_2021_TK1_Putusan_PN_TANGERANG_Nomor_1614_Pid_Sus_2021_PN_Tng_Tanggal_3_Nopember_2021__Penuntut_Umum_HADI_WIDODO__SHTerdakwa_1_SUBUR_RAHARJO_Bin_SUGITO2_AMAR_SAHIDIN_Als_ABANG_Bin_WARSITO.txt (Kelengkapan: 100.0%)\n",
            "[ 12/79] ‚úÖ case_2021_TK1_Putusan_PN_INDRAMAYU_Nomor_214_Pid_Sus_2021_PN_Idm_Tanggal_21_Oktober_2021__Penuntut_Umum_1_M__ICHSAN__S_H___M_H_2_TISNA_P__WIJAYA__SHTerdakwa_1_ISMAEL_IBRAHIM_KHALEEL__alias_ISMAILI_AJAT.txt (Kelengkapan: 100.0%)\n",
            "[ 13/79] ‚úÖ case_2021_TK1_Putusan_PN_INDRAMAYU_Nomor_213_Pid_Sus_2021_PN_Idm_Tanggal_21_Oktober_2021__Penuntut_Umum_JIHANTO_NUR_RACHMAN__SHTerdakwa_1_ULFIYATI_Alias_ULFI_Binti_SUTIMAN2_MAHFUDZ_SIDDIQ_Alias_M_DAKIM.txt (Kelengkapan: 100.0%)\n",
            "[ 14/79] ‚úÖ case_2021_TK1_Putusan_PN_INDRAMAYU_Nomor_215_Pid_Sus_2021_PN_Idm_Tanggal_21_Oktober_2021__Penuntut_Umum_1_M__ICHSAN__S_H___M_H_2_TISNA_P__WIJAYA__SHTerdakwa_1_ANDI_SOPANDI_alias_ANDI_BIN_UUM_2_DAUSTADI.txt (Kelengkapan: 100.0%)\n",
            "[ 15/79] ‚úÖ case_2025_TK1_Putusan_PA_LUBUK_PAKAM_Nomor_2063_Pdt_G_2025_PA_Lpk_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 16/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_2898_Pdt_G_2025_PA_Tgrs_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 17/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_3022_Pdt_G_2025_PA_Tgrs_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 18/79] ‚úÖ case_2021_TK1_Putusan_PT_BANTEN_Nomor_108_PID_SUS_2021_PT_BTN_Tanggal_18_Oktober_2021__Pembanding_Penuntut_Umum___AGUSTRI_HARTONO__SH__MHTerbanding_Terdakwa___TOFIK_TRIYATNO_Bin_TASMIARJO_ALM.txt (Kelengkapan: 100.0%)\n",
            "[ 19/79] ‚úÖ case_2021_TK1_Putusan_PN_PALOPO_Nomor_114_Pid_Sus_2021_PN_Plp_Tanggal_12_Oktober_2021__Penuntut_Umum_1_YANUAR_FIHAWIANO_SH2_AHMAD_SULHAN_S_H3_Erlysa_Said__S_H_Terdakwa_MELFI_INDIRIATI_PUTRI_Als__S_BATO.txt (Kelengkapan: 100.0%)\n",
            "[ 20/79] ‚úÖ case_2021_TK1_Putusan_PN_MATARAM_Nomor_467_Pid_Sus_2021_PN_Mtr_Tanggal_30_September_2021__Penuntut_Umum_1_HENDRO_SAYEKTI_SH_2_M_BUSTANUL__ARIFIN_SH_MH_3_MOCH__TAUFIQ_ISMAIL__SHTerdakwa_PANDRI__AZ_ANDRE.txt (Kelengkapan: 100.0%)\n",
            "[ 21/79] ‚úÖ case_2021_TK1_Putusan_PN_CILACAP_Nomor_197_Pid_Sus_2021_PN_Clp_Tanggal_30_September_2021__Penuntut_Umum_Santa_Novena_Christy_SHTerdakwa_GULIYAH_Binti_Alm_TEGUH_SUPARDI.txt (Kelengkapan: 100.0%)\n",
            "[ 22/79] ‚úÖ case_2021_TK1_Putusan_PN_PURWOKERTO_Nomor_124_Pid_Sus_2021_PN_Pwt_Tanggal_7_September_2021__Penuntut_Umum_MARYANI_WIDIYASTUTITerdakwa_OSI_NAVITALIA_ALS_OCI_BINTI_SUNARTO.txt (Kelengkapan: 100.0%)\n",
            "[ 23/79] ‚úÖ case_2021_TK1_Putusan_PN_PURWOKERTO_Nomor_126_Pid_Sus_2021_PN_Pwt_Tanggal_7_September_2021__Penuntut_Umum_MARYANI_WIDIYASTUTITerdakwa_KARMANESA_FEBRIARI_ALS_ESA_BIN_NIAT_NGUDIANTO.txt (Kelengkapan: 100.0%)\n",
            "[ 24/79] ‚úÖ case_2025_TK1_Putusan_PA_Ngamprah_Nomor_1378_Pdt_G_2025_PA_Nph_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 25/79] ‚úÖ case_2025_TK1_Putusan_PA_Ngamprah_Nomor_1205_Pdt_G_2025_PA_Nph_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 26/79] ‚úÖ case_2021_TK1_Putusan_PN_PURWOKERTO_Nomor_125_Pid_Sus_2021_PN_Pwt_Tanggal_7_September_2021__Penuntut_Umum_MARYANI_WIDIYASTUTITerdakwa_JEFRI_TOMS_PARDIANTO_ALS_JEFRI_ALS_GATEL_BIN_PARDIMAN.txt (Kelengkapan: 100.0%)\n",
            "[ 27/79] ‚úÖ case_2021_TK1_Putusan_PN_ROKAN_HILIR_Nomor_234_Pid_Sus_2021_PN_Rhl_Tanggal_6_September_2021__Penuntut_Umum_1_MARULITUA_J__SITANGGANG__SH_2_YONGKI_ARVIUS__S_H_MHTerdakwa_EKO_SUMBARA_Alias_EKO_Bin_IR_Alm.txt (Kelengkapan: 100.0%)\n",
            "[ 28/79] ‚úÖ case_2021_TK1_Putusan_PT_BANTEN_Nomor_92_PID_SUS_2021_PT_BTN_Tanggal_31_Agustus_2021__Pembanding_Penuntut_Umum___GORUT_PERTHIKA__SHTerbanding_Terdakwa_I___MAYANG_APRILLA_RAHMAYANTI_als_MAMI_APRILA_CHOI.txt (Kelengkapan: 100.0%)\n",
            "[ 29/79] ‚úÖ case_2021_TK1_Putusan_PN_Ngabang_Nomor_65_Pid_Sus_2021_PN_Nba_Tanggal_30_Agustus_2021__Penuntut_Umum_Pewira_Saputra_SHTerdakwa_Wan_Wan_Anak_dari_Alm_Liu_Po_Fha.txt (Kelengkapan: 100.0%)\n",
            "[ 30/79] ‚úÖ case_2021_TK1_Putusan_PN_Ngabang_Nomor_64_Pid_Sus_2021_PN_Nba_Tanggal_30_Agustus_2021__Penuntut_Umum_Pewira_Saputra_SHTerdakwa_Susanti_Alias_Aling_Anak_Dari_Siau_Ket_Loy.txt (Kelengkapan: 100.0%)\n",
            "[ 31/79] ‚úÖ case_2021_TK1_Putusan_PN_JAKARTA_UTARA_Nomor_596_Pid_Sus_2021_PN_Jkt_Utr_Tanggal_16_Agustus_2021__Penuntut_Umum_DYOFA_YUDHISTIRA__SHTerdakwa_ALI_NURUDIN_ALIAS_ALI_.txt (Kelengkapan: 100.0%)\n",
            "[ 32/79] ‚úÖ case_2021_TK1_Putusan_PN_JAKARTA_UTARA_Nomor_595_Pid_Sus_2021_PN_Jkt_Utr_Tanggal_16_Agustus_2021__Penuntut_Umum_DYOFA_YUDHISTIRA__SHTerdakwa_YUDHISTIRA_ARMIN_ALIAS_YUDI.txt (Kelengkapan: 100.0%)\n",
            "[ 33/79] ‚úÖ case_2021_TK1_Putusan_PN_MANDAILING_NATAL_Nomor_87_Pid_Sus_2021_PN_Mdl_Tanggal_9_Agustus_2021__Penuntut_Umum_1_NURHAYATI_PULUNGAN__SH2_HERIYANTO_MANURUNG__SHTerdakwa_FITRIANI_ALIAS_JABUKE.txt (Kelengkapan: 100.0%)\n",
            "[ 34/79] ‚úÖ case_2021_TK1_Putusan_PN_BENGKULU_Nomor_227_Pid_Sus_2021_PN_Bgl_Tanggal_22_Juli_2021__Penuntut_Umum_SRI_RAHMITerdakwa_RIWANSYAH__S_Pd_Als_RIWAN_Als_CIN_Als_MAMI_Bin_YASUR_I.txt (Kelengkapan: 100.0%)\n",
            "[ 35/79] ‚úÖ case_2025_TK1_Putusan_PA_Sei_Rampah_Nomor_648_Pdt_G_2025_PA_Srh_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 36/79] ‚úÖ case_2025_TK1_Putusan_PA_Sei_Rampah_Nomor_655_Pdt_G_2025_PA_Srh_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 37/79] ‚úÖ case_2025_TK1_Putusan_PA_Sei_Rampah_Nomor_660_Pdt_G_2025_PA_Srh_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 38/79] ‚úÖ case_2021_TK1_Putusan_PN_PASANGKAYU_Nomor_78_Pid_Sus_2021_PN_Pky_Tanggal_21_Juli_2021__Penuntut_Umum_HAFIZ_AKBAR_RITONGA__SHTerdakwa_NURSANTI_alias_BUNDA_binti_TASWIN_MOITA.txt (Kelengkapan: 100.0%)\n",
            "[ 39/79] ‚úÖ case_2021_TK1_Putusan_PN_PASANGKAYU_Nomor_79_Pid_Sus_2021_PN_Pky_Tanggal_21_Juli_2021__Penuntut_Umum_FRI_HARMOKO__SH__MHTerdakwa_RUHANI_alias_RIFKA_binti_ABD__LATIF.txt (Kelengkapan: 100.0%)\n",
            "[ 40/79] ‚úÖ case_2021_TK1_Putusan_PN_SURABAYA_Nomor_1135_Pid_Sus_2021_PN_Sby_Tanggal_1_Juli_2021__Penuntut_Umum_SULFIKAR__SHTerdakwa_NUR_RAHMAT_KISWO_PRANGGONO_BIN_SENO_BT_PRANGGONO.txt (Kelengkapan: 100.0%)\n",
            "[ 41/79] ‚úÖ case_2021_TK1_Putusan_PT_KUPANG_Nomor_82_PID_2021_PT_KPG_Tanggal_30_Juni_2021__Pembanding_Terbanding_Terdakwa_II___YOPPI_NALLETerbanding_Pembanding_Penuntut_Umum___CHRISTOFEL_H__MALLAKA__S_HTerbaSEMUEL.txt (Kelengkapan: 100.0%)\n",
            "[ 42/79] ‚úÖ case_2021_TK1_Putusan_PT_KUPANG_Nomor_77_PID_2021_PT_KPG_Tanggal_30_Juni_2021__Pembanding_Terdakwa_I___YOPPI_NALLETerbanding_Penuntut_Umum___CHRISTOFEL_H__MALLAKA__S_H.txt (Kelengkapan: 100.0%)\n",
            "[ 43/79] ‚úÖ case_2021_TK1_Putusan_PN_JAKARTA_UTARA_Nomor_341_Pid_Sus_2021_PN_Jkt_Utr_Tanggal_9_Juni_2021__Penuntut_Umum_ERNI_PRAMOTI__SHTerdakwa_ARDIAN_FIRMANSYAH_BIN_IWONG_TASWAN_.txt (Kelengkapan: 100.0%)\n",
            "[ 44/79] ‚úÖ case_2021_TK1_Putusan_PN_MAJALENGKA_Nomor_59_Pid_Sus_2021_PN_Mjl_Tanggal_2_Juni_2021__Penuntut_Umum_ADE_MULYANI__SHTerdakwa_AHMAD_MUAMAR_Alias_AMAR_bin_MUHDLOR.txt (Kelengkapan: 100.0%)\n",
            "[ 45/79] ‚úÖ case_2021_TK1_Putusan_PN_MAJALENGKA_Nomor_58_Pid_Sus_2021_PN_Mjl_Tanggal_2_Juni_2021__Penuntut_Umum_DANU_TRISNAWANTO__S_H_Terdakwa_AGUNG_SUBEKTI_Bin_DURIA.txt (Kelengkapan: 100.0%)\n",
            "[ 46/79] ‚úÖ case_2025_TK1_Putusan_PA_PONOROGO_Nomor_883_Pdt_G_2025_PA_Po_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 47/79] ‚úÖ case_2025_TK1_Putusan_PA_PONOROGO_Nomor_188_Pdt_P_2025_PA_Po_Tanggal_25_Juni_2025__Pemohon_melawan_Termohon.txt (Kelengkapan: 80.0%)\n",
            "[ 48/79] ‚úÖ case_2025_TK1_Putusan_PA_PONOROGO_Nomor_746_Pdt_G_2025_PA_Po_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 40.0%)\n",
            "[ 49/79] ‚úÖ case_2021_TK1_Putusan_PN_DENPASAR_Nomor_211_Pid_Sus_2021_PN_Dps_Tanggal_25_Mei_2021__Penuntut_Umum_Dewi_Agustin_Adiputri__SH_MHTerdakwa_Maulana_Aldi.txt (Kelengkapan: 100.0%)\n",
            "[ 50/79] ‚úÖ case_2021_TK1_Putusan_PN_SURABAYA_Nomor_575_Pid_Sus_2021_PN_Sby_Tanggal_24_Mei_2021__Penuntut_Umum_DZULKIFLY_NENTO__SHTerdakwa_RICO_LINGGAR_JAYA_BIN_EDI_WAHYONO.txt (Kelengkapan: 100.0%)\n",
            "[ 51/79] ‚úÖ case_2021_TK1_Putusan_PN_TENGGARONG_Nomor_141_Pid_Sus_2021_PN_Trg_Tanggal_4_Mei_2021__Penuntut_Umum_FITRI_IRA_P__SH_Terdakwa_SUNIYE_Als_SOIMAH_Binti_ARIF.txt (Kelengkapan: 100.0%)\n",
            "[ 52/79] ‚úÖ case_2021_TK1_Putusan_PN_BENGKULU_Nomor_87_Pid_Sus_2021_PN_Bgl_Tanggal_4_Mei_2021__Penuntut_Umum_J_HUTAGAOL_SH_MHTerdakwa_HIKMAT_DEKI_Als_DEKI_Als_MAMI_SHISI_Bin_WAHIDIN.txt (Kelengkapan: 100.0%)\n",
            "[ 53/79] ‚úÖ case_2021_TK1_Putusan_PN_TENGGARONG_Nomor_142_Pid_Sus_2021_PN_Trg_Tanggal_4_Mei_2021__Penuntut_Umum_FITRI_IRA_P__SH_Terdakwa_RANI_Als_RANI_RAHAYU_Alias_DIFA_Binti_IPAR.txt (Kelengkapan: 100.0%)\n",
            "[ 54/79] ‚úÖ case_2021_TK1_Putusan_PN_TEGAL_Nomor_22_Pid_Sus_2021_PN_Tgl_Tanggal_4_Mei_2021__Penuntut_Umum_1_Haerati__SH2_GRETA_ANASTASIA__S_H__M_H_3_Intan_Kafa_Arbina__SH_MHTerdakwa_MUAMAR_KADAFI.txt (Kelengkapan: 100.0%)\n",
            "[ 55/79] ‚úÖ case_2021_TK1_Putusan_PN_TANJUNG_SELOR_Nomor_19_Pid_Sus_2021_PN_Tjs_Tanggal_28_April_2021__Penuntut_Umum_DANU_BAGUS_PRATAMA__S_HTerdakwa_MUHAMAD_SAFRIANSYAH_Als_BULOT_Bin_MILI.txt (Kelengkapan: 100.0%)\n",
            "[ 56/79] ‚úÖ case_2021_TK1_Putusan_PT_PADANG_Nomor_70_PID_SUS_2021_PT_PDG_Tanggal_27_April_2021__Pembanding_Penuntut_Umum_I___MEILYA_TRISNA__SH__MHTerbanding_Terdakwa___DIAN_EKA_PUTRA_Pgl__DIAN.txt (Kelengkapan: 100.0%)\n",
            "[ 57/79] ‚úÖ case_2021_TK1_Putusan_PN_SUBANG_Nomor_66_Pid_Sus_2021_PN_SNG_Tanggal_27_April_2021__Penuntut_Umum_ADITYO_ISMUTOMO__SH_Terdakwa_WANAP_als_MANAP_bin_TAKIM.txt (Kelengkapan: 100.0%)\n",
            "[ 58/79] ‚úÖ case_2021_TK1_Putusan_PN_AMURANG_Nomor_5_Pid_Sus_2021_PN_Amr_Tanggal_22_April_2021__Penuntut_Umum_M__REZA_PAHLEPI__SHTerdakwa_VICKY_FERNANDO_BAHIHI_alias_VIKI.txt (Kelengkapan: 100.0%)\n",
            "[ 59/79] ‚úÖ case_2021_TK1_Putusan_PN_SANGGAU_Nomor_75_Pid_Sus_2021_PN_Sag_Tanggal_22_April_2021__Penuntut_Umum_MIFA_AL_FAHMI__S_H_Terdakwa_BAKRI_Bin_CICUK_Alm.txt (Kelengkapan: 100.0%)\n",
            "[ 60/79] ‚úÖ case_2021_TK1_Putusan_PN_AMURANG_Nomor_6_Pid_Sus_2021_PN_Amr_Tanggal_22_April_2021__Penuntut_Umum_M__REZA_PAHLEPI__SHTerdakwa_RICKY_JUNIOR_TUMBELAKA_alias_RIKI.txt (Kelengkapan: 100.0%)\n",
            "[ 61/79] ‚úÖ case_2021_TK1_Putusan_PN_AMURANG_Nomor_7_Pid_Sus_2021_PN_Amr_Tanggal_22_April_2021__Penuntut_Umum_M__REZA_PAHLEPI__SHTerdakwa_RIJAL_SUMAMPOW_alias_JAL.txt (Kelengkapan: 100.0%)\n",
            "[ 62/79] ‚úÖ case_2025_TK1_Putusan_PA_LAMONGAN_Nomor_1419_Pdt_G_2025_PA_Lmg_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 40.0%)\n",
            "[ 63/79] ‚úÖ case_2025_TK1_Putusan_PA_LAMONGAN_Nomor_1375_Pdt_G_2025_PA_Lmg_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 64/79] ‚úÖ case_2025_TK1_Putusan_PA_LAMONGAN_Nomor_1436_Pdt_G_2025_PA_Lmg_Tanggal_25_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 65/79] ‚úÖ case_2021_TK1_Putusan_PN_ENDE_Nomor_11_Pid_Sus_2021_PN_End_Tanggal_19_April_2021__Penuntut_Umum_1_OKKY_PRASETYO_AJIE2_TERESIA_WEKO__SHTerdakwa_STEFANUS_KUASA_Alias_EFAN.txt (Kelengkapan: 100.0%)\n",
            "[ 66/79] ‚úÖ case_2021_TK1_Putusan_PN_ENDE_Nomor_10_Pid_Sus_2021_PN_End_Tanggal_19_April_2021__Penuntut_Umum_1_OKKY_PRASETYO_AJIE2_TERESIA_WEKO__SHTerdakwa_MARIA_YUNIANTI_JAGONG_Alias_YUNI.txt (Kelengkapan: 100.0%)\n",
            "[ 67/79] ‚úÖ case_2021_TK1_Putusan_PN_BANYUWANGI_Nomor_76_Pid_Sus_2021_PN_Byw_Tanggal_12_April_2021__Penuntut_Umum_1_I_KETUT_GDE_DAME_NEGARA__SH2_GANDHI_MUCHLISIN__S_H_Terdakwa_SUWITO__Als__PAK_TO_Bin_MUSIMAN.txt (Kelengkapan: 100.0%)\n",
            "[ 68/79] ‚úÖ case_2021_TK1_Putusan_PN_PANGKALAN_BUN_Nomor_57_Pid_Sus_2021_PN_Pbu_Tanggal_7_April_2021__Penuntut_Umum_1_GOMGOMAN_H_SIMBOLON__S_H___M_H_2_GANES_ADI_KUSUMA__S_H_Terdakwa_RINDA_EVANNA_HOTMAULI_SIAIAHAAN.txt (Kelengkapan: 100.0%)\n",
            "[ 69/79] ‚úÖ case_2021_TK1_Putusan_PN_SAMBAS_Nomor_22_Pid_Sus_2021_PN_Sbs_Tanggal_22_Maret_2021__Penuntut_Umum_1_Muhammad_Nur_Faisal_Wijaya__S_H_2_I_in_Lindayani__S_H___M_H_Terdakwa_RIKKY_OKTADO_Als_RIKI_Bin_N__Alm.txt (Kelengkapan: 100.0%)\n",
            "[ 70/79] ‚úÖ case_2021_TK1_Putusan_PN_SUBANG_Nomor_40_Pid_Sus_2021_PN_SNG_Tanggal_17_Maret_2021__Penuntut_Umum_AZAM_AKHMAD_AKHSYA__S_H_Terdakwa_RIZAL_FIKRI_NURROHIMUDIN.txt (Kelengkapan: 100.0%)\n",
            "[ 71/79] ‚úÖ case_2021_TK1_Putusan_PN_KUALA_SIMPANG_Nomor_22_Pid_Sus_2021_PN_Ksp_Tanggal_8_Maret_2021__Penuntut_Umum_MARIONO__SH_MHTerdakwa_HENGKIE_BIN_EFENDI.txt (Kelengkapan: 100.0%)\n",
            "[ 72/79] ‚úÖ case_2021_TK1_Putusan_PN_KUALA_SIMPANG_Nomor_23_Pid_Sus_2021_PN_Ksp_Tanggal_8_Maret_2021__Penuntut_Umum_MARIONO__SH_MHTerdakwa_2_SUPRAYETNI_Binti_Alm_KASIMIN3_RAZALI_Bin_alm_SULAIMAN4_ROSMINI_BinTUBARA.txt (Kelengkapan: 100.0%)\n",
            "[ 73/79] ‚úÖ case_2021_TK1_Putusan_PT_MANADO_Nomor_10_PID_SUS_2021_PT_MND_Tanggal_25_Februari_2021__Pembanding_Penuntut_Umum___JENNY_R_WAYONG__SHTerbanding_Terdakwa___MICHAEL_UMBOH.txt (Kelengkapan: 100.0%)\n",
            "[ 74/79] ‚úÖ case_2021_TK1_Putusan_PT_YOGYAKARTA_Nomor_12_PID_SUS_2021_PT_YYK_Tanggal_18_Februari_2021__Pembanding_Penuntut_Umum___AGUS_KURNIAWAN_SHTerbanding_Terdakwa___SITI_FATIMAH_Als__NADIRA_Als__MBAK_MBUL.txt (Kelengkapan: 100.0%)\n",
            "[ 75/79] ‚úÖ case_2021_TK1_Putusan_PT_MATARAM_Nomor_12_PID_SUS_2021_PT_MTR_Tanggal_16_Februari_2021__Pembanding_Penuntut_Umum_I___SAHDI_SH_Terbanding_Terdakwa___H_HUSNUL_ANSORI_ALIAS_H_ANSORI.txt (Kelengkapan: 100.0%)\n",
            "[ 76/79] ‚úÖ case_2021_TK1_Putusan_PT_MANADO_Nomor_4_PID_2021_PT_MND_Tanggal_10_Februari_2021__Identitas_Pihak_Tidak_Dipublikasi.txt (Kelengkapan: 100.0%)\n",
            "[ 77/79] ‚úÖ case_2021_TK1_Putusan_PN_SAMBAS_Nomor_7_Pid_Sus_2021_PN_Sbs_Tanggal_9_Februari_2021__Penuntut_Umum_1_Meirita_Pakpahan__S_H_2_Salomo_Saing__S_H___M_H_Terdakwa_ELISA_BINTI_NAJIR.txt (Kelengkapan: 100.0%)\n",
            "[ 78/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_2726_Pdt_G_2025_PA_Tgrs_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "[ 79/79] ‚úÖ case_2025_TK1_Putusan_PA_TIGARAKSA_Nomor_3026_Pdt_G_2025_PA_Tgrs_Tanggal_24_Juni_2025__Penggugat_melawan_Tergugat.txt (Kelengkapan: 60.0%)\n",
            "------------------------------------------------------------\n",
            "‚úÖ BERHASIL: 79 file\n",
            "‚ùå GAGAL: 0 file\n",
            "üìÑ CSV Lokal: /data/processed/konten_kunci_20250625_121448.csv\n",
            "üíæ CSV GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/konten_kunci_20250625_121448.csv\n",
            "üìÑ JSON Lokal: /data/processed/konten_kunci_20250625_121448.json\n",
            "üíæ JSON GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/konten_kunci_20250625_121448.json\n",
            "üìã Laporan disimpan: /data/processed/laporan_konten_20250625_121448.txt\n",
            "üìä Total konten kunci berhasil diekstrak: 79 record\n",
            "üíæ File tersimpan di 2 lokasi: lokal & Google Drive\n",
            "\n",
            "üéâ EKSTRAKSI KONTEN KUNCI SELESAI!\n",
            "Total konten: 79 record\n",
            "File output tersimpan di: /data/processed/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n"
      ],
      "metadata": {
        "id": "HFXp8Or-A1AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from collections import Counter\n",
        "import logging\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Ic-_vIQ9A2eI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Generate features untuk machine learning dari dokumen putusan\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/drive/MyDrive/perdagangan_orang\"):\n",
        "        self.base_dir = base_dir\n",
        "        self.cleaned_dir = \"/data/raw\"\n",
        "        self.gdrive_cleaned_dir = os.path.join(base_dir, \"CLEANED\")\n",
        "        self.input_dir = \"/data/processed\"  # Input dari langkah sebelumnya\n",
        "        self.output_dir = \"/data/processed\"\n",
        "        self.gdrive_output_dir = os.path.join(base_dir, \"data\", \"processed\")  # Output Google Drive\n",
        "        self.logs_dir = \"/logs\"\n",
        "\n",
        "        # Buat direktori\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.gdrive_output_dir, exist_ok=True)\n",
        "        os.makedirs(self.logs_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"üîß FEATURE ENGINEERING\")\n",
        "        print(f\"Input teks: {self.cleaned_dir} atau {self.gdrive_cleaned_dir}\")\n",
        "        print(f\"Input metadata: {self.input_dir}\")\n",
        "        print(f\"Output Lokal: {self.output_dir}\")\n",
        "        print(f\"Output GDrive: {self.gdrive_output_dir}\")\n",
        "\n",
        "        # Initialize vectorizers\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=1000,\n",
        "            stop_words=self.get_indonesian_stopwords(),\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        )\n",
        "\n",
        "        self.count_vectorizer = CountVectorizer(\n",
        "            max_features=500,\n",
        "            stop_words=self.get_indonesian_stopwords(),\n",
        "            ngram_range=(1, 1),\n",
        "            min_df=2\n",
        "        )\n",
        "\n",
        "        # Label encoders dan scalers\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def get_indonesian_stopwords(self) -> List[str]:\n",
        "        \"\"\"Daftar stopwords bahasa Indonesia untuk legal documents\"\"\"\n",
        "        return [\n",
        "            'yang', 'dan', 'di', 'ke', 'dari', 'pada', 'dengan', 'untuk', 'dalam', 'oleh',\n",
        "            'adalah', 'akan', 'telah', 'sudah', 'dapat', 'harus', 'tidak', 'belum', 'juga',\n",
        "            'bahwa', 'sebagai', 'atau', 'jika', 'karena', 'sehingga', 'maka', 'agar', 'itu',\n",
        "            'ini', 'tersebut', 'hal', 'ada', 'sebuah', 'suatu', 'semua', 'setiap', 'beberapa',\n",
        "            'pengadilan', 'hakim', 'terdakwa', 'penggugat', 'tergugat', 'putusan', 'perkara',\n",
        "            'pasal', 'undang', 'hukum', 'pidana', 'perdata', 'nomor', 'tanggal', 'tahun',\n",
        "            'republik', 'indonesia', 'negeri', 'jaksa', 'penuntut', 'umum', 'saksi', 'bukti'\n",
        "        ]\n",
        "\n",
        "    def load_text_files(self) -> Dict[str, str]:\n",
        "        \"\"\"Load semua file teks yang sudah dibersihkan\"\"\"\n",
        "        texts = {}\n",
        "\n",
        "        # Cek direktori mana yang tersedia\n",
        "        if os.path.exists(self.cleaned_dir):\n",
        "            source_dir = self.cleaned_dir\n",
        "        elif os.path.exists(self.gdrive_cleaned_dir):\n",
        "            source_dir = self.gdrive_cleaned_dir\n",
        "        else:\n",
        "            logger.error(\"Tidak ada direktori teks yang ditemukan\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"üìÇ Loading teks dari: {source_dir}\")\n",
        "\n",
        "        for filename in os.listdir(source_dir):\n",
        "            if filename.endswith('.txt'):\n",
        "                filepath = os.path.join(source_dir, filename)\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                        text = f.read()\n",
        "                    texts[filename] = text\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "        print(f\"üìÅ Loaded {len(texts)} file teks\")\n",
        "        return texts\n",
        "\n",
        "    def load_metadata(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Load metadata dari langkah i\"\"\"\n",
        "        metadata_files = [f for f in os.listdir(self.input_dir) if f.startswith('metadata_') and f.endswith('.csv')]\n",
        "\n",
        "        if not metadata_files:\n",
        "            logger.warning(\"Tidak ada file metadata yang ditemukan\")\n",
        "            return None\n",
        "\n",
        "        # Ambil file metadata terbaru\n",
        "        latest_metadata = max(metadata_files)\n",
        "        metadata_path = os.path.join(self.input_dir, latest_metadata)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(metadata_path, encoding='utf-8')\n",
        "            print(f\"üìä Loaded metadata: {len(df)} records dari {latest_metadata}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading metadata: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_content(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Load konten kunci dari langkah ii\"\"\"\n",
        "        content_files = [f for f in os.listdir(self.input_dir) if f.startswith('konten_kunci_') and f.endswith('.csv')]\n",
        "\n",
        "        if not content_files:\n",
        "            logger.warning(\"Tidak ada file konten kunci yang ditemukan\")\n",
        "            return None\n",
        "\n",
        "        # Ambil file konten terbaru\n",
        "        latest_content = max(content_files)\n",
        "        content_path = os.path.join(self.input_dir, latest_content)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(content_path, encoding='utf-8')\n",
        "            print(f\"üìù Loaded konten kunci: {len(df)} records dari {latest_content}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading konten: {e}\")\n",
        "            return None\n",
        "\n",
        "    def count_syllables(self, word: str) -> int:\n",
        "        \"\"\"Estimasi jumlah suku kata dalam kata (untuk bahasa Indonesia)\"\"\"\n",
        "        vowels = 'aeiouAEIOU'\n",
        "        syllable_count = 0\n",
        "        prev_char_was_vowel = False\n",
        "\n",
        "        for char in word:\n",
        "            if char in vowels:\n",
        "                if not prev_char_was_vowel:\n",
        "                    syllable_count += 1\n",
        "                prev_char_was_vowel = True\n",
        "            else:\n",
        "                prev_char_was_vowel = False\n",
        "\n",
        "        return max(1, syllable_count)  # Minimal 1 suku kata\n",
        "\n",
        "    def calculate_flesch_score(self, words: int, sentences: int, syllables: int) -> float:\n",
        "        \"\"\"Hitung Flesch Reading Ease Score (adaptasi untuk Indonesia)\"\"\"\n",
        "        if sentences == 0 or words == 0:\n",
        "            return 0\n",
        "\n",
        "        avg_sentence_length = words / sentences\n",
        "        avg_syllables_per_word = syllables / words\n",
        "\n",
        "        # Formula Flesch (disesuaikan untuk bahasa Indonesia)\n",
        "        score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)\n",
        "        return max(0, min(100, score))\n",
        "\n",
        "    def calculate_text_features(self, texts: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"Hitung fitur-fitur teks dasar yang komprehensif\"\"\"\n",
        "        features_data = []\n",
        "\n",
        "        for filename, text in texts.items():\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            # Preprocessing\n",
        "            text_lower = text.lower()\n",
        "            words = text.split()\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            paragraphs = [p for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "            # 1. BASIC LENGTH FEATURES\n",
        "            char_count = len(text)\n",
        "            word_count = len(words)\n",
        "            sentence_count = len(sentences)\n",
        "            paragraph_count = len(paragraphs)\n",
        "\n",
        "            # 2. ADVANCED LEXICAL FEATURES\n",
        "            unique_words = len(set([w.lower() for w in words]))\n",
        "            lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
        "            avg_word_length = np.mean([len(word) for word in words]) if word_count > 0 else 0\n",
        "            avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
        "\n",
        "            # 3. READABILITY METRICS\n",
        "            syllable_count = sum([self.count_syllables(word) for word in words])\n",
        "            flesch_score = self.calculate_flesch_score(word_count, sentence_count, syllable_count)\n",
        "\n",
        "            # 4. LEGAL DOCUMENT SPECIFIC FEATURES\n",
        "            legal_terms_count = len(re.findall(r'\\b(?:pasal|undang|hukum|pidana|perdata|terdakwa|penggugat|hakim|jaksa|dakwaan|vonis|putusan)\\b', text_lower))\n",
        "            law_references = len(re.findall(r'\\b(?:uu|undang.*?undang)\\s+(?:no\\.?|nomor)\\s+\\d+', text_lower))\n",
        "            article_references = len(re.findall(r'\\bpasal\\s+\\d+', text_lower))\n",
        "            court_mentions = len(re.findall(r'\\bpengadilan\\s+(?:negeri|tinggi|agama|militer)', text_lower))\n",
        "\n",
        "            # 5. NUMERIC AND DATE FEATURES\n",
        "            number_mentions = len(re.findall(r'\\b\\d+\\b', text))\n",
        "            date_mentions = len(re.findall(r'\\b\\d{1,2}[\\s/\\-]\\w+[\\s/\\-]\\d{4}\\b', text))\n",
        "            money_mentions = len(re.findall(r'\\b(?:rp\\.?|rupiah)\\s*\\d', text_lower))\n",
        "            percentage_mentions = len(re.findall(r'\\d+\\s*%', text))\n",
        "\n",
        "            # 6. STRUCTURAL DOCUMENT FEATURES\n",
        "            has_header = 1 if re.search(r'\\bputusan\\b.*\\bpengadilan\\b', text_lower) else 0\n",
        "            has_case_number = 1 if re.search(r'\\bnomor\\s*:\\s*\\d+/', text_lower) else 0\n",
        "            has_parties = 1 if re.search(r'\\b(?:terdakwa|penggugat|tergugat)\\s*:', text_lower) else 0\n",
        "            has_consideration = 1 if re.search(r'\\bmenimbang\\b', text_lower) else 0\n",
        "            has_decision = 1 if re.search(r'\\bmengadili\\b', text_lower) else 0\n",
        "            has_conclusion = 1 if re.search(r'\\b(?:demikian|kesimpulan)\\b', text_lower) else 0\n",
        "\n",
        "            # 7. CONTENT TYPE FEATURES\n",
        "            has_dakwaan = 1 if re.search(r'\\bdakwaan\\b', text_lower) else 0\n",
        "            has_tuntutan = 1 if re.search(r'\\btuntutan\\b', text_lower) else 0\n",
        "            has_pembelaan = 1 if re.search(r'\\bpembelaan\\b', text_lower) else 0\n",
        "            has_saksi = 1 if re.search(r'\\bsaksi\\b', text_lower) else 0\n",
        "            has_bukti = 1 if re.search(r'\\bbarang\\s+bukti\\b', text_lower) else 0\n",
        "\n",
        "            # 8. LINGUISTIC COMPLEXITY FEATURES\n",
        "            complex_sentences = len([s for s in sentences if len(s.split()) > 20])\n",
        "            question_count = len(re.findall(r'\\?', text))\n",
        "            exclamation_count = len(re.findall(r'!', text))\n",
        "            quoted_text_count = len(re.findall(r'\"[^\"]*\"', text))\n",
        "\n",
        "            # 9. PARTY INVOLVEMENT FEATURES\n",
        "            defendant_mentions = len(re.findall(r'\\bterdakwa\\b', text_lower))\n",
        "            plaintiff_mentions = len(re.findall(r'\\bpenggugat\\b', text_lower))\n",
        "            judge_mentions = len(re.findall(r'\\bhakim\\b', text_lower))\n",
        "            prosecutor_mentions = len(re.findall(r'\\bjaksa\\b', text_lower))\n",
        "\n",
        "            # 10. LEGAL REASONING FEATURES\n",
        "            because_count = len(re.findall(r'\\b(?:karena|sebab|disebabkan)\\b', text_lower))\n",
        "            therefore_count = len(re.findall(r'\\b(?:oleh karena|dengan demikian|maka)\\b', text_lower))\n",
        "            evidence_count = len(re.findall(r'\\b(?:bukti|terbukti|membuktikan)\\b', text_lower))\n",
        "            violation_count = len(re.findall(r'\\b(?:melanggar|pelanggaran|melakukan)\\b', text_lower))\n",
        "\n",
        "            features_data.append({\n",
        "                'nama_file': filename,\n",
        "\n",
        "                # Basic Features\n",
        "                'char_count': char_count,\n",
        "                'word_count': word_count,\n",
        "                'sentence_count': sentence_count,\n",
        "                'paragraph_count': paragraph_count,\n",
        "\n",
        "                # Lexical Features\n",
        "                'unique_words': unique_words,\n",
        "                'lexical_diversity': lexical_diversity,\n",
        "                'avg_word_length': avg_word_length,\n",
        "                'avg_sentence_length': avg_sentence_length,\n",
        "\n",
        "                # Readability\n",
        "                'syllable_count': syllable_count,\n",
        "                'flesch_score': flesch_score,\n",
        "\n",
        "                # Legal Specific\n",
        "                'legal_terms_count': legal_terms_count,\n",
        "                'law_references': law_references,\n",
        "                'article_references': article_references,\n",
        "                'court_mentions': court_mentions,\n",
        "\n",
        "                # Numeric Features\n",
        "                'number_mentions': number_mentions,\n",
        "                'date_mentions': date_mentions,\n",
        "                'money_mentions': money_mentions,\n",
        "                'percentage_mentions': percentage_mentions,\n",
        "\n",
        "                # Document Structure\n",
        "                'has_header': has_header,\n",
        "                'has_case_number': has_case_number,\n",
        "                'has_parties': has_parties,\n",
        "                'has_consideration': has_consideration,\n",
        "                'has_decision': has_decision,\n",
        "                'has_conclusion': has_conclusion,\n",
        "\n",
        "                # Content Type\n",
        "                'has_dakwaan': has_dakwaan,\n",
        "                'has_tuntutan': has_tuntutan,\n",
        "                'has_pembelaan': has_pembelaan,\n",
        "                'has_saksi': has_saksi,\n",
        "                'has_bukti': has_bukti,\n",
        "\n",
        "                # Linguistic Complexity\n",
        "                'complex_sentences': complex_sentences,\n",
        "                'question_count': question_count,\n",
        "                'exclamation_count': exclamation_count,\n",
        "                'quoted_text_count': quoted_text_count,\n",
        "\n",
        "                # Party Mentions\n",
        "                'defendant_mentions': defendant_mentions,\n",
        "                'plaintiff_mentions': plaintiff_mentions,\n",
        "                'judge_mentions': judge_mentions,\n",
        "                'prosecutor_mentions': prosecutor_mentions,\n",
        "\n",
        "                # Legal Reasoning\n",
        "                'because_count': because_count,\n",
        "                'therefore_count': therefore_count,\n",
        "                'evidence_count': evidence_count,\n",
        "                'violation_count': violation_count\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(features_data)\n",
        "\n",
        "    def create_bag_of_words_features(self, texts: Dict[str, str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Buat bag-of-words dan TF-IDF features dengan preprocessing yang lebih baik\"\"\"\n",
        "        filenames = list(texts.keys())\n",
        "        text_content = [texts[fname] for fname in filenames]\n",
        "\n",
        "        # Filter empty texts\n",
        "        valid_indices = [i for i, text in enumerate(text_content) if text.strip()]\n",
        "        valid_filenames = [filenames[i] for i in valid_indices]\n",
        "        valid_texts = [text_content[i] for i in valid_indices]\n",
        "\n",
        "        if not valid_texts:\n",
        "            return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "        # Preprocessing teks\n",
        "        processed_texts = []\n",
        "        for text in valid_texts:\n",
        "            # Bersihkan teks\n",
        "            text = re.sub(r'\\d+', 'NUMBER', text)  # Replace numbers\n",
        "            text = re.sub(r'[^\\w\\s]', ' ', text)   # Remove punctuation\n",
        "            text = re.sub(r'\\s+', ' ', text)       # Normalize whitespace\n",
        "            processed_texts.append(text.lower())\n",
        "\n",
        "        # 1. COUNT VECTORIZER (Bag of Words)\n",
        "        try:\n",
        "            count_matrix = self.count_vectorizer.fit_transform(processed_texts)\n",
        "            count_feature_names = self.count_vectorizer.get_feature_names_out()\n",
        "\n",
        "            bow_df = pd.DataFrame(\n",
        "                count_matrix.toarray(),\n",
        "                columns=[f'bow_{name}' for name in count_feature_names],\n",
        "                index=valid_filenames\n",
        "            )\n",
        "            bow_df.reset_index(inplace=True)\n",
        "            bow_df.rename(columns={'index': 'nama_file'}, inplace=True)\n",
        "\n",
        "            print(f\"üéØ Bag-of-Words: {bow_df.shape[1]-1} features\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating BoW features: {e}\")\n",
        "            bow_df = pd.DataFrame()\n",
        "\n",
        "        # 2. TF-IDF VECTORIZER\n",
        "        try:\n",
        "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_texts)\n",
        "            tfidf_feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "            tfidf_df = pd.DataFrame(\n",
        "                tfidf_matrix.toarray(),\n",
        "                columns=[f'tfidf_{name}' for name in tfidf_feature_names],\n",
        "                index=valid_filenames\n",
        "            )\n",
        "            tfidf_df.reset_index(inplace=True)\n",
        "            tfidf_df.rename(columns={'index': 'nama_file'}, inplace=True)\n",
        "\n",
        "            print(f\"üìà TF-IDF: {tfidf_df.shape[1]-1} features\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating TF-IDF features: {e}\")\n",
        "            tfidf_df = pd.DataFrame()\n",
        "\n",
        "        return bow_df, tfidf_df\n",
        "\n",
        "    def create_qa_pairs(self, texts: Dict[str, str], metadata_df: pd.DataFrame = None) -> List[Dict]:\n",
        "        \"\"\"Buat QA pairs sederhana untuk training dengan lebih banyak template\"\"\"\n",
        "        qa_pairs = []\n",
        "\n",
        "        for filename, text in texts.items():\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            # Template QA yang lebih komprehensif\n",
        "            qa_templates = [\n",
        "                # Basic Information Extraction\n",
        "                {\n",
        "                    'question': 'Siapa terdakwa dalam perkara ini?',\n",
        "                    'pattern': r'terdakwa\\s*:\\s*([A-Z][^\\n\\r,;]+?)(?:\\s*(?:,|;|\\n))',\n",
        "                    'type': 'entity_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Siapa hakim dalam perkara ini?',\n",
        "                    'pattern': r'hakim\\s+(?:ketua|anggota)\\s*:\\s*([A-Z][^\\n\\r,;]+?)(?:\\s*(?:,|;|\\n))',\n",
        "                    'type': 'entity_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Siapa jaksa penuntut umum?',\n",
        "                    'pattern': r'(?:jaksa\\s+)?penuntut\\s+umum\\s*:\\s*([A-Z][^\\n\\r,;]+?)(?:\\s*(?:,|;|\\n))',\n",
        "                    'type': 'entity_extraction'\n",
        "                },\n",
        "\n",
        "                # Case Classification\n",
        "                {\n",
        "                    'question': 'Apa jenis perkara ini?',\n",
        "                    'pattern': r'perkara\\s+(pidana\\s+(?:khusus|umum)|perdata|tindak\\s+pidana\\s+\\w+)',\n",
        "                    'type': 'classification'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Apakah ini perkara perdagangan orang?',\n",
        "                    'pattern': r'(?:perkara\\s+)?tindak\\s+pidana\\s+(perdagangan orang)',\n",
        "                    'type': 'classification'\n",
        "                },\n",
        "\n",
        "                # Legal References\n",
        "                {\n",
        "                    'question': 'Pasal apa yang dilanggar?',\n",
        "                    'pattern': r'(?:melanggar\\s+)?pasal\\s+(\\d+(?:\\s+(?:ayat|huruf)\\s*\\([^)]+\\))?)',\n",
        "                    'type': 'legal_reference'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Undang-undang apa yang dirujuk?',\n",
        "                    'pattern': r'(?:undang[- ]undang|uu)\\s+(?:republik\\s+indonesia\\s+)?(?:no\\.?\\s*|nomor\\s+)?(\\d+\\s+tahun\\s+\\d{4})',\n",
        "                    'type': 'legal_reference'\n",
        "                },\n",
        "\n",
        "                # Temporal Information\n",
        "                {\n",
        "                    'question': 'Kapan putusan ini dibacakan?',\n",
        "                    'pattern': r'(?:dibacakan|diputuskan)\\s+(?:pada\\s+)?tanggal\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})',\n",
        "                    'type': 'date_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Kapan tindak pidana terjadi?',\n",
        "                    'pattern': r'(?:pada|tanggal)\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})[^,]*(?:terdakwa|melakukan)',\n",
        "                    'type': 'date_extraction'\n",
        "                },\n",
        "\n",
        "                # Decision and Punishment\n",
        "                {\n",
        "                    'question': 'Apa putusan hakim?',\n",
        "                    'pattern': r'mengadili\\s*:\\s*([^.]+\\.)',\n",
        "                    'type': 'decision_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Berapa lama hukuman yang dijatuhkan?',\n",
        "                    'pattern': r'(?:dengan\\s+)?hukuman\\s+(?:penjara|kurungan)\\s+(?:selama\\s+)?(\\d+\\s+(?:tahun|bulan|hari))',\n",
        "                    'type': 'punishment_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Berapa denda yang harus dibayar?',\n",
        "                    'pattern': r'(?:denda|membayar)\\s+(?:sebesar\\s+)?(?:rp\\.?\\s*)?([0-9.,]+(?:\\s*(?:juta|ribu|miliar))?)',\n",
        "                    'type': 'fine_extraction'\n",
        "                },\n",
        "\n",
        "                # Evidence and Facts\n",
        "                {\n",
        "                    'question': 'Apa barang bukti dalam perkara ini?',\n",
        "                    'pattern': r'barang\\s+bukti\\s*(?:berupa|adalah)?\\s*:?\\s*([^.]+)',\n",
        "                    'type': 'evidence_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Berapa kerugian yang ditimbulkan?',\n",
        "                    'pattern': r'kerugian\\s+(?:negara|keuangan)\\s+(?:sebesar\\s+)?(?:rp\\.?\\s*)?([0-9.,]+(?:\\s*(?:juta|ribu|miliar))?)',\n",
        "                    'type': 'damage_extraction'\n",
        "                },\n",
        "\n",
        "                # Legal Reasoning\n",
        "                {\n",
        "                    'question': 'Mengapa terdakwa dianggap bersalah?',\n",
        "                    'pattern': r'(?:terbukti|bersalah)\\s+(?:secara\\s+sah\\s+dan\\s+meyakinkan\\s+)?([^.]+)',\n",
        "                    'type': 'reasoning_extraction'\n",
        "                },\n",
        "                {\n",
        "                    'question': 'Apa pertimbangan hakim?',\n",
        "                    'pattern': r'menimbang\\s*[,:]\\s*([^;]+)',\n",
        "                    'type': 'consideration_extraction'\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            for template in qa_templates:\n",
        "                matches = re.finditer(template['pattern'], text, re.IGNORECASE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    answer = match.group(1).strip()\n",
        "                    if answer and len(answer) > 3 and len(answer) < 300:  # Filter jawaban\n",
        "                        # Bersihkan jawaban\n",
        "                        answer = re.sub(r'\\s+', ' ', answer)\n",
        "                        answer = answer.strip('.,;:')\n",
        "\n",
        "                        qa_pairs.append({\n",
        "                            'filename': filename,\n",
        "                            'question': template['question'],\n",
        "                            'answer': answer,\n",
        "                            'question_type': template['type'],\n",
        "                            'context': text[max(0, match.start()-100):match.end()+100],\n",
        "                            'confidence': min(1.0, len(answer) / 100)  # Simple confidence score\n",
        "                        })\n",
        "\n",
        "        print(f\"‚ùì Generated {len(qa_pairs)} QA pairs\")\n",
        "        return qa_pairs\n",
        "\n",
        "    def encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Encode categorical features menjadi numerical dengan handling yang lebih baik\"\"\"\n",
        "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "        categorical_columns = [col for col in categorical_columns if col != 'nama_file']\n",
        "\n",
        "        df_encoded = df.copy()\n",
        "\n",
        "        for col in categorical_columns:\n",
        "            if col not in self.label_encoders:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "\n",
        "            # Handle missing values\n",
        "            df_encoded[col] = df_encoded[col].fillna('unknown')\n",
        "\n",
        "            try:\n",
        "                df_encoded[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df_encoded[col])\n",
        "                print(f\"üî¢ Encoded {col}: {len(self.label_encoders[col].classes_)} categories\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error encoding {col}: {e}\")\n",
        "\n",
        "        return df_encoded\n",
        "\n",
        "    def combine_all_features(self, text_features: pd.DataFrame, bow_features: pd.DataFrame,\n",
        "                           tfidf_features: pd.DataFrame, metadata_df: pd.DataFrame = None,\n",
        "                           content_df: pd.DataFrame = None) -> pd.DataFrame:\n",
        "        \"\"\"Gabungkan semua features menjadi satu dataset dengan handling yang robust\"\"\"\n",
        "\n",
        "        # Start dengan text features\n",
        "        combined_df = text_features.copy()\n",
        "\n",
        "        # Join dengan metadata jika tersedia\n",
        "        if metadata_df is not None and not metadata_df.empty:\n",
        "            combined_df = pd.merge(combined_df, metadata_df, on='nama_file', how='left')\n",
        "            print(f\"üìä Joined dengan metadata: {combined_df.shape}\")\n",
        "\n",
        "        # Join dengan content jika tersedia\n",
        "        if content_df is not None and not content_df.empty:\n",
        "            # Pilih kolom penting dari content\n",
        "            content_cols = ['nama_file']\n",
        "            if 'kelengkapan_konten_persen' in content_df.columns:\n",
        "                content_cols.append('kelengkapan_konten_persen')\n",
        "            if 'panjang_teks' in content_df.columns:\n",
        "                content_cols.append('panjang_teks')\n",
        "            if 'jumlah_kata' in content_df.columns:\n",
        "                content_cols.append('jumlah_kata')\n",
        "\n",
        "            available_cols = [col for col in content_cols if col in content_df.columns]\n",
        "            if len(available_cols) > 1:  # More than just nama_file\n",
        "                content_subset = content_df[available_cols].copy()\n",
        "                combined_df = pd.merge(combined_df, content_subset, on='nama_file', how='left')\n",
        "                print(f\"üìù Joined dengan konten: {combined_df.shape}\")\n",
        "\n",
        "        # Join dengan BoW features jika tersedia\n",
        "        if not bow_features.empty:\n",
        "            combined_df = pd.merge(combined_df, bow_features, on='nama_file', how='left')\n",
        "            print(f\"üéØ Joined dengan BoW: {combined_df.shape}\")\n",
        "\n",
        "        # Join dengan TF-IDF features jika tersedia\n",
        "        if not tfidf_features.empty:\n",
        "            combined_df = pd.merge(combined_df, tfidf_features, on='nama_file', how='left')\n",
        "            print(f\"üìà Joined dengan TF-IDF: {combined_df.shape}\")\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def create_target_variables(self, combined_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Buat target variables yang komprehensif untuk supervised learning\"\"\"\n",
        "        df_with_targets = combined_df.copy()\n",
        "\n",
        "        # 1. CLASSIFICATION TARGETS\n",
        "        # Jenis perkara classification\n",
        "        if 'jenis_perkara' in df_with_targets.columns:\n",
        "            df_with_targets['is_pidana'] = df_with_targets['jenis_perkara'].apply(\n",
        "                lambda x: 1 if x and 'pidana' in str(x).lower() else 0\n",
        "            )\n",
        "            df_with_targets['is_perdagangan_orang'] = df_with_targets['jenis_perkara'].apply(\n",
        "                lambda x: 1 if x and 'perdagangan_orang' in str(x).lower() else 0\n",
        "            )\n",
        "            df_with_targets['is_perdata'] = df_with_targets['jenis_perkara'].apply(\n",
        "                lambda x: 1 if x and 'perdata' in str(x).lower() else 0\n",
        "            )\n",
        "\n",
        "        # 2. REGRESSION TARGETS\n",
        "        # Kompleksitas perkara berdasarkan jumlah kata dan fitur lainnya\n",
        "        if 'word_count' in df_with_targets.columns:\n",
        "            # Document complexity score (1-4)\n",
        "            df_with_targets['complexity_score'] = pd.cut(\n",
        "                df_with_targets['word_count'],\n",
        "                bins=[0, 1000, 5000, 10000, float('inf')],\n",
        "                labels=[1, 2, 3, 4]\n",
        "            ).astype(int)\n",
        "\n",
        "            # Document length category\n",
        "            df_with_targets['length_category'] = pd.cut(\n",
        "                df_with_targets['word_count'],\n",
        "                bins=[0, 2000, 7500, 15000, float('inf')],\n",
        "                labels=['short', 'medium', 'long', 'very_long']\n",
        "            )\n",
        "\n",
        "        # Legal complexity based on multiple factors\n",
        "        legal_complexity = 0\n",
        "        if 'article_references' in df_with_targets.columns:\n",
        "            legal_complexity += df_with_targets['article_references'].fillna(0) * 0.3\n",
        "        if 'law_references' in df_with_targets.columns:\n",
        "            legal_complexity += df_with_targets['law_references'].fillna(0) * 0.4\n",
        "        if 'legal_terms_count' in df_with_targets.columns:\n",
        "            legal_complexity += (df_with_targets['legal_terms_count'].fillna(0) / 10) * 0.3\n",
        "\n",
        "        df_with_targets['legal_complexity'] = legal_complexity\n",
        "\n",
        "        # 3. BINARY TARGETS\n",
        "        # Kelengkapan dokumen\n",
        "        if 'has_decision' in df_with_targets.columns and 'has_consideration' in df_with_targets.columns:\n",
        "            df_with_targets['is_complete_doc'] = (\n",
        "                (df_with_targets['has_decision'] == 1) &\n",
        "                (df_with_targets['has_consideration'] == 1) &\n",
        "                (df_with_targets['has_parties'] == 1)\n",
        "            ).astype(int)\n",
        "\n",
        "        # Document quality indicators\n",
        "        if 'has_case_number' in df_with_targets.columns:\n",
        "            df_with_targets['has_proper_format'] = (\n",
        "                (df_with_targets['has_case_number'] == 1) &\n",
        "                (df_with_targets['has_header'] == 1)\n",
        "            ).astype(int)\n",
        "\n",
        "        # High-quality legal document\n",
        "        quality_score = 0\n",
        "        quality_features = ['has_consideration', 'has_decision', 'has_parties', 'has_case_number']\n",
        "        for feature in quality_features:\n",
        "            if feature in df_with_targets.columns:\n",
        "                quality_score += df_with_targets[feature].fillna(0)\n",
        "\n",
        "        df_with_targets['quality_score'] = quality_score\n",
        "        df_with_targets['is_high_quality'] = (quality_score >= 3).astype(int)\n",
        "\n",
        "        # 4. MULTI-CLASS TARGETS\n",
        "        # Document type classification\n",
        "        doc_type = []\n",
        "        for idx, row in df_with_targets.iterrows():\n",
        "            if row.get('has_dakwaan', 0) == 1:\n",
        "                doc_type.append('criminal_case')\n",
        "            elif row.get('is_perdata', 0) == 1:\n",
        "                doc_type.append('civil_case')\n",
        "            elif row.get('has_decision', 0) == 1:\n",
        "                doc_type.append('judgment')\n",
        "            else:\n",
        "                doc_type.append('other')\n",
        "\n",
        "        df_with_targets['document_type'] = doc_type\n",
        "\n",
        "        # Case severity (based on legal terms and complexity)\n",
        "        severity_bins = [0, 5, 15, 30, float('inf')]\n",
        "        severity_labels = ['low', 'medium', 'high', 'critical']\n",
        "\n",
        "        if 'legal_terms_count' in df_with_targets.columns:\n",
        "            df_with_targets['case_severity'] = pd.cut(\n",
        "                df_with_targets['legal_terms_count'].fillna(0),\n",
        "                bins=severity_bins,\n",
        "                labels=severity_labels\n",
        "            )\n",
        "\n",
        "        print(f\"üéØ Created comprehensive target variables\")\n",
        "        return df_with_targets\n",
        "\n",
        "    def create_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Buat derived features dari kombinasi fitur existing\"\"\"\n",
        "        df_derived = df.copy()\n",
        "\n",
        "        # 1. RATIO FEATURES\n",
        "        if 'unique_words' in df_derived.columns and 'word_count' in df_derived.columns:\n",
        "            df_derived['vocabulary_richness'] = df_derived['unique_words'] / (df_derived['word_count'] + 1)\n",
        "\n",
        "        if 'legal_terms_count' in df_derived.columns and 'word_count' in df_derived.columns:\n",
        "            df_derived['legal_term_density'] = df_derived['legal_terms_count'] / (df_derived['word_count'] + 1)\n",
        "\n",
        "        if 'sentence_count' in df_derived.columns and 'paragraph_count' in df_derived.columns:\n",
        "            df_derived['sentences_per_paragraph'] = df_derived['sentence_count'] / (df_derived['paragraph_count'] + 1)\n",
        "\n",
        "        # 2. INTERACTION FEATURES\n",
        "        if 'article_references' in df_derived.columns and 'law_references' in df_derived.columns:\n",
        "            df_derived['total_legal_refs'] = df_derived['article_references'].fillna(0) + df_derived['law_references'].fillna(0)\n",
        "\n",
        "        if 'defendant_mentions' in df_derived.columns and 'prosecutor_mentions' in df_derived.columns:\n",
        "            df_derived['prosecution_intensity'] = df_derived['prosecutor_mentions'].fillna(0) / (df_derived['defendant_mentions'].fillna(0) + 1)\n",
        "\n",
        "        # 3. COMPOSITE SCORES\n",
        "        # Document formality score\n",
        "        formality_features = ['has_header', 'has_case_number', 'has_parties', 'has_consideration', 'has_decision']\n",
        "        formality_score = 0\n",
        "        for feature in formality_features:\n",
        "            if feature in df_derived.columns:\n",
        "                formality_score += df_derived[feature].fillna(0)\n",
        "        df_derived['formality_score'] = formality_score / len(formality_features)\n",
        "\n",
        "        # Content richness score\n",
        "        content_features = ['has_dakwaan', 'has_bukti', 'has_saksi', 'has_tuntutan']\n",
        "        content_score = 0\n",
        "        for feature in content_features:\n",
        "            if feature in df_derived.columns:\n",
        "                content_score += df_derived[feature].fillna(0)\n",
        "        df_derived['content_richness'] = content_score / len(content_features)\n",
        "\n",
        "        print(f\"üîÑ Created derived features\")\n",
        "        return df_derived\n",
        "\n",
        "    def perform_feature_selection(self, df: pd.DataFrame, n_components: int = 50) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Lakukan feature selection dan dimensionality reduction\"\"\"\n",
        "        # Separate numerical and categorical columns\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        numeric_cols = [col for col in numeric_cols if col != 'nama_file']\n",
        "\n",
        "        if len(numeric_cols) < 2:\n",
        "            print(\"‚ö†Ô∏è Tidak cukup fitur numerik untuk PCA\")\n",
        "            return df, pd.DataFrame()\n",
        "\n",
        "        # Prepare data for PCA\n",
        "        numeric_data = df[numeric_cols].fillna(0)\n",
        "\n",
        "        # Standardize features\n",
        "        try:\n",
        "            scaled_data = self.scaler.fit_transform(numeric_data)\n",
        "\n",
        "            # Apply PCA\n",
        "            n_components = min(n_components, len(numeric_cols), len(df) - 1)\n",
        "            pca = PCA(n_components=n_components)\n",
        "            pca_features = pca.fit_transform(scaled_data)\n",
        "\n",
        "            # Create PCA DataFrame\n",
        "            pca_columns = [f'pca_{i+1}' for i in range(n_components)]\n",
        "            pca_df = pd.DataFrame(pca_features, columns=pca_columns, index=df.index)\n",
        "            pca_df['nama_file'] = df['nama_file'].values\n",
        "\n",
        "            # Calculate explained variance\n",
        "            explained_variance = pca.explained_variance_ratio_\n",
        "            cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "            print(f\"üìä PCA completed: {n_components} components explain {cumulative_variance[-1]:.2%} variance\")\n",
        "\n",
        "            return df, pca_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in PCA: {e}\")\n",
        "            return df, pd.DataFrame()\n",
        "\n",
        "    def generate_feature_summary(self, combined_df: pd.DataFrame, qa_pairs: List[Dict], pca_df: pd.DataFrame = None) -> str:\n",
        "        \"\"\"Generate comprehensive feature summary\"\"\"\n",
        "        summary = []\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(\"FEATURE ENGINEERING SUMMARY REPORT\")\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Dataset Overview\n",
        "        summary.append(\"üìä DATASET OVERVIEW\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        summary.append(f\"Total Documents: {combined_df.shape[0]}\")\n",
        "        summary.append(f\"Total Features: {combined_df.shape[1]}\")\n",
        "        summary.append(f\"QA Pairs Generated: {len(qa_pairs)}\")\n",
        "        if pca_df is not None and not pca_df.empty:\n",
        "            summary.append(f\"PCA Components: {pca_df.shape[1] - 1}\")  # -1 for nama_file\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Feature Categories\n",
        "        summary.append(\"üîß FEATURE CATEGORIES\")\n",
        "        summary.append(\"-\" * 30)\n",
        "\n",
        "        # Count features by category\n",
        "        bow_features = len([col for col in combined_df.columns if col.startswith('bow_')])\n",
        "        tfidf_features = len([col for col in combined_df.columns if col.startswith('tfidf_')])\n",
        "        text_features = len([col for col in combined_df.columns if col in [\n",
        "            'char_count', 'word_count', 'sentence_count', 'paragraph_count',\n",
        "            'unique_words', 'lexical_diversity', 'avg_word_length'\n",
        "        ]])\n",
        "\n",
        "        summary.append(f\"Text Features: {text_features}\")\n",
        "        summary.append(f\"Bag-of-Words Features: {bow_features}\")\n",
        "        summary.append(f\"TF-IDF Features: {tfidf_features}\")\n",
        "        summary.append(f\"Legal Features: {len([col for col in combined_df.columns if 'legal' in col])}\")\n",
        "        summary.append(f\"Target Variables: {len([col for col in combined_df.columns if col.startswith('is_') or col.endswith('_score')])}\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Data Quality\n",
        "        summary.append(\"üìà DATA QUALITY METRICS\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        summary.append(f\"Missing Value Ratio: {combined_df.isnull().sum().sum() / (combined_df.shape[0] * combined_df.shape[1]):.2%}\")\n",
        "        summary.append(f\"Complete Cases: {combined_df.dropna().shape[0]} ({combined_df.dropna().shape[0]/combined_df.shape[0]:.1%})\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # Feature Types\n",
        "        summary.append(\"üìã FEATURE TYPES\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        for dtype in combined_df.dtypes.value_counts().items():\n",
        "            summary.append(f\"{dtype[0]}: {dtype[1]} columns\")\n",
        "        summary.append(\"\")\n",
        "\n",
        "        # QA Statistics\n",
        "        if qa_pairs:\n",
        "            summary.append(\"‚ùì QA PAIRS STATISTICS\")\n",
        "            summary.append(\"-\" * 30)\n",
        "            qa_types = Counter([qa['question_type'] for qa in qa_pairs])\n",
        "            for qa_type, count in qa_types.most_common():\n",
        "                summary.append(f\"{qa_type}: {count} pairs\")\n",
        "            summary.append(\"\")\n",
        "\n",
        "        # Column List\n",
        "        summary.append(\"üìù ALL COLUMNS\")\n",
        "        summary.append(\"-\" * 30)\n",
        "        for i, col in enumerate(combined_df.columns, 1):\n",
        "            summary.append(f\"{i:3d}. {col}\")\n",
        "\n",
        "        summary.append(\"\")\n",
        "        summary.append(\"=\" * 70)\n",
        "        summary.append(\"END OF REPORT\")\n",
        "        summary.append(\"=\" * 70)\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def save_features_to_files(self, combined_df: pd.DataFrame, qa_pairs: List[Dict], pca_df: pd.DataFrame = None):\n",
        "        \"\"\"Simpan semua features ke file dengan backup di Google Drive\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # 1. SAVE MAIN FEATURES DATASET\n",
        "        features_filename = f\"features_{timestamp}.csv\"\n",
        "        features_path_local = os.path.join(self.output_dir, features_filename)\n",
        "        features_path_gdrive = os.path.join(self.gdrive_output_dir, features_filename)\n",
        "\n",
        "        combined_df.to_csv(features_path_local, index=False, encoding='utf-8')\n",
        "        combined_df.to_csv(features_path_gdrive, index=False, encoding='utf-8')\n",
        "        print(f\"üìÑ Features CSV Lokal: {features_path_local}\")\n",
        "        print(f\"üíæ Features CSV GDrive: {features_path_gdrive}\")\n",
        "\n",
        "        # 2. SAVE PCA FEATURES if available\n",
        "        if pca_df is not None and not pca_df.empty:\n",
        "            pca_filename = f\"features_pca_{timestamp}.csv\"\n",
        "            pca_path_local = os.path.join(self.output_dir, pca_filename)\n",
        "            pca_path_gdrive = os.path.join(self.gdrive_output_dir, pca_filename)\n",
        "\n",
        "            pca_df.to_csv(pca_path_local, index=False, encoding='utf-8')\n",
        "            pca_df.to_csv(pca_path_gdrive, index=False, encoding='utf-8')\n",
        "            print(f\"üìä PCA CSV Lokal: {pca_path_local}\")\n",
        "            print(f\"üíæ PCA CSV GDrive: {pca_path_gdrive}\")\n",
        "\n",
        "        # 3. SAVE QA PAIRS\n",
        "        if qa_pairs:\n",
        "            qa_filename = f\"qa_pairs_{timestamp}.json\"\n",
        "            qa_path_local = os.path.join(self.output_dir, qa_filename)\n",
        "            qa_path_gdrive = os.path.join(self.gdrive_output_dir, qa_filename)\n",
        "\n",
        "            with open(qa_path_local, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "            with open(qa_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"‚ùì QA JSON Lokal: {qa_path_local}\")\n",
        "            print(f\"üíæ QA JSON GDrive: {qa_path_gdrive}\")\n",
        "\n",
        "        # 4. SAVE FEATURE SUMMARY\n",
        "        summary_filename = f\"feature_summary_{timestamp}.txt\"\n",
        "        summary_path_local = os.path.join(self.output_dir, summary_filename)\n",
        "        summary_path_gdrive = os.path.join(self.gdrive_output_dir, summary_filename)\n",
        "\n",
        "        summary_content = self.generate_feature_summary(combined_df, qa_pairs, pca_df)\n",
        "\n",
        "        with open(summary_path_local, 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_content)\n",
        "        with open(summary_path_gdrive, 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_content)\n",
        "\n",
        "        print(f\"üìã Summary Lokal: {summary_path_local}\")\n",
        "        print(f\"üíæ Summary GDrive: {summary_path_gdrive}\")\n",
        "\n",
        "        return {\n",
        "            'features_local': features_path_local,\n",
        "            'features_gdrive': features_path_gdrive,\n",
        "            'qa_local': qa_path_local if qa_pairs else None,\n",
        "            'qa_gdrive': qa_path_gdrive if qa_pairs else None,\n",
        "            'summary_local': summary_path_local,\n",
        "            'summary_gdrive': summary_path_gdrive,\n",
        "            'pca_local': pca_path_local if pca_df is not None and not pca_df.empty else None,\n",
        "            'pca_gdrive': pca_path_gdrive if pca_df is not None and not pca_df.empty else None\n",
        "        }\n",
        "\n",
        "    def process_all_features(self) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "        \"\"\"Proses semua tahap feature engineering dengan error handling yang robust\"\"\"\n",
        "        print(\"üîß iii. FEATURE ENGINEERING - COMPREHENSIVE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"Generating features: Text stats, BoW, TF-IDF, QA pairs, PCA\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # 1. LOAD DATA\n",
        "            print(\"\\nüìÇ Loading data...\")\n",
        "            texts = self.load_text_files()\n",
        "            if not texts:\n",
        "                print(\"‚ùå Tidak ada file teks yang ditemukan!\")\n",
        "                return pd.DataFrame(), []\n",
        "\n",
        "            metadata_df = self.load_metadata()\n",
        "            content_df = self.load_content()\n",
        "\n",
        "            # 2. CALCULATE TEXT FEATURES\n",
        "            print(\"\\nüìä Calculating comprehensive text features...\")\n",
        "            text_features = self.calculate_text_features(texts)\n",
        "            print(f\"‚úÖ Text features: {text_features.shape}\")\n",
        "\n",
        "            # 3. CREATE BAG-OF-WORDS AND TF-IDF\n",
        "            print(\"\\nüéØ Creating BoW and TF-IDF features...\")\n",
        "            bow_features, tfidf_features = self.create_bag_of_words_features(texts)\n",
        "\n",
        "            # 4. CREATE QA PAIRS\n",
        "            print(\"\\n‚ùì Creating comprehensive QA pairs...\")\n",
        "            qa_pairs = self.create_qa_pairs(texts, metadata_df)\n",
        "\n",
        "            # 5. COMBINE ALL FEATURES\n",
        "            print(\"\\nüîó Combining all features...\")\n",
        "            combined_df = self.combine_all_features(\n",
        "                text_features, bow_features, tfidf_features, metadata_df, content_df\n",
        "            )\n",
        "\n",
        "            # 6. CREATE DERIVED FEATURES\n",
        "            print(\"\\nüîÑ Creating derived features...\")\n",
        "            combined_df = self.create_derived_features(combined_df)\n",
        "\n",
        "            # 7. ENCODE CATEGORICAL FEATURES\n",
        "            print(\"\\nüî¢ Encoding categorical features...\")\n",
        "            combined_df = self.encode_categorical_features(combined_df)\n",
        "\n",
        "            # 8. CREATE TARGET VARIABLES\n",
        "            print(\"\\nüéØ Creating target variables...\")\n",
        "            combined_df = self.create_target_variables(combined_df)\n",
        "\n",
        "            # 9. FEATURE SELECTION AND PCA\n",
        "            print(\"\\nüìä Performing feature selection and PCA...\")\n",
        "            combined_df, pca_df = self.perform_feature_selection(combined_df)\n",
        "\n",
        "            # 10. SAVE ALL FEATURES\n",
        "            print(\"\\nüíæ Saving features to files...\")\n",
        "            file_paths = self.save_features_to_files(combined_df, qa_pairs, pca_df)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(f\"‚úÖ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\")\n",
        "            print(f\"üìä Final dataset: {combined_df.shape}\")\n",
        "            print(f\"‚ùì QA pairs: {len(qa_pairs)}\")\n",
        "            print(f\"üíæ Files saved to 2 locations: lokal & Google Drive\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            return combined_df, qa_pairs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in feature engineering: {e}\")\n",
        "            print(f\"üí• ERROR: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return pd.DataFrame(), []\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fungsi utama untuk menjalankan feature engineering\"\"\"\n",
        "    print(\"üöÄ MULAI COMPREHENSIVE FEATURE ENGINEERING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        engineer = FeatureEngineer()\n",
        "        features_df, qa_pairs = engineer.process_all_features()\n",
        "\n",
        "        if not features_df.empty:\n",
        "            print(f\"\\nüéâ FEATURE ENGINEERING BERHASIL!\")\n",
        "            print(f\"Dataset shape: {features_df.shape}\")\n",
        "            print(f\"QA pairs: {len(qa_pairs)}\")\n",
        "            print(\"File output tersimpan di:\")\n",
        "            print(\"  - Lokal: /data/processed/\")\n",
        "            print(\"  - GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Tidak ada features yang berhasil dibuat.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nüí• ERROR: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g2LD3pNA6D7",
        "outputId": "ee648e5c-6b2e-419a-d1cf-4b0902e3c50a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ MULAI COMPREHENSIVE FEATURE ENGINEERING\n",
            "======================================================================\n",
            "üîß FEATURE ENGINEERING\n",
            "Input teks: /data/raw atau /content/drive/MyDrive/perdagangan_orang/CLEANED\n",
            "Input metadata: /data/processed\n",
            "Output Lokal: /data/processed\n",
            "Output GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed\n",
            "üîß iii. FEATURE ENGINEERING - COMPREHENSIVE\n",
            "============================================================\n",
            "Generating features: Text stats, BoW, TF-IDF, QA pairs, PCA\n",
            "============================================================\n",
            "\n",
            "üìÇ Loading data...\n",
            "üìÇ Loading teks dari: /content/drive/MyDrive/perdagangan_orang/CLEANED\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Tidak ada file metadata yang ditemukan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Loaded 79 file teks\n",
            "üìù Loaded konten kunci: 79 records dari konten_kunci_20250625_121448.csv\n",
            "\n",
            "üìä Calculating comprehensive text features...\n",
            "‚úÖ Text features: (79, 42)\n",
            "\n",
            "üéØ Creating BoW and TF-IDF features...\n",
            "üéØ Bag-of-Words: 500 features\n",
            "üìà TF-IDF: 1000 features\n",
            "\n",
            "‚ùì Creating comprehensive QA pairs...\n",
            "‚ùì Generated 4059 QA pairs\n",
            "\n",
            "üîó Combining all features...\n",
            "üìù Joined dengan konten: (79, 45)\n",
            "üéØ Joined dengan BoW: (79, 545)\n",
            "üìà Joined dengan TF-IDF: (79, 1545)\n",
            "\n",
            "üîÑ Creating derived features...\n",
            "üîÑ Created derived features\n",
            "\n",
            "üî¢ Encoding categorical features...\n",
            "\n",
            "üéØ Creating target variables...\n",
            "üéØ Created comprehensive target variables\n",
            "\n",
            "üìä Performing feature selection and PCA...\n",
            "üìä PCA completed: 50 components explain 93.96% variance\n",
            "\n",
            "üíæ Saving features to files...\n",
            "üìÑ Features CSV Lokal: /data/processed/features_20250625_122117.csv\n",
            "üíæ Features CSV GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/features_20250625_122117.csv\n",
            "üìä PCA CSV Lokal: /data/processed/features_pca_20250625_122117.csv\n",
            "üíæ PCA CSV GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/features_pca_20250625_122117.csv\n",
            "‚ùì QA JSON Lokal: /data/processed/qa_pairs_20250625_122117.json\n",
            "üíæ QA JSON GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/qa_pairs_20250625_122117.json\n",
            "üìã Summary Lokal: /data/processed/feature_summary_20250625_122117.txt\n",
            "üíæ Summary GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/feature_summary_20250625_122117.txt\n",
            "\n",
            "============================================================\n",
            "‚úÖ FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\n",
            "üìä Final dataset: (79, 1561)\n",
            "‚ùì QA pairs: 4059\n",
            "üíæ Files saved to 2 locations: lokal & Google Drive\n",
            "============================================================\n",
            "\n",
            "üéâ FEATURE ENGINEERING BERHASIL!\n",
            "Dataset shape: (79, 1561)\n",
            "QA pairs: 4059\n",
            "File output tersimpan di:\n",
            "  - Lokal: /data/processed/\n",
            "  - GDrive: /content/drive/MyDrive/perdagangan_orang/data/processed/\n"
          ]
        }
      ]
    }
  ]
}